"""
å¿ƒç†å’¨è¯¢å¯¹è¯ç³»ç»Ÿ - LangGraphä¼˜åŒ–ç‰ˆæœ¬

è¯¥ç‰ˆæœ¬å°†å¯¹è¯æµç¨‹æ‹†åˆ†ä¸ºå¤šä¸ªç‹¬ç«‹çš„èŠ‚ç‚¹ï¼Œé€šè¿‡LangGraphè¿›è¡Œç¼–æ’ï¼š
1. è¾“å…¥é¢„å¤„ç†èŠ‚ç‚¹
2. å±æœºæ£€æµ‹èŠ‚ç‚¹
3. ä¸Šä¸‹æ–‡æ„å»ºèŠ‚ç‚¹
4. æœç´¢èŠ‚ç‚¹ï¼ˆå¯é€‰ï¼‰
5. è®¡åˆ’æ›´æ–°èŠ‚ç‚¹
6. LLMå“åº”èŠ‚ç‚¹
7. åå¤„ç†èŠ‚ç‚¹
8. æ•°æ®ä¿å­˜èŠ‚ç‚¹

ä¼˜åŒ–ç‚¹ï¼š
- å¹¶è¡Œå¤„ç†éä¾èµ–æ“ä½œ
- æ¡ä»¶è·¯ç”±å‡å°‘ä¸å¿…è¦çš„è®¡ç®—
- ä¼˜åŒ–æ•°æ®åº“æ“ä½œæ‰¹æ¬¡
- å¼‚æ­¥å¤„ç†æœç´¢åŠŸèƒ½
"""

import os
import json
import sys
from pathlib import Path
import asyncio
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import Dict, List, Tuple, Any, Optional

sys.path.append(str(Path(__file__).parent.parent))

from datetime import datetime
import re
import logging
import requests
from dotenv import load_dotenv
from langchain_core.messages import HumanMessage
from langchain_openai import ChatOpenAI
from langgraph.graph import StateGraph, END
from pydantic import BaseModel, Field
from snownlp import SnowNLP

from utils.extract_json import extract_json
from dao.database import Database

# åˆ†ææŠ¥å‘ŠæœåŠ¡å¯¼å…¥
try:
    from service.analysis_report_service import AnalysisReportService
    ANALYSIS_REPORT_AVAILABLE = True
except ImportError:
    ANALYSIS_REPORT_AVAILABLE = False

# RAGç›¸å…³å¯¼å…¥
try:
    from rag import RAGService, IntentRouter
    from rag.langgraph_nodes import create_rag_node, create_web_search_node
    from rag.core.rag_retriever import RerankedResult
    RAG_AVAILABLE = True
except ImportError:
    RAG_AVAILABLE = False

import warnings

warnings.filterwarnings("ignore", category=UserWarning, module="pydantic")

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# åˆ›å»ºlogger
logger = logging.getLogger(__name__)


# æ‰©å±•çš„ä¼šè¯çŠ¶æ€æ•°æ®æ¨¡å‹
class OptimizedSessionState(BaseModel):
    # åŸºæœ¬ä¿¡æ¯
    user_input: str
    user_id: str = "default_user"
    session_id: str | None = None

    # å“åº”ç›¸å…³
    response: str | None = None
    emotion: str = "neutral"

    # å†å²è®°å½•
    history: List[Dict[str, str]] = Field(default_factory=list)

    # ç”¨æˆ·ç”»åƒå’Œä¸Šä¸‹æ–‡
    user_profile: Dict[str, Any] = Field(default_factory=dict)
    memory_context: str = ""

    # å±æœºæ£€æµ‹
    crisis_detected: bool = False
    crisis_reason: str | None = None

    # æœç´¢ç›¸å…³
    need_search: bool = False
    search_results: str | None = None

    # RAGç›¸å…³
    need_rag: bool = False
    rag_context: str = ""
    has_rag_context: bool = False
    
    # æ„å›¾è¯†åˆ«
    intent_result: Dict[str, Any] = Field(default_factory=dict)
    route_decision: str = "direct_chat"  # direct_chat, rag, web_search

    # è®¡åˆ’å’Œåˆ†æ
    plan: Dict[str, Any] = Field(default_factory=dict)
    pattern_analysis: Dict[str, Any] | None = None
    
    # å¼•å¯¼æ€§è¯¢é—®
    inquiry_result: Dict[str, Any] | None = None
    need_guided_inquiry: bool = False
    
    # æ¨¡å¼åˆ†æ
    need_pattern_analysis: bool = False
    
    # åˆ†ææŠ¥å‘Š
    need_analysis_report: bool = False
    analysis_report: Dict[str, Any] | None = None

    # å¤„ç†çŠ¶æ€
    processing_stage: str = "init"
    skip_search: bool = False
    skip_plan_update: bool = False

    # æ€§èƒ½ç›‘æ§
    stage_timings: Dict[str, float] = Field(default_factory=dict)
    total_start_time: float = 0.0


# ä¼˜åŒ–çš„å±æœºæ£€æµ‹å™¨
class OptimizedCrisisDetector:
    def __init__(self):
        self._keywords = {
            "high_risk": {"è‡ªæ€", "æƒ³æ­»", "æ´»ä¸ä¸‹å»", "ç»“æŸç”Ÿå‘½", "æ€äºº", "ä¼¤å®³è‡ªå·±"},
            "medium_risk": {"å—ä¸äº†", "ç»æœ›", "å´©æºƒ", "ç—›è‹¦", "æ²¡æœ‰å¸Œæœ›"},
        }
        self._sentiment_threshold = -0.3
        self._cache = {}  # ç®€å•çš„ç»“æœç¼“å­˜

    def check(self, text: str) -> Tuple[bool, str | None, str]:
        """
        æ£€æµ‹å±æœºç¨‹åº¦
        è¿”å›ï¼š(æ˜¯å¦å±æœº, åŸå› , ä¸¥é‡ç¨‹åº¦)
        """
        if text in self._cache:
            return self._cache[text]

        # æ£€æŸ¥é«˜é£é™©å…³é”®è¯
        for keyword in self._keywords["high_risk"]:
            if keyword in text:
                result = (True, f"æ£€æµ‹åˆ°é«˜å±è¯: '{keyword}'", "high")
                self._cache[text] = result
                return result

        # æ£€æŸ¥ä¸­é£é™©å…³é”®è¯
        for keyword in self._keywords["medium_risk"]:
            if keyword in text:
                result = (True, f"æ£€æµ‹åˆ°ä¸­å±è¯: '{keyword}'", "medium")
                self._cache[text] = result
                return result

        # æƒ…æ„Ÿåˆ†æ
        # try:
        #     polarity = SnowNLP(text).sentiments * 2 - 1
        #     if polarity <= self._sentiment_threshold:
        #         result = (True, f"æƒ…æ„Ÿææ€§è¿‡ä½ (polarity={polarity:.2f})", "low")
        #         self._cache[text] = result
        #         return result
        # except Exception:
        #     pass

        result = (False, None, "none")
        self._cache[text] = result
        return result


# ä¼˜åŒ–çš„æœç´¢æœåŠ¡
class OptimizedSearchService:
    def __init__(self):
        self.api_key = os.getenv("SERPAPI_KEY")
        self.search_triggers = [
            "ä»€ä¹ˆæ˜¯",
            "å¦‚ä½•",
            "ä¸ºä»€ä¹ˆ",
            "æ€ä¹ˆåŠ",
            "æœ€æ–°",
            "ç°åœ¨",
            "ä»Šå¤©",
            "æ–°é—»",
            "å¤©æ°”",
            "æŸ¥è¯¢",
            "æœç´¢",
        ]
        self.timeout = 15  # å¢åŠ è¶…æ—¶æ—¶é—´åˆ°15ç§’
        self.max_results = 3
        self.retry_count = 2  # æ·»åŠ é‡è¯•æ¬¡æ•°

    def should_search(self, text: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦éœ€è¦æœç´¢"""
        return any(trigger in text for trigger in self.search_triggers)

    async def search_async(self, query: str) -> str:
        """å¼‚æ­¥æœç´¢"""
        if not self.api_key:
            return "âš ï¸ [æœç´¢åŠŸèƒ½æœªå¯ç”¨: æœªè®¾ç½® SERPAPI_KEY]"

        try:
            loop = asyncio.get_event_loop()
            result = await loop.run_in_executor(None, self._sync_search, query)
            return result
        except Exception as e:
            return f"æœç´¢å‡ºé”™: {e}"

    def _sync_search(self, query: str) -> str:
        """åŒæ­¥æœç´¢å®ç°"""
        for attempt in range(self.retry_count):
            try:
                print(f"ğŸ” å¼€å§‹ç½‘ç»œæœç´¢ï¼ˆå°è¯• {attempt + 1}/{self.retry_count}ï¼‰ï¼ŒæŸ¥è¯¢: {query}")
                print(f"ğŸ“Š æœç´¢é…ç½®: æœ€å¤§ç»“æœæ•°={self.max_results}, è¶…æ—¶æ—¶é—´={self.timeout}ç§’")
                
                r = requests.get(
                    "https://serpapi.com/search",
                    params={
                        "q": query,
                        "api_key": self.api_key,
                        "hl": "zh-cn",
                        "num": self.max_results,
                    },
                    timeout=self.timeout,
                )
                r.raise_for_status()
                data = r.json()

                snippets = []
                organic_results = data.get("organic_results", [])
                
                print(f"âœ… æœç´¢APIè°ƒç”¨æˆåŠŸï¼Œè·å¾— {len(organic_results)} ä¸ªåŸå§‹ç»“æœ")
                
                for i, item in enumerate(organic_results[:self.max_results], 1):
                    title = item.get('title', '').strip()
                    snippet = item.get('snippet', '').strip()
                    link = item.get('link', '').strip()
                
                    
                    snippets.append(
                        f"æ ‡é¢˜: {title}\n"
                        f"æ‘˜è¦: {snippet}\n"
                        f"é“¾æ¥: {link}"
                    )

                final_result = "\n\n".join(snippets) if snippets else "æœªæ‰¾åˆ°ç›¸å…³æœç´¢ç»“æœ"
                print(f"ğŸ“‹ æœ€ç»ˆæœç´¢ç»“æœé•¿åº¦: {len(final_result)} å­—ç¬¦")
                print("=" * 60)
                print("å®Œæ•´æœç´¢ç»“æœ:")
                print(final_result)
                print("=" * 60)
                
                return final_result
                
            except requests.exceptions.Timeout as e:
                error_msg = f"æœç´¢è¶…æ—¶ (å°è¯• {attempt + 1}/{self.retry_count}): {e}"
                print(f"âš ï¸ {error_msg}")
                if attempt == self.retry_count - 1:  # æœ€åä¸€æ¬¡å°è¯•å¤±è´¥
                    return f"æœç´¢æœåŠ¡æš‚æ—¶ä¸å¯ç”¨ï¼Œè¯·ç¨åé‡è¯•ã€‚åŸå› ï¼šè¿æ¥è¶…æ—¶ï¼Œå¯èƒ½æ˜¯ç½‘ç»œçŠ¶å†µä¸ä½³æˆ–æœåŠ¡ç¹å¿™ã€‚"
                continue
            except Exception as e:
                error_msg = f"æœç´¢å‡ºé”™ (å°è¯• {attempt + 1}/{self.retry_count}): {e}"
                print(f"âŒ {error_msg}")
                if attempt == self.retry_count - 1:  # æœ€åä¸€æ¬¡å°è¯•å¤±è´¥
                    return f"æœç´¢æœåŠ¡é‡åˆ°é—®é¢˜: {e}"
                continue



# ä¼˜åŒ–çš„èŠå¤©æœåŠ¡
class OptimizedChatService:
    def __init__(self, database: Database):
        self.db = database
        self.model = os.environ.get("MODEL_NAME", "deepseek-chat")
        self.client = ChatOpenAI(
            model=self.model,
            api_key=os.environ.get("OPENAI_API_KEY"),
            base_url=os.environ.get("BASE_URL", "https://api.deepseek.com/v1"),
            temperature=float(os.environ.get("TEMPERATURE", "0.7")),
            max_tokens=int(os.environ.get("MAX_TOKENS", "1000")),
            timeout=60,  # å¢åŠ è¶…æ—¶æ—¶é—´åˆ°60ç§’ï¼Œé¿å…è®¡åˆ’æ›´æ–°è¶…æ—¶
        )

        self.enable_guided_inquiry = (
            os.environ.get("ENABLE_GUIDED_INQUIRY", "true").lower() == "true"
        )
        self.enable_pattern_analysis = (
            os.environ.get("ENABLE_PATTERN_ANALYSIS", "true").lower() == "true"
        )

        # åŠ è½½æç¤ºè¯æ¨¡æ¿
        self.prompt_template = self._load_prompt_template()
        self.planning_prompt = self._load_planning_prompt()
        self.guided_inquiry_prompt = self._load_guided_inquiry_prompt()
        self.pattern_analysis_prompt = self._load_pattern_analysis_prompt()

        # åˆ›å»ºçº¿ç¨‹æ± ç”¨äºå¹¶è¡Œå¤„ç†
        self.executor = ThreadPoolExecutor(max_workers=3)

        # åˆå§‹åŒ–åˆ†ææŠ¥å‘ŠæœåŠ¡
        self.analysis_service = None
        if ANALYSIS_REPORT_AVAILABLE:
            try:
                self.analysis_service = AnalysisReportService()
                print("   âœ… åˆ†ææŠ¥å‘ŠæœåŠ¡åˆå§‹åŒ–æˆåŠŸ")
            except Exception as e:
                print(f"   âŒ åˆ†ææŠ¥å‘ŠæœåŠ¡åˆå§‹åŒ–å¤±è´¥: {e}")

        # åˆå§‹åŒ–RAGç›¸å…³ç»„ä»¶
        self.rag_service = None
        self.intent_router = None
        self.rag_node = None
        self.web_search_node = None
        
        if RAG_AVAILABLE:
            self._initialize_rag_components()

    def _load_prompt_template(self) -> str:
        """åŠ è½½å’¨è¯¢å¸ˆæç¤ºè¯æ¨¡æ¿"""
        prompt_dir = os.path.join(os.path.dirname(__file__), "../prompt")
        os.makedirs(prompt_dir, exist_ok=True)
        prompt_file = os.path.join(prompt_dir, "counselor_prompt.txt")

        if not os.path.exists(prompt_file):
            default_prompt = """ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„å¿ƒç†å’¨è¯¢å¸ˆï¼Œåå«"çŸ¥å·±å’¨è¯¢å¸ˆ"ã€‚ä½ çš„ç›®æ ‡æ˜¯é€šè¿‡å¯¹è¯å¸®åŠ©ç”¨æˆ·è§£å†³å¿ƒç†å›°æ‰°ã€æƒ…ç»ªé—®é¢˜ï¼Œå¹¶æä¾›ä¸“ä¸šçš„å¿ƒç†æ”¯æŒã€‚

è¯·æ³¨æ„ä»¥ä¸‹æŒ‡å¯¼åŸåˆ™ï¼š
1. ä¿æŒå…±æƒ…ã€å°Šé‡å’Œæ”¯æŒçš„æ€åº¦
2. æä¾›å¾ªè¯çš„å¿ƒç†å­¦å»ºè®®
3. ä¸è¦ç»™å‡ºåŒ»ç–—è¯Šæ–­æˆ–å¤„æ–¹
4. å½“ç”¨æˆ·éœ€è¦ä¸“ä¸šåŒ»ç–—å¸®åŠ©æ—¶ï¼Œå»ºè®®ä»–ä»¬å¯»æ±‚ä¸“ä¸šåŒ»ç”Ÿçš„å¸®åŠ©
5. å›å¤è¦ç®€æ´ã€æ¸…æ™°ï¼Œæ˜“äºç”¨æˆ·ç†è§£
6. é€‚å½“ä½¿ç”¨å¼€æ”¾å¼é—®é¢˜é¼“åŠ±ç”¨æˆ·è¡¨è¾¾

ç¤ºä¾‹å›å¤æ ¼å¼ï¼š
"æˆ‘ç†è§£ä½ ç°åœ¨çš„æ„Ÿå—ã€‚è¿™ç§æƒ…å†µä¸‹ï¼Œä½ å¯ä»¥å°è¯•...
å¸Œæœ›è¿™äº›å»ºè®®å¯¹ä½ æœ‰æ‰€å¸®åŠ©ï¼"
"""
            with open(prompt_file, "w", encoding="utf-8") as f:
                f.write(default_prompt)
            return default_prompt

        with open(prompt_file, "r", encoding="utf-8") as f:
            return f.read()

    def _load_planning_prompt(self) -> str:
        """åŠ è½½è®¡åˆ’æç¤ºè¯æ¨¡æ¿"""
        prompt_dir = os.path.join(os.path.dirname(__file__), "../prompt")
        prompt_file = os.path.join(prompt_dir, "planning_prompt.txt")

        if not os.path.exists(prompt_file):
            # åˆ›å»ºé»˜è®¤çš„è®¡åˆ’æç¤ºè¯
            default_planning_prompt = """ä½ æ˜¯ä¸€ä¸ªå¯¹è¯è®¡åˆ’åˆ†æå™¨ã€‚æ ¹æ®ç”¨æˆ·çš„è¾“å…¥å’Œå†å²å¯¹è¯ï¼Œæ›´æ–°å¯¹è¯è®¡åˆ’ã€‚

è¿”å›JSONæ ¼å¼çš„è®¡åˆ’ï¼ŒåŒ…å«ä»¥ä¸‹å­—æ®µï¼š
- user_intent: ç”¨æˆ·æ„å›¾åˆ†æ
- current_state: å½“å‰å¯¹è¯çŠ¶æ€
- steps: å»ºè®®çš„å¯¹è¯æ­¥éª¤
- context: ä¸Šä¸‹æ–‡ä¿¡æ¯
- inquiry_status: å¼•å¯¼å¼è¯¢é—®çŠ¶æ€

è¯·ä¿æŒJSONæ ¼å¼çš„å®Œæ•´æ€§ã€‚"""

            with open(prompt_file, "w", encoding="utf-8") as f:
                f.write(default_planning_prompt)
            return default_planning_prompt

        with open(prompt_file, "r", encoding="utf-8") as f:
            return f.read()
    
    def _load_guided_inquiry_prompt(self) -> str:
        """åŠ è½½å¼•å¯¼æ€§è¯¢é—®æç¤ºè¯æ¨¡æ¿"""
        prompt_dir = os.path.join(os.path.dirname(__file__), "../prompt")
        os.makedirs(prompt_dir, exist_ok=True)
        prompt_file = os.path.join(prompt_dir, "guided_inquiry_prompt.txt")

        if not os.path.exists(prompt_file):
            default_prompt = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å¿ƒç†å’¨è¯¢å¸ˆåŠ©æ‰‹ï¼Œè´Ÿè´£è¯„ä¼°å¯¹è¯ä¸­çš„ä¿¡æ¯å®Œæ•´æ€§ã€‚

åŸºäºå½“å‰ä¼šè¯å†å²å’Œæœ€æ–°æ¶ˆæ¯ï¼Œè¯„ä¼°ä¿¡æ¯çš„å®Œæ•´ç¨‹åº¦ï¼Œå¹¶ç¡®å®šæ˜¯å¦éœ€è¦è¿›è¡Œå¼•å¯¼æ€§è¯¢é—®ã€‚

è¿”å›JSONæ ¼å¼çš„è¯„ä¼°ç»“æœï¼ŒåŒ…å«ä»¥ä¸‹å­—æ®µï¼š
- need_inquiry: æ˜¯å¦éœ€è¦å¼•å¯¼æ€§è¯¢é—®ï¼ˆbooleanï¼‰
- current_stage: å½“å‰å¯¹è¯é˜¶æ®µï¼ˆstringï¼‰
- information_completeness: ä¿¡æ¯å®Œæ•´åº¦ç™¾åˆ†æ¯”ï¼ˆ0-100ï¼‰
- missing_info: ç¼ºå¤±çš„é‡è¦ä¿¡æ¯åˆ—è¡¨
- suggested_questions: å»ºè®®çš„å¼•å¯¼æ€§é—®é¢˜ï¼ˆæœ€å¤š2ä¸ªï¼‰
- reason: è¯„ä¼°åŸå› 

è¯·ä¿æŒJSONæ ¼å¼çš„å®Œæ•´æ€§ã€‚"""
            with open(prompt_file, "w", encoding="utf-8") as f:
                f.write(default_prompt)
            return default_prompt

        with open(prompt_file, "r", encoding="utf-8") as f:
            return f.read()
    
    def _load_pattern_analysis_prompt(self) -> str:
        """åŠ è½½æ¨¡å¼åˆ†ææç¤ºè¯æ¨¡æ¿"""
        prompt_dir = os.path.join(os.path.dirname(__file__), "../prompt")
        os.makedirs(prompt_dir, exist_ok=True)
        prompt_file = os.path.join(prompt_dir, "pattern_analysis_prompt.txt")

        if not os.path.exists(prompt_file):
            default_prompt = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å¿ƒç†å­¦è¡Œä¸ºæ¨¡å¼åˆ†æå¸ˆã€‚

åŸºäºç”¨æˆ·çš„å¯¹è¯å†å²å’Œæ”¶é›†çš„ä¿¡æ¯ï¼Œè¿›è¡Œæ·±åº¦çš„è¡Œä¸ºæ¨¡å¼åˆ†æã€‚

è¿”å›JSONæ ¼å¼çš„åˆ†æç»“æœï¼ŒåŒ…å«ä»¥ä¸‹ç»“æ„ï¼š
{
    "pattern_analysis": {
        "trigger_patterns": {"common_triggers": [], "trigger_intensity": "", "trigger_frequency": ""},
        "cognitive_patterns": {"thinking_styles": [], "cognitive_biases": [], "core_beliefs": []},
        "emotional_patterns": {"primary_emotions": [], "emotion_regulation": "", "emotion_duration": ""},
        "behavioral_patterns": {"coping_strategies": [], "behavior_effectiveness": "", "behavior_habits": []},
        "interpersonal_patterns": {"interaction_style": "", "support_utilization": "", "social_behaviors": []},
        "resource_patterns": {"personal_strengths": [], "successful_experiences": [], "growth_potential": []}
    },
    "pattern_summary": "æ¨¡å¼æ€»ç»“",
    "key_insights": ["å…³é”®æ´å¯Ÿ1", "å…³é”®æ´å¯Ÿ2"],
    "consultation_recommendations": ["å’¨è¯¢å»ºè®®1", "å’¨è¯¢å»ºè®®2"]
}

è¯·ä¿æŒJSONæ ¼å¼çš„å®Œæ•´æ€§ã€‚"""
            with open(prompt_file, "w", encoding="utf-8") as f:
                f.write(default_prompt)
            return default_prompt

        with open(prompt_file, "r", encoding="utf-8") as f:
            return f.read()

    def batch_get_user_data(self, user_id: str) -> Dict[str, Any]:
        """æ‰¹é‡è·å–ç”¨æˆ·æ•°æ®"""
        try:
            # å¹¶è¡Œè·å–ç”¨æˆ·æ•°æ®
            futures = {
                "profile": self.executor.submit(self.db.get_user_profile, user_id),
                "memory": self.executor.submit(
                    self.db.get_long_term_memory, user_id, 5
                ),
                "emotion_history": self.executor.submit(
                    self.db.get_emotion_history, user_id
                ),
            }

            results = {}
            for key, future in futures.items():
                try:
                    results[key] = future.result(timeout=2)
                except Exception as e:
                    print(f"Error getting {key}: {e}")
                    results[key] = {} if key == "profile" else []

            return results
        except Exception as e:
            print(f"Error in batch_get_user_data: {e}")
            return {"profile": {}, "memory": [], "emotion_history": []}

    def format_memory_context(self, memories: List[Dict[str, Any]]) -> str:
        """æ ¼å¼åŒ–è®°å¿†ä¸Šä¸‹æ–‡"""
        if not memories:
            return ""
        return "\n".join(
            f"{m.get('time', '')}: {m.get('content', '')}" for m in memories
        )

    def extract_emotion(self, content: str) -> Tuple[str, str]:
        """æå–æƒ…ç»ªä¿¡æ¯"""
        # æ£€æŸ¥æ˜¾å¼æƒ…ç»ªæ ‡è®°
        emotion_match = re.search(
            r"#(happy|sad|angry|sleepy|neutral)\b", content, re.IGNORECASE
        )
        if emotion_match:
            emotion = emotion_match.group(1).lower()
            clean_content = re.sub(
                r"\s*#(happy|sad|angry|sleepy|neutral)\b",
                "",
                content,
                flags=re.IGNORECASE,
            )
            return clean_content, emotion

        # åŸºäºå…³é”®è¯çš„æƒ…ç»ªè¯†åˆ«
        content_lower = content.lower()
        emotion_keywords = {
            "happy": ["å¼€å¿ƒ", "é«˜å…´", "å¿«ä¹", "å¾ˆå¥½", "å¾ˆæ£’", "å…´å¥‹", "æ»¡æ„"],
            "sad": ["æ‚²ä¼¤", "éš¾è¿‡", "ä¼¤å¿ƒ", "ç—›è‹¦", "æŠ‘éƒ", "å¤±è½"],
            "angry": ["ç”Ÿæ°”", "æ„¤æ€’", "æ¼ç«", "çƒ¦èº", "çƒ¦æ¼", "ä¸æ»¡"],
            "sleepy": ["ç´¯äº†", "ç–²æƒ«", "å›°", "ç¡è§‰", "ä¼‘æ¯", "ç–²åŠ³"],
        }

        for emotion, keywords in emotion_keywords.items():
            if any(word in content_lower for word in keywords):
                return content, emotion

        return content, "neutral"
    
    def _assess_information_completeness(self, session_id: str, user_input: str, history: List[Dict[str, str]]) -> Dict[str, Any]:
        """è¯„ä¼°ä¿¡æ¯å®Œæ•´æ€§"""
        try:
            # æ„å»ºå†å²å¯¹è¯ä¸Šä¸‹æ–‡
            context = ""
            if history:
                recent_messages = history[-5:]  # å–æœ€è¿‘5è½®å¯¹è¯
                context = "\n".join([
                    f"ç”¨æˆ·: {msg.get('user', '')}\nå’¨è¯¢å¸ˆ: {msg.get('assistant', '')}"
                    for msg in recent_messages
                ])
            
            # æ„å»ºè¯„ä¼°æ¶ˆæ¯
            messages = [
                {"role": "system", "content": self.guided_inquiry_prompt},
                {
                    "role": "user", 
                    "content": f"å¯¹è¯å†å²:\n{context}\n\nå½“å‰æ¶ˆæ¯: {user_input}\n\nè¯·è¯„ä¼°ä¿¡æ¯å®Œæ•´æ€§å¹¶è¿”å›JSONæ ¼å¼çš„ç»“æœã€‚"
                }
            ]
            
            # è°ƒç”¨LLMè¿›è¡Œè¯„ä¼°
            response = self.client.invoke(messages)
            reply = response.content.strip()
            
            # è§£æJSONç»“æœ
            inquiry_result = extract_json(reply)
            if inquiry_result:
                return inquiry_result
            else:
                # æ‰‹åŠ¨è§£æå¤‡ç”¨æ–¹æ¡ˆ
                return self._parse_inquiry_manually(reply)
                
        except Exception as e:
            print(f"Error in _assess_information_completeness: {e}")
            return {
                "need_inquiry": False,
                "current_stage": "è¯„ä¼°å¤±è´¥",
                "information_completeness": 50,
                "missing_info": [],
                "suggested_questions": [],
                "reason": f"è¯„ä¼°å‡ºé”™: {e}"
            }
    
    def _analyze_behavior_pattern(self, session_id: str, collected_info: Dict[str, Any]) -> Dict[str, Any]:
        """åˆ†æè¡Œä¸ºæ¨¡å¼"""
        try:
            # æ„å»ºåˆ†æä¸Šä¸‹æ–‡
            history = collected_info.get("conversation_history", [])
            context = ""
            if history:
                context = "\n".join([
                    f"ç”¨æˆ·: {msg.get('user', '')}\nå’¨è¯¢å¸ˆ: {msg.get('assistant', '')}"
                    for msg in history[-10:]  # å–æœ€è¿‘10è½®å¯¹è¯
                ])
            
            # æ„å»ºåˆ†ææ¶ˆæ¯
            messages = [
                {"role": "system", "content": self.pattern_analysis_prompt},
                {
                    "role": "user",
                    "content": f"å¯¹è¯å†å²:\n{context}\n\nä¼šè¯ä¿¡æ¯: {json.dumps(collected_info, ensure_ascii=False)}\n\nè¯·è¿›è¡Œæ·±åº¦çš„è¡Œä¸ºæ¨¡å¼åˆ†æå¹¶è¿”å›JSONæ ¼å¼çš„ç»“æœã€‚"
                }
            ]
            
            # è°ƒç”¨LLMè¿›è¡Œåˆ†æ
            response = self.client.invoke(messages)
            reply = response.content.strip()
            
            # è§£æJSONç»“æœ
            pattern_result = extract_json(reply)
            if pattern_result:
                return pattern_result
            else:
                # æ‰‹åŠ¨è§£æå¤‡ç”¨æ–¹æ¡ˆ
                return self._parse_pattern_manually(reply)
                
        except Exception as e:
            print(f"Error in _analyze_behavior_pattern: {e}")
            return {
                "pattern_analysis": {
                    "trigger_patterns": {"common_triggers": [], "trigger_intensity": "æœªçŸ¥", "trigger_frequency": "æœªçŸ¥"},
                    "cognitive_patterns": {"thinking_styles": [], "cognitive_biases": [], "core_beliefs": []},
                    "emotional_patterns": {"primary_emotions": [], "emotion_regulation": "æœªçŸ¥", "emotion_duration": "æœªçŸ¥"},
                    "behavioral_patterns": {"coping_strategies": [], "behavior_effectiveness": "æœªçŸ¥", "behavior_habits": []},
                    "interpersonal_patterns": {"interaction_style": "æœªçŸ¥", "support_utilization": "æœªçŸ¥", "social_behaviors": []},
                    "resource_patterns": {"personal_strengths": [], "successful_experiences": [], "growth_potential": []}
                },
                "pattern_summary": f"æ¨¡å¼åˆ†æå¤±è´¥: {e}",
                "key_insights": ["éœ€è¦æ›´å¤šä¿¡æ¯è¿›è¡Œåˆ†æ"],
                "consultation_recommendations": ["ç»§ç»­å¯¹è¯æ”¶é›†ä¿¡æ¯"]
            }
    
    def _parse_inquiry_manually(self, text: str) -> Dict[str, Any]:
        """æ‰‹åŠ¨è§£æå¼•å¯¼æ€§è¯¢é—®ç»“æœçš„å¤‡ç”¨æ–¹æ³•"""
        try:
            result = {
                "need_inquiry": True,
                "current_stage": "åŸºç¡€æƒ…å†µäº†è§£",
                "missing_info": [],
                "suggested_questions": [],
                "information_completeness": 50,
                "reason": "æ‰‹åŠ¨è§£æç»“æœ"
            }
            
            # æŸ¥æ‰¾ä¿¡æ¯å®Œæ•´åº¦
            completeness_match = re.search(r'ä¿¡æ¯å®Œæ•´[åº¦æ€§]?[ï¼š:]\s*(\d+)%?', text)
            if completeness_match:
                result["information_completeness"] = int(completeness_match.group(1))
            
            # æŸ¥æ‰¾å½“å‰é˜¶æ®µ
            stage_match = re.search(r'å½“å‰é˜¶æ®µ[ï¼š:]\s*([^\n]+)', text)
            if stage_match:
                result["current_stage"] = stage_match.group(1).strip()
            
            # æŸ¥æ‰¾å»ºè®®çš„é—®é¢˜
            questions = re.findall(r'[12]\.?\s*([^ï¼Ÿ?]*[ï¼Ÿ?])', text)
            if questions:
                result["suggested_questions"] = [q.strip() for q in questions[:2]]
            
            # åˆ¤æ–­æ˜¯å¦éœ€è¦è¯¢é—®
            if result["information_completeness"] >= 80:
                result["need_inquiry"] = False
                result["current_stage"] = "ä¿¡æ¯å……åˆ†"
            
            return result
            
        except Exception as e:
            print(f"Manual parsing failed: {e}")
            return {
                "need_inquiry": False,
                "current_stage": "è§£æå¤±è´¥",
                "information_completeness": 50,
                "missing_info": [],
                "suggested_questions": [],
                "reason": f"è§£æé”™è¯¯: {e}"
            }
    
    def _parse_pattern_manually(self, text: str) -> Dict[str, Any]:
        """æ‰‹åŠ¨è§£æè¡Œä¸ºæ¨¡å¼åˆ†æç»“æœçš„å¤‡ç”¨æ–¹æ³•"""
        try:
            pattern_analysis = {
                "pattern_analysis": {
                    "trigger_patterns": {
                        "common_triggers": [],
                        "trigger_intensity": "ä¸­",
                        "trigger_frequency": "ç»å¸¸"
                    },
                    "cognitive_patterns": {
                        "thinking_styles": [],
                        "cognitive_biases": [],
                        "core_beliefs": []
                    },
                    "emotional_patterns": {
                        "primary_emotions": [],
                        "emotion_regulation": "éƒ¨åˆ†æœ‰æ•ˆ",
                        "emotion_duration": "ä¸­ç­‰"
                    },
                    "behavioral_patterns": {
                        "coping_strategies": [],
                        "behavior_effectiveness": "éƒ¨åˆ†æœ‰æ•ˆ",
                        "behavior_habits": []
                    },
                    "interpersonal_patterns": {
                        "interaction_style": "è¢«åŠ¨",
                        "support_utilization": "éƒ¨åˆ†",
                        "social_behaviors": []
                    },
                    "resource_patterns": {
                        "personal_strengths": [],
                        "successful_experiences": [],
                        "growth_potential": []
                    }
                },
                "pattern_summary": "åŸºäºå¯¹è¯å†…å®¹è¿›è¡Œçš„æ‰‹åŠ¨æ¨¡å¼åˆ†æ",
                "key_insights": ["éœ€è¦è¿›ä¸€æ­¥åˆ†æ", "æ¨¡å¼è¯†åˆ«ä¸­", "æŒç»­å…³æ³¨"],
                "consultation_recommendations": ["ä¿æŒå¼€æ”¾æ²Ÿé€š", "å»ºç«‹ä¿¡ä»»å…³ç³»", "é€æ­¥æ·±å…¥äº†è§£"]
            }
            
            return pattern_analysis
            
        except Exception as e:
            print(f"Manual pattern parsing failed: {e}")
            return {
                "pattern_analysis": {
                    "trigger_patterns": {"common_triggers": [], "trigger_intensity": "æœªçŸ¥", "trigger_frequency": "æœªçŸ¥"},
                    "cognitive_patterns": {"thinking_styles": [], "cognitive_biases": [], "core_beliefs": []},
                    "emotional_patterns": {"primary_emotions": [], "emotion_regulation": "æœªçŸ¥", "emotion_duration": "æœªçŸ¥"},
                    "behavioral_patterns": {"coping_strategies": [], "behavior_effectiveness": "æœªçŸ¥", "behavior_habits": []},
                    "interpersonal_patterns": {"interaction_style": "æœªçŸ¥", "support_utilization": "æœªçŸ¥", "social_behaviors": []},
                    "resource_patterns": {"personal_strengths": [], "successful_experiences": [], "growth_potential": []}
                },
                "pattern_summary": f"æ¨¡å¼åˆ†æè§£æå¤±è´¥: {e}",
                "key_insights": ["éœ€è¦é‡æ–°åˆ†æ"],
                "consultation_recommendations": ["ç»§ç»­æ”¶é›†ä¿¡æ¯"]
            }
    
    def _initialize_rag_components(self):
        """åˆå§‹åŒ–RAGç›¸å…³ç»„ä»¶"""
        try:
            # æ£€æŸ¥RAGæ˜¯å¦å¯ç”¨
            rag_enabled = os.environ.get("ENABLE_RAG", "true").lower() == "true"
            if not rag_enabled:
                print("   â„¹ï¸ RAGåŠŸèƒ½å·²ç¦ç”¨")
                return
            
            # å°è¯•ä»start.pyæ¨¡å—è·å–RAGæœåŠ¡
            start_module = sys.modules.get('start')
            main_module = sys.modules.get('__main__')
            
            if start_module and hasattr(start_module, 'rag_service'):
                self.rag_service = start_module.rag_service
                print("   âœ… ä½¿ç”¨å…¨å±€RAGæœåŠ¡")
                logger.info("ä»startæ¨¡å—è·å–RAGæœåŠ¡")
            elif main_module and hasattr(main_module, 'rag_service'):
                self.rag_service = main_module.rag_service
                print("   âœ… ä½¿ç”¨å…¨å±€RAGæœåŠ¡")
                logger.info("ä»mainæ¨¡å—è·å–RAGæœåŠ¡")
            else:
                # å¦‚æœè·å–ä¸åˆ°ï¼Œåˆ›å»ºæ–°çš„RAGæœåŠ¡ï¼ˆè¿™é€šå¸¸å‘ç”Ÿåœ¨æµ‹è¯•ç¯å¢ƒä¸­ï¼‰
                print("   âš ï¸ æœªæ‰¾åˆ°å…¨å±€RAGæœåŠ¡ï¼Œåˆ›å»ºæ–°å®ä¾‹")
                logger.warning("æœªæ‰¾åˆ°å…¨å±€RAGæœåŠ¡ï¼Œåˆ›å»ºæ–°çš„å®ä¾‹")
                from rag.core.rag_service import RAGCoreService
                
                knowledge_source_dir = str(Path(__file__).parent.parent / "knowledge_source")
                data_dir = str(Path(__file__).parent.parent / "data")
                embedding_model_path = str(Path(__file__).parent.parent / "qwen_embeddings")
                rerank_model_path = str(Path(__file__).parent.parent / "qwen_reranker")
                
                self.rag_service = RAGCoreService(
                    knowledge_source_dir=knowledge_source_dir,
                    data_dir=data_dir,
                    embedding_model_path=embedding_model_path,
                    rerank_model_path=rerank_model_path,
                    device="auto"
                )
                
                # åˆå§‹åŒ–æœåŠ¡
                success = self.rag_service.initialize()
                if success:
                    print("   âœ… æ–°RAGå®ä¾‹åˆå§‹åŒ–æˆåŠŸ")
                else:
                    print("   âŒ æ–°RAGå®ä¾‹åˆå§‹åŒ–å¤±è´¥")
            
            # åˆå§‹åŒ–æ„å›¾è·¯ç”±å™¨
            self.intent_router = IntentRouter(self.client)
            
            # åˆå§‹åŒ–webæœç´¢èŠ‚ç‚¹
            search_enabled = bool(os.getenv("SERPAPI_KEY"))
            self.web_search_node = create_web_search_node(search_enabled)
            if search_enabled:
                print("   âœ… ç½‘ç»œæœç´¢ç»„ä»¶å°±ç»ª")
            else:
                print("   âš ï¸ ç½‘ç»œæœç´¢ç»„ä»¶æœªå¯ç”¨: æœªè®¾ç½®SERPAPI_KEY")
            
        except Exception as e:
            print(f"   âŒ RAGç»„ä»¶åˆå§‹åŒ–å¤±è´¥: {e}")
            logger.error(f"RAGç»„ä»¶åˆå§‹åŒ–å¤±è´¥: {e}")
            self.rag_service = None
            self.intent_router = None
            self.web_search_node = None


# å…¨å±€æœåŠ¡å®ä¾‹ï¼ˆå»¶è¿Ÿåˆå§‹åŒ–ï¼‰
_db_instance = None
_crisis_detector_instance = None
_search_service_instance = None
_chat_service_instance = None

def get_db_instance():
    global _db_instance
    if _db_instance is None:
        _db_instance = Database()
    return _db_instance

def get_crisis_detector_instance():
    global _crisis_detector_instance
    if _crisis_detector_instance is None:
        _crisis_detector_instance = OptimizedCrisisDetector()
    return _crisis_detector_instance

def get_search_service_instance():
    global _search_service_instance
    if _search_service_instance is None:
        _search_service_instance = OptimizedSearchService()
    return _search_service_instance

def get_chat_service_instance():
    global _chat_service_instance
    if _chat_service_instance is None:
        _chat_service_instance = OptimizedChatService(get_db_instance())
    return _chat_service_instance

# === LangGraph èŠ‚ç‚¹å‡½æ•° ===

"""è¾“å…¥é¢„å¤„ç†èŠ‚ç‚¹"""
def preprocess_input(state: OptimizedSessionState) -> OptimizedSessionState:
    start_time = datetime.now().timestamp()
    state.total_start_time = start_time

    # åŸºæœ¬ä¿¡æ¯è®¾ç½®
    state.user_input = state.user_input.strip()
    state.session_id = state.session_id or state.user_id
    state.processing_stage = "preprocessed"

    # åˆ¤æ–­æ˜¯å¦éœ€è¦æœç´¢
    search_service = get_search_service_instance()
    state.need_search = search_service.should_search(state.user_input)

    # è®°å½•æ—¶é—´
    state.stage_timings["preprocess"] = datetime.now().timestamp() - start_time

    return state


"""å±æœºæ£€æµ‹èŠ‚ç‚¹"""
def detect_crisis(state: OptimizedSessionState) -> OptimizedSessionState:
    start_time = datetime.now().timestamp()

    crisis_detector = get_crisis_detector_instance()
    crisis, reason, severity = crisis_detector.check(state.user_input)
    state.crisis_detected = crisis
    state.crisis_reason = reason

    if crisis:
        # æ ¹æ®ä¸¥é‡ç¨‹åº¦ç”Ÿæˆä¸åŒçš„å“åº”
        if severity == "high":
            state.response = (
                f"âš ï¸ æ£€æµ‹åˆ°é«˜å±æƒ…ç»ªå±æœº: {reason}\n\n"
                "è¯·ç«‹å³è”ç³»ä¸“ä¸šäººå£«æˆ–æ‹¨æ‰“å¿ƒç†æ´åŠ©çƒ­çº¿ 400-161-9995ã€‚\n"
                "æ‚¨çš„ç”Ÿå‘½å¾ˆå®è´µï¼Œè¯·å¯»æ±‚å¸®åŠ©ã€‚æˆ‘ä»¬å…³å¿ƒæ‚¨ã€‚"
            )
        elif severity == "medium":
            state.response = (
                f"âš ï¸ æ£€æµ‹åˆ°æƒ…ç»ªå›°æ‰°: {reason}\n\n"
                "æˆ‘ç†è§£æ‚¨ç°åœ¨çš„æ„Ÿå—å¾ˆå›°éš¾ã€‚è®©æˆ‘ä»¬ä¸€èµ·é¢å¯¹è¿™ä¸ªæŒ‘æˆ˜ã€‚\n"
                "å¦‚æœéœ€è¦ï¼Œä¹Ÿå¯ä»¥è”ç³»ä¸“ä¸šå¿ƒç†æ´åŠ©çƒ­çº¿ 400-161-9995ã€‚"
            )
        else:
            state.response = (
                f"æˆ‘æ³¨æ„åˆ°æ‚¨å¯èƒ½æƒ…ç»ªä¸å¤ªå¥½: {reason}\n\n"
                "æˆ‘åœ¨è¿™é‡Œé™ªä¼´æ‚¨ï¼Œè®©æˆ‘ä»¬ä¸€èµ·èŠèŠå§ã€‚"
            )

        state.processing_stage = "crisis_handled"
        # ä¿å­˜å±æœºè®°å½•
        chat_service = get_chat_service_instance()
        chat_service.db.save_long_term_memory(
            state.user_id, f"[CRISIS-{severity.upper()}] {state.user_input} â€“ {reason}"
        )

    state.stage_timings["crisis_detection"] = datetime.now().timestamp() - start_time
    return state


"""æ„å›¾è¯†åˆ«èŠ‚ç‚¹"""
def intent_analysis(state: OptimizedSessionState) -> OptimizedSessionState:
    start_time = datetime.now().timestamp()
    
    try:
        # å¦‚æœå±æœºå·²æ£€æµ‹åˆ°ï¼Œè·³è¿‡æ„å›¾åˆ†æ
        if state.crisis_detected:
            state.route_decision = "direct_chat"
            state.need_rag = False
            state.need_search = False
            state.stage_timings["intent_analysis"] = datetime.now().timestamp() - start_time
            return state
        
        # è·å–èŠå¤©æœåŠ¡å®ä¾‹è¿›è¡Œæ„å›¾åˆ†æ
        chat_service = get_chat_service_instance()
        
        if chat_service and chat_service.intent_router:
            # æ„å»ºå¯¹è¯ä¸Šä¸‹æ–‡
            context = ""
            if state.history:
                recent_messages = state.history[-3:]  # å–æœ€è¿‘3è½®å¯¹è¯
                context = "\n".join([
                    f"ç”¨æˆ·: {msg.get('user', '')}\nå’¨è¯¢å¸ˆ: {msg.get('assistant', '')}"
                    for msg in recent_messages
                ])
            
            # æ‰§è¡Œæ„å›¾è¯†åˆ«
            routing_decision = chat_service.intent_router.get_routing_decision(
                state.user_input, context
            )
            
            state.intent_result = routing_decision['intent']
            state.route_decision = routing_decision['route']
            
            # è®¾ç½®è·¯ç”±æ ‡å¿—
            if state.route_decision == "rag":
                state.need_rag = True
                state.need_search = False
            elif state.route_decision == "web_search":
                state.need_rag = False
                state.need_search = True
            else:
                state.need_rag = False
                state.need_search = False
                
            print(f"æ„å›¾è¯†åˆ«ç»“æœ: è·¯ç”±åˆ° {state.route_decision}")
            
        else:
            # å¦‚æœRAGä¸å¯ç”¨ï¼Œä½¿ç”¨ç®€å•çš„å…³é”®è¯åˆ¤æ–­
            from rag.intent_router import SimpleIntentRouter
            simple_router = SimpleIntentRouter()
            
            state.need_rag = simple_router.should_use_rag(state.user_input)
            state.need_search = simple_router.should_use_web_search(state.user_input)
            
            if state.need_rag:
                state.route_decision = "rag"
            elif state.need_search:
                state.route_decision = "web_search"
            else:
                state.route_decision = "direct_chat"
                
    except Exception as e:
        print(f"æ„å›¾è¯†åˆ«å¤±è´¥: {e}")
        state.route_decision = "direct_chat"
        state.need_rag = False
        state.need_search = False
    
    state.stage_timings["intent_analysis"] = datetime.now().timestamp() - start_time
    return state


"""RAGæ£€ç´¢èŠ‚ç‚¹"""
def rag_retrieval(state: OptimizedSessionState) -> OptimizedSessionState:
    start_time = datetime.now().timestamp()
    
    try:
        chat_service = get_chat_service_instance()
        
        if chat_service and chat_service.rag_service:
            # ç›´æ¥ä½¿ç”¨RAGæœåŠ¡è¿›è¡Œæ£€ç´¢ï¼Œæ˜¾ç¤ºè¯¦ç»†çš„ç²—æ’å’Œç²¾æ’è¿‡ç¨‹
            logger.info(f"å¼€å§‹RAGæ£€ç´¢: {state.user_input[:50]}...")
            print(f"ğŸ” å¼€å§‹RAGæ£€ç´¢ï¼ŒæŸ¥è¯¢: {state.user_input}")
            
            # æ‰§è¡Œè¯¦ç»†çš„RAGæœç´¢ï¼Œè·å–ç²—æ’å’Œç²¾æ’ç»“æœ
            retriever = chat_service.rag_service.retriever
            
            # 1. ç²—æ’é˜¶æ®µï¼šå‘é‡ç›¸ä¼¼åº¦æœç´¢ï¼Œè·å–æ›´å¤šå€™é€‰
            print("\nğŸ“Š ç¬¬ä¸€é˜¶æ®µï¼šç²—æ’ (å‘é‡ç›¸ä¼¼åº¦æœç´¢)")
            vector_results = chat_service.rag_service.vector_store.search(
                state.user_input, 
                top_k=6  # è·å–æ›´å¤šå€™é€‰ç”¨äºå±•ç¤ºç²—æ’æ•ˆæœ
            )
            
            if vector_results:
                print(f"   âœ… ç²—æ’å®Œæˆï¼Œè·å¾— {len(vector_results)} ä¸ªå€™é€‰æ–‡æ¡£")
                for i, result in enumerate(vector_results, 1):
                    print(f"   [{i}] ç›¸ä¼¼åº¦: {result.score:.4f} | æ¥æº: {result.chunk.source_file}")
                    print(f"       å†…å®¹é¢„è§ˆ: {result.chunk.content[:80]}...")
            else:
                print("   âŒ ç²—æ’æœªæ‰¾åˆ°ç›¸å…³æ–‡æ¡£")
                state.rag_context = ""
                state.has_rag_context = False
                return state
            
            # 2. ç²¾æ’é˜¶æ®µï¼šä½¿ç”¨é‡æ’åºæ¨¡å‹
            print(f"\nğŸ¯ ç¬¬äºŒé˜¶æ®µï¼šç²¾æ’ (é‡æ’åºæ¨¡å‹)")
            if retriever.rerank_enabled:
                print("   âš¡ ä½¿ç”¨Qwené‡æ’åºæ¨¡å‹è¿›è¡Œç²¾æ’...")
                reranked_results = retriever.search(
                    state.user_input, 
                    top_k=3, 
                    use_rerank=True,
                    rerank_top_k=len(vector_results)
                )
                
                if reranked_results:
                    print(f"   âœ… ç²¾æ’å®Œæˆï¼Œæœ€ç»ˆé€‰æ‹© {len(reranked_results)} ä¸ªæœ€ç›¸å…³æ–‡æ¡£")
                    for result in reranked_results:
                        rerank_str = f"é‡æ’åº: {result.rerank_score:.4f}" if result.rerank_score is not None else "æœªé‡æ’åº"
                        print(f"   [TOP{result.final_rank}] ç›¸ä¼¼åº¦: {result.similarity_score:.4f} | {rerank_str}")
                        print(f"           æ¥æº: {result.chunk.source_file}")
                        print(f"           å†…å®¹: {result.chunk.content[:60]}...")
                else:
                    print("   âŒ ç²¾æ’å¤„ç†å¤±è´¥")
                    reranked_results = []
            else:
                print("   âš ï¸ é‡æ’åºæ¨¡å‹æœªå¯ç”¨ï¼Œä½¿ç”¨ç²—æ’ç»“æœ")
                reranked_results = []
                for i, result in enumerate(vector_results[:3]):
                    from rag.core.rag_retriever import RerankedResult
                    reranked_results.append(RerankedResult(
                        chunk=result.chunk,
                        similarity_score=result.score,
                        rerank_score=None,
                        final_rank=i + 1
                    ))
            
            # 3. ç”Ÿæˆæœ€ç»ˆä¸Šä¸‹æ–‡
            if reranked_results:
                context_parts = []
                for i, result in enumerate(reranked_results, 1):
                    context_parts.append(f"[æ–‡æ¡£{i}] æ¥æº: {result.chunk.source_file}")
                    context_parts.append(result.chunk.content)
                    context_parts.append("")  # ç©ºè¡Œåˆ†éš”
                
                context = "\n".join(context_parts)
                
                logger.info(f"RAGæ£€ç´¢æˆåŠŸï¼Œè·å¾— {len(context)} å­—ç¬¦çš„ä¸Šä¸‹æ–‡")
                state.rag_context = context
                state.has_rag_context = True
                print(f"\nğŸ“‹ ä¸Šä¸‹æ–‡ç”Ÿæˆå®Œæˆ: {len(context)} å­—ç¬¦")
                
                # 4. æ˜¾ç¤ºå‘é€ç»™æ¨¡å‹çš„å®Œæ•´prompt
                print(f"\nğŸ’¬ å‘é€ç»™æ¨¡å‹çš„å®Œæ•´prompt:")
                print("=" * 80)
                
                # æ„å»ºå®Œæ•´çš„promptï¼ˆæ¨¡æ‹Ÿå®é™…å‘é€ç»™æ¨¡å‹çš„å†…å®¹ï¼‰
                system_context = "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å¿ƒç†å¥åº·åŠ©æ‰‹ï¼ŒåŸºäºæä¾›çš„å‚è€ƒæ–‡æ¡£æ¥å›ç­”ç”¨æˆ·é—®é¢˜ã€‚"
                full_prompt = f"""ç³»ç»Ÿè§’è‰²: {system_context}

å‚è€ƒæ–‡æ¡£:
{context}

ç”¨æˆ·é—®é¢˜: {state.user_input}

è¯·åŸºäºä¸Šè¿°å‚è€ƒæ–‡æ¡£ï¼Œä¸ºç”¨æˆ·æä¾›ä¸“ä¸šã€å‡†ç¡®çš„å›ç­”ã€‚å¦‚æœå‚è€ƒæ–‡æ¡£ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·è¯šå®è¯´æ˜ã€‚"""
                
                print(full_prompt)
                print("=" * 80)
                
            else:
                logger.warning("RAGæ£€ç´¢æœªæ‰¾åˆ°ç›¸å…³å†…å®¹")
                state.rag_context = ""
                state.has_rag_context = False
                print("RAGæ£€ç´¢æœªæ‰¾åˆ°ç›¸å…³å†…å®¹")
        else:
            print("RAGæœåŠ¡ä¸å¯ç”¨")
            state.rag_context = ""
            state.has_rag_context = False
            
    except Exception as e:
        print(f"RAGæ£€ç´¢å¤±è´¥: {e}")
        logger.error(f"RAGæ£€ç´¢å¤±è´¥: {e}")
        state.rag_context = ""
        state.has_rag_context = False
    
    state.stage_timings["rag_retrieval"] = datetime.now().timestamp() - start_time
    return state


"""ç½‘ç»œæœç´¢æ£€ç´¢èŠ‚ç‚¹"""
def web_search_retrieval(state: OptimizedSessionState) -> OptimizedSessionState:
    """ç½‘ç»œæœç´¢æ£€ç´¢èŠ‚ç‚¹"""
    start_time = datetime.now().timestamp()
    
    try:
        chat_service = get_chat_service_instance()
        search_service = get_search_service_instance()
        
        print(f"ğŸŒ è¿›å…¥ç½‘ç»œæœç´¢èŠ‚ç‚¹ï¼ŒæŸ¥è¯¢: {state.user_input}")
        
        # é¦–å…ˆå°è¯•ä½¿ç”¨å…¨å±€æœç´¢æœåŠ¡
        if search_service and search_service.api_key:
            print("âœ… ä½¿ç”¨å…¨å±€æœç´¢æœåŠ¡è¿›è¡Œç½‘ç»œæœç´¢")
            # ä½¿ç”¨åŒæ­¥æœç´¢
            search_results = search_service._sync_search(state.user_input)
            state.search_results = search_results
            print(f"ğŸ¯ ç½‘ç»œæœç´¢å®Œæˆï¼Œç»“æœé•¿åº¦: {len(search_results)} å­—ç¬¦")
        elif chat_service and chat_service.web_search_node:
            print("ğŸ”„ ä½¿ç”¨å¤‡ç”¨ç½‘ç»œæœç´¢èŠ‚ç‚¹")
            # ä½¿ç”¨ç½‘ç»œæœç´¢èŠ‚ç‚¹
            state_dict = {
                "user_input": state.user_input,
                "need_web_search": True,
                "intent_result": {}
            }
            result_dict = chat_service.web_search_node(state_dict)
            state.search_results = result_dict.get("search_results", "")
            print(f"ğŸ¯ ç½‘ç»œæœç´¢å®Œæˆï¼Œç»“æœé•¿åº¦: {len(state.search_results or '')} å­—ç¬¦")
        else:
            print("âŒ ç½‘ç»œæœç´¢æœåŠ¡ä¸å¯ç”¨: æœªè®¾ç½®SERPAPI_KEYæˆ–æœç´¢æœåŠ¡æœªåˆå§‹åŒ–")
            state.search_results = "âš ï¸ [æœç´¢åŠŸèƒ½æœªå¯ç”¨: æœªè®¾ç½® SERPAPI_KEY]"
            
    except Exception as e:
        print(f"âŒ ç½‘ç»œæœç´¢å¤±è´¥: {e}")
        state.search_results = f"æœç´¢å‡ºé”™: {e}"
    
    state.stage_timings["web_search"] = datetime.now().timestamp() - start_time
    return state


"""ä¸Šä¸‹æ–‡æ„å»ºèŠ‚ç‚¹"""
def build_context(state: OptimizedSessionState) -> OptimizedSessionState:
    start_time = datetime.now().timestamp()

    # æ‰¹é‡è·å–ç”¨æˆ·æ•°æ®
    chat_service = get_chat_service_instance()
    user_data = chat_service.batch_get_user_data(state.user_id)

    state.user_profile = user_data.get("profile", {})
    state.memory_context = chat_service.format_memory_context(
        user_data.get("memory", [])
    )

    state.processing_stage = "context_built"
    state.stage_timings["context_building"] = datetime.now().timestamp() - start_time

    return state


"""æ›´æ–°å¯¹è¯è®¡åˆ’èŠ‚ç‚¹"""
def update_plan(state: OptimizedSessionState) -> OptimizedSessionState:
    start_time = datetime.now().timestamp()

    if state.skip_plan_update:
        state.stage_timings["plan_update"] = 0
        return state

    # æ£€æµ‹çŸ¥å·±æŠ¥å‘Šè¯·æ±‚
    report_keywords = [
        "çŸ¥å·±æŠ¥å‘Š", "çŸ¥å·±åˆ†ææŠ¥å‘Š", "ç”ŸæˆæŠ¥å‘Š", "æˆ‘çš„æŠ¥å‘Š", 
        "å¿ƒç†åˆ†ææŠ¥å‘Š", "åˆ†ææŠ¥å‘Š", "çŸ¥å·±æŠ¥å‘Šç”Ÿæˆ"
    ]
    
    user_input_lower = state.user_input.lower()
    is_report_request = any(keyword in user_input_lower for keyword in report_keywords)
    
    if is_report_request:
        print(f"ğŸ” æ£€æµ‹åˆ°çŸ¥å·±æŠ¥å‘Šè¯·æ±‚: {state.user_input}")
        # ç›´æ¥æ ‡è®°éœ€è¦ç”Ÿæˆåˆ†ææŠ¥å‘Š
        state.need_analysis_report = True
        # è·³è¿‡å¸¸è§„çš„è®¡åˆ’æ›´æ–°æµç¨‹ï¼Œç›´æ¥è¿›è¡ŒæŠ¥å‘Šç”Ÿæˆ
        state.processing_stage = "report_requested"
        print("âœ… å·²è®¾ç½®åˆ†ææŠ¥å‘Šç”Ÿæˆæ ‡å¿—")
        state.stage_timings["plan_update"] = datetime.now().timestamp() - start_time
        return state

    retry_count = 2  # æ·»åŠ é‡è¯•æœºåˆ¶
    
    for attempt in range(retry_count):
        try:
            # è·å–æˆ–åˆ›å»ºè®¡åˆ’
            chat_service = get_chat_service_instance()
            plan = chat_service.db.get_session_plan(state.session_id)
            if not plan:
                plan = {
                    "session_id": state.session_id,
                    "user_intent": {
                        "type": "unknown",
                        "description": "",
                        "confidence": 0.0,
                        "identified_at": datetime.now().isoformat(),
                    },
                    "current_state": {
                        "stage": "intent_identification",
                        "progress": 0.0,
                        "last_updated": datetime.now().isoformat(),
                    },
                    "steps": [],
                    "context": {"key_points": [], "emotions": [], "concerns": []},
                    "inquiry_status": {
                        "stage": "åˆå§‹é˜¶æ®µ",
                        "information_completeness": 0,
                        "collected_info": {},
                        "pattern_analyzed": False,
                    },
                }

            # æ„å»ºæ¶ˆæ¯
            messages = [
                {"role": "system", "content": chat_service.planning_prompt},
                {
                    "role": "user",
                    "content": f"Current plan: {json.dumps(plan, ensure_ascii=False)}\n\n"
                    f"Current message: {state.user_input}\n\n"
                    f"History: {json.dumps(state.history, ensure_ascii=False)}",
                },
            ]

            print(f"ğŸ”„ è®¡åˆ’æ›´æ–° (å°è¯• {attempt + 1}/{retry_count})")
            
            # è°ƒç”¨LLMæ›´æ–°è®¡åˆ’
            response = chat_service.client.invoke(messages)
            reply = response.content.strip()

            # è§£ææ›´æ–°åçš„è®¡åˆ’
            updated_plan = extract_json(reply)
            if updated_plan:
                if "inquiry_status" not in updated_plan:
                    updated_plan["inquiry_status"] = plan.get(
                        "inquiry_status",
                        {
                            "stage": "åˆå§‹é˜¶æ®µ",
                            "information_completeness": 0,
                            "collected_info": {},
                            "pattern_analyzed": False,
                        },
                    )

                chat_service.db.save_session_plan(state.session_id, updated_plan)
                state.plan = updated_plan
            else:
                plan["current_state"]["last_updated"] = datetime.now().isoformat()
                state.plan = plan

            print("âœ… è®¡åˆ’æ›´æ–°æˆåŠŸ")
            break  # æˆåŠŸåˆ™é€€å‡ºé‡è¯•å¾ªç¯
            
        except Exception as e:
            error_msg = f"è®¡åˆ’æ›´æ–°å¤±è´¥ (å°è¯• {attempt + 1}/{retry_count}): {e}"
            print(f"âŒ {error_msg}")
            
            if attempt == retry_count - 1:  # æœ€åä¸€æ¬¡å°è¯•å¤±è´¥
                print("âš ï¸ è®¡åˆ’æ›´æ–°å½»åº•å¤±è´¥ï¼Œä½¿ç”¨åŸºç¡€è®¡åˆ’")
                state.plan = {
                    "session_id": state.session_id,
                    "current_state": {
                        "stage": "åŸºç¡€å¯¹è¯",
                        "last_updated": datetime.now().isoformat(),
                    },
                    "inquiry_status": {
                        "stage": "åˆå§‹é˜¶æ®µ",
                        "information_completeness": 0,
                        "collected_info": {},
                        "pattern_analyzed": False,
                    },
                }
            else:
                continue  # é‡è¯•

    state.processing_stage = "plan_updated"
    state.stage_timings["plan_update"] = datetime.now().timestamp() - start_time

    return state


"""å¼•å¯¼æ€§è¯¢é—®è¯„ä¼°èŠ‚ç‚¹"""
def guided_inquiry_assessment(state: OptimizedSessionState) -> OptimizedSessionState:
    start_time = datetime.now().timestamp()
    
    # è·å–èŠå¤©æœåŠ¡å®ä¾‹
    chat_service = get_chat_service_instance()
    
    # åˆ¤æ–­æ˜¯å¦éœ€è¦è¿›è¡Œå¼•å¯¼æ€§è¯¢é—®
    if (chat_service.enable_guided_inquiry and 
        len(state.history) <= 10 and 
        not state.plan.get("inquiry_status", {}).get("pattern_analyzed", False)):
        
        state.need_guided_inquiry = True
        
        # è¯„ä¼°ä¿¡æ¯å®Œæ•´æ€§
        inquiry_result = chat_service._assess_information_completeness(
            state.session_id, state.user_input, state.history
        )
        
        state.inquiry_result = inquiry_result
        
        # æ›´æ–°è®¡åˆ’ä¸­çš„è¯¢é—®çŠ¶æ€
        if not state.plan.get("inquiry_status"):
            state.plan["inquiry_status"] = {
                "stage": "åˆå§‹é˜¶æ®µ",
                "information_completeness": 0,
                "collected_info": {},
                "pattern_analyzed": False
            }
        
        state.plan["inquiry_status"]["information_completeness"] = inquiry_result.get("information_completeness", 0)
        state.plan["inquiry_status"]["stage"] = inquiry_result.get("current_stage", "åˆå§‹é˜¶æ®µ")
        
        # ä¿å­˜å¼•å¯¼æ€§è¯¢é—®ç»“æœ
        chat_service.db.save_inquiry_result(state.session_id, inquiry_result)
        
        # ä¿å­˜å¼•å¯¼æ€§è¯¢é—®å†å²è®°å½•
        chat_service.db.save_inquiry_history(state.session_id, inquiry_result)
        
        print(f"Information completeness: {inquiry_result.get('information_completeness', 0)}%")
        
        # åˆ¤æ–­æ˜¯å¦éœ€è¦è¿›è¡Œæ¨¡å¼åˆ†æ
        should_analyze = (
            chat_service.enable_pattern_analysis and
            (inquiry_result.get("information_completeness", 0) >= 80 or 
             len(state.history) >= 4)  # 4è½®å¯¹è¯åå¼ºåˆ¶åˆ†æ
        )
        
        if should_analyze and not state.plan["inquiry_status"]["pattern_analyzed"]:
            state.need_pattern_analysis = True
            
    elif not chat_service.enable_guided_inquiry:
        print("Guided inquiry is disabled by configuration.")
        
        # å³ä½¿å¼•å¯¼æ€§è¯¢é—®è¢«ç¦ç”¨ï¼Œä»ç„¶æ£€æŸ¥æ˜¯å¦éœ€è¦è¿›è¡Œæ¨¡å¼åˆ†æ
        if (chat_service.enable_pattern_analysis and
            len(state.history) >= 4 and
            not state.plan.get("inquiry_status", {}).get("pattern_analyzed", False)):
            state.need_pattern_analysis = True
    
    state.processing_stage = "inquiry_assessed"
    state.stage_timings["guided_inquiry"] = datetime.now().timestamp() - start_time
    
    return state


"""ç”¨æˆ·æ¨¡å¼åˆ†æèŠ‚ç‚¹"""
def pattern_analysis(state: OptimizedSessionState) -> OptimizedSessionState:
    start_time = datetime.now().timestamp()
    
    # è·å–èŠå¤©æœåŠ¡å®ä¾‹
    chat_service = get_chat_service_instance()
    
    if state.need_pattern_analysis:
        print("Triggering behavior pattern analysis...")
        
        # æ”¶é›†æ‰€æœ‰å¯¹è¯ä¿¡æ¯ç”¨äºæ¨¡å¼åˆ†æ
        collected_info = {
            "session_id": state.session_id,
            "conversation_history": state.history + [{"user": state.user_input, "role": "user"}],
            "plan_context": state.plan.get("context", {}),
            "inquiry_stage": state.inquiry_result.get("current_stage", "ä¿¡æ¯å……åˆ†") if state.inquiry_result else "ä¿¡æ¯å……åˆ†",
            "inquiry_result": state.inquiry_result
        }
        
        pattern_analysis_result = chat_service._analyze_behavior_pattern(state.session_id, collected_info)
        
        if pattern_analysis_result:
            state.pattern_analysis = pattern_analysis_result
            
            # æ›´æ–°è®¡åˆ’çŠ¶æ€
            if not state.plan.get("inquiry_status"):
                state.plan["inquiry_status"] = {
                    "stage": "åˆå§‹é˜¶æ®µ",
                    "information_completeness": 0,
                    "collected_info": {},
                    "pattern_analyzed": False
                }
            
            state.plan["inquiry_status"]["pattern_analyzed"] = True
            state.plan["inquiry_status"]["pattern_analysis_completed_at"] = datetime.now().isoformat()
            
            # ä¿å­˜æ¨¡å¼åˆ†æç»“æœåˆ°æ•°æ®åº“
            chat_service.db.save_pattern_analysis(state.session_id, pattern_analysis_result)
            
            # ä¿å­˜æ›´æ–°åçš„ä¼šè¯è®¡åˆ’
            chat_service.db.save_session_plan(state.session_id, state.plan)
            
            print("Behavior pattern analysis completed and saved.")
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦ç”Ÿæˆåˆ†ææŠ¥å‘Š
            # å½“æ¨¡å¼åˆ†æå®Œæˆä¸”ä¿¡æ¯å®Œæ•´åº¦è¾ƒé«˜æ—¶ï¼Œå¯ä»¥ç”ŸæˆæŠ¥å‘Š
            if (chat_service.analysis_service and 
                state.inquiry_result and 
                state.inquiry_result.get("information_completeness", 0) >= 80):
                state.need_analysis_report = True
        else:
            print("Behavior pattern analysis failed.")
    
    state.processing_stage = "pattern_analyzed"
    state.stage_timings["pattern_analysis"] = datetime.now().timestamp() - start_time
    
    return state


def _generate_basic_report(state: OptimizedSessionState) -> str:
    """ç”ŸæˆåŸºç¡€æŠ¥å‘Šï¼ˆå½“è¯¦ç»†æŠ¥å‘Šæ— æ³•ç”Ÿæˆæ—¶ä½¿ç”¨ï¼‰"""
    chat_service = get_chat_service_instance()
    
    # å°è¯•è·å–åŸºç¡€ç»Ÿè®¡æ•°æ®
    try:
        user_sessions = chat_service.db.get_user_sessions(state.user_id) if chat_service.db else []
        session_count = len(user_sessions)
        
        # è®¡ç®—æ€»æ¶ˆæ¯æ•°
        total_messages = 0
        for session in user_sessions:
            try:
                count = chat_service.db.get_user_message_count(session)
                total_messages += count
            except:
                continue
        
        # å°è¯•è·å–æœ€è¿‘çš„äº‹ä»¶
        recent_events = []
        if user_sessions:
            for session in user_sessions[-3:]:  # æœ€è¿‘3ä¸ªä¼šè¯
                try:
                    events = chat_service.db.get_events(session)
                    recent_events.extend(events[-2:])  # æ¯ä¸ªä¼šè¯æœ€å¤š2ä¸ªäº‹ä»¶
                except:
                    continue
        
        # å°è¯•è·å–æƒ…ç»ªè®°å½•
        emotion_records = []
        try:
            emotion_records = chat_service.db.get_emotion_history(state.user_id, limit=10)
        except:
            pass
        
    except Exception as e:
        print(f"è·å–æ•°æ®æ—¶å‡ºé”™: {e}")
        session_count = 0
        total_messages = 0
        recent_events = []
        emotion_records = []
    
    basic_report = "ğŸ“Š çŸ¥å·±æŠ¥å‘Š - åŸºç¡€å¿ƒç†å¥åº·åˆ†æ\n"
    basic_report += "=" * 50 + "\n\n"
    
    basic_report += "ğŸ” æŠ¥å‘ŠåŸºæœ¬ä¿¡æ¯\n"
    basic_report += f"â€¢ ç”¨æˆ·ID: {state.user_id}\n"
    basic_report += f"â€¢ ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n"
    basic_report += f"â€¢ ä¼šè¯æ•°é‡: {session_count}ä¸ª\n"
    basic_report += f"â€¢ æ¶ˆæ¯æ€»æ•°: {total_messages}æ¡\n"
    basic_report += f"â€¢ è®°å½•äº‹ä»¶: {len(recent_events)}ä¸ª\n"
    basic_report += f"â€¢ æƒ…ç»ªè®°å½•: {len(emotion_records)}æ¡\n\n"
    
    # æ•°æ®çŠ¶æ€è¯„ä¼°
    basic_report += "ğŸ“ æ•°æ®çŠ¶æ€è¯„ä¼°\n"
    if total_messages < 10:
        basic_report += "â€¢ å½“å‰å¤„äºå’¨è¯¢åˆæœŸé˜¶æ®µï¼Œæ•°æ®ç§¯ç´¯è¾ƒå°‘\n"
        basic_report += "â€¢ å»ºè®®ç»§ç»­ä¿æŒäº¤æµï¼Œä»¥ä¾¿è·å¾—æ›´å‡†ç¡®çš„åˆ†æ\n"
    elif total_messages < 30:
        basic_report += "â€¢ å·²å»ºç«‹äº†åˆæ­¥çš„å¯¹è¯åŸºç¡€\n"
        basic_report += "â€¢ æ•°æ®æ­£åœ¨ç§¯ç´¯ä¸­ï¼Œåˆ†æå‡†ç¡®æ€§å°†é€æ­¥æå‡\n"
    else:
        basic_report += "â€¢ å·²ç§¯ç´¯äº†ä¸°å¯Œçš„å¯¹è¯æ•°æ®\n"
        basic_report += "â€¢ å…·å¤‡äº†è¿›è¡Œæ·±åº¦åˆ†æçš„åŸºç¡€æ¡ä»¶\n"
    basic_report += "\n"
    
    # ç®€è¦åˆ†æ
    if recent_events:
        basic_report += "ğŸ¯ è¿‘æœŸäº‹ä»¶æ¦‚è§ˆ\n"
        event_types = {}
        for event in recent_events[-5:]:  # æœ€è¿‘5ä¸ªäº‹ä»¶
            event_type = event.get('primaryType', 'å…¶ä»–')
            event_types[event_type] = event_types.get(event_type, 0) + 1
        
        if event_types:
            basic_report += "ä¸»è¦å…³æ³¨é¢†åŸŸ:\n"
            for event_type, count in sorted(event_types.items(), key=lambda x: x[1], reverse=True):
                type_name_map = {
                    'emotional': 'æƒ…ç»ªç®¡ç†',
                    'behavioral': 'è¡Œä¸ºæ¨¡å¼', 
                    'cognitive': 'è®¤çŸ¥æ€è€ƒ',
                    'interpersonal': 'äººé™…å…³ç³»',
                    'physiological': 'èº«å¿ƒå¥åº·',
                    'lifeEvent': 'ç”Ÿæ´»äº‹ä»¶'
                }
                display_name = type_name_map.get(event_type, event_type)
                basic_report += f"â€¢ {display_name}: {count}æ¬¡è®°å½•\n"
            basic_report += "\n"
    
    # æƒ…ç»ªçŠ¶æ€ç®€æ
    if emotion_records:
        basic_report += "ğŸ˜Š æƒ…ç»ªçŠ¶æ€ç®€æ\n"
        try:
            recent_emotions = emotion_records[:5]  # æœ€è¿‘5æ¡
            avg_score = sum(float(e.get('emotion_score', 0)) for e in recent_emotions) / len(recent_emotions)
            
            if avg_score >= 7:
                emotion_desc = "æ•´ä½“æƒ…ç»ªçŠ¶æ€è¾ƒä¸ºç§¯æ"
            elif avg_score >= 5:
                emotion_desc = "æƒ…ç»ªçŠ¶æ€ç›¸å¯¹å¹³ç¨³"
            else:
                emotion_desc = "å¯èƒ½éœ€è¦å…³æ³¨æƒ…ç»ªè°ƒèŠ‚"
            
            basic_report += f"â€¢ è¿‘æœŸæƒ…ç»ªè¯„åˆ†: {avg_score:.1f}/10\n"
            basic_report += f"â€¢ çŠ¶æ€è¯„ä¼°: {emotion_desc}\n\n"
        except:
            basic_report += "â€¢ æƒ…ç»ªæ•°æ®æ­£åœ¨ç§¯ç´¯ä¸­\n\n"
    
    # ä¸ªæ€§åŒ–å»ºè®®
    basic_report += "ğŸ’¡ å½“å‰å»ºè®®\n"
    if total_messages < 10:
        basic_report += "â€¢ ç»§ç»­ä¿æŒå¼€æ”¾çš„äº¤æµæ€åº¦ï¼Œåˆ†äº«æ›´å¤šå…·ä½“çš„æ„Ÿå—å’Œæƒ³æ³•\n"
        basic_report += "â€¢ å¯ä»¥å°è¯•æè¿°å…·ä½“çš„ç”Ÿæ´»åœºæ™¯å’Œæƒ…ç»ªä½“éªŒ\n"
    else:
        basic_report += "â€¢ å®šæœŸè¿›è¡Œè‡ªæˆ‘åæ€ï¼Œå…³æ³¨æƒ…ç»ªå’Œè¡Œä¸ºæ¨¡å¼çš„å˜åŒ–\n"
        basic_report += "â€¢ ç»§ç»­è®°å½•é‡è¦çš„ç”Ÿæ´»äº‹ä»¶å’Œæƒ…ç»ªçŠ¶æ€\n"
    
    basic_report += "â€¢ å»ºç«‹è‰¯å¥½çš„ä½œæ¯ä¹ æƒ¯ï¼Œä¿æŒèº«å¿ƒå¥åº·\n"
    basic_report += "â€¢ å¦‚æœ‰éœ€è¦ï¼Œå¯»æ±‚ä¸“ä¸šå¿ƒç†å’¨è¯¢å¸ˆçš„æ”¯æŒ\n\n"
    
    # ç³»ç»Ÿè¯´æ˜
    basic_report += "ğŸ“‹ æŠ¥å‘Šè¯´æ˜\n"
    basic_report += "æœ¬æŠ¥å‘ŠåŸºäºå½“å‰å¯ç”¨çš„å¯¹è¯å’Œè¡Œä¸ºæ•°æ®ç”Ÿæˆã€‚ç”±äºæ•°æ®æœ‰é™ï¼Œ\n"
    basic_report += "è¿™æ˜¯ä¸€ä»½åŸºç¡€ç‰ˆåˆ†ææŠ¥å‘Šã€‚éšç€äº¤æµçš„æ·±å…¥å’Œæ•°æ®çš„ç§¯ç´¯ï¼Œ\n"
    basic_report += "ç³»ç»Ÿå°†èƒ½å¤Ÿæä¾›æ›´è¯¦ç»†å’Œä¸ªæ€§åŒ–çš„åˆ†ææŠ¥å‘Šã€‚\n\n"
    
    basic_report += "å¦‚éœ€è·å¾—æ›´å…¨é¢çš„åˆ†æï¼Œè¯·ç»§ç»­ä¿æŒå¯¹è¯å¹¶è®°å½•ç›¸å…³ä¿¡æ¯ã€‚\n"
    basic_report += "æ‚¨çš„éšç§å’Œæ•°æ®å®‰å…¨å§‹ç»ˆå—åˆ°ä¸¥æ ¼ä¿æŠ¤ã€‚"
    
    return basic_report


"""ç”Ÿæˆåˆ†ææŠ¥å‘ŠèŠ‚ç‚¹"""
def generate_analysis_report(state: OptimizedSessionState) -> OptimizedSessionState:
    start_time = datetime.now().timestamp()
    
    # è·å–èŠå¤©æœåŠ¡å®ä¾‹
    chat_service = get_chat_service_instance()
    
    if state.need_analysis_report:
        print("ğŸ”„ å¼€å§‹ç”ŸæˆçŸ¥å·±æŠ¥å‘Š...")
        
        # æ£€æŸ¥åˆ†ææœåŠ¡æ˜¯å¦å¯ç”¨
        if not hasattr(chat_service, 'analysis_service') or not chat_service.analysis_service:
            print("âš ï¸ åˆ†ææœåŠ¡ä¸å¯ç”¨ï¼Œç”ŸæˆåŸºç¡€æŠ¥å‘Š")
            state.response = _generate_basic_report(state)
            state.processing_stage = "analysis_report_generated"
            state.stage_timings["analysis_report"] = datetime.now().timestamp() - start_time
            return state
        
        try:
            print("ğŸ“Š è°ƒç”¨åˆ†ææœåŠ¡ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š...")
            # ç”Ÿæˆç”¨æˆ·åˆ†ææŠ¥å‘Š
            report = chat_service.analysis_service.generate_user_report(
                user_id=state.user_id,
                session_ids=None,  # è‡ªåŠ¨è·å–æ‰€æœ‰ä¼šè¯
                time_period=30  # æœ€è¿‘30å¤©
            )
            
            if "error" not in report:
                print("âœ… è¯¦ç»†æŠ¥å‘Šç”ŸæˆæˆåŠŸ")
                state.analysis_report = report
                
                # ç”Ÿæˆç³»ç»Ÿæ€§çš„è¯¦ç»†åˆ†ææŠ¥å‘Š
                ai_analysis = report.get("ai_analysis", {})
                metadata = report.get("metadata", {})
                
                # æ„å»ºå®Œæ•´çš„ç³»ç»Ÿæ€§åˆ†ææŠ¥å‘Š
                report_response = "ğŸ“Š çŸ¥å·±æŠ¥å‘Š - å…¨é¢å¿ƒç†å¥åº·åˆ†ææŠ¥å‘Š\n"
                report_response += "=" * 50 + "\n\n"
                
                # === 1. æŠ¥å‘ŠåŸºæœ¬ä¿¡æ¯ ===
                report_response += "ğŸ” æŠ¥å‘ŠåŸºæœ¬ä¿¡æ¯\n"
                report_response += f"â€¢ ç”¨æˆ·ID: {state.user_id}\n"
                report_response += f"â€¢ ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n"
                report_response += f"â€¢ åˆ†ææ—¶é—´èŒƒå›´: æœ€è¿‘30å¤©\n"
                report_response += f"â€¢ åˆ†æä¼šè¯æ•°: {metadata.get('sessions_analyzed', 0)}\n"
                report_response += f"â€¢ äº‹ä»¶æ€»æ•°: {metadata.get('total_events', 0)}\n"
                report_response += f"â€¢ æƒ…ç»ªè®°å½•æ•°: {metadata.get('emotion_records', 0)}\n\n"
                
                # === 2. æ‰§è¡Œæ‘˜è¦ ===
                summary = ai_analysis.get("summary", {})
                if summary:
                    report_response += "ğŸ¯ æ‰§è¡Œæ‘˜è¦\n"
                    report_response += f"â€¢ æ•´ä½“çŠ¶æ€: {summary.get('overallStatus', 'éœ€è¦æ›´å¤šæ•°æ®')}\n"
                    report_response += f"â€¢ é£é™©ç­‰çº§: {summary.get('riskLevel', 'æœªè¯„ä¼°')}\n"
                    report_response += f"â€¢ è¿›æ­¥è¶‹åŠ¿: {summary.get('progressTrend', 'å¾…è§‚å¯Ÿ')}\n\n"
                    
                    # å…³é”®å‘ç°
                    key_findings = summary.get("keyFindings", [])
                    if key_findings:
                        report_response += "ğŸ” å…³é”®å‘ç°\n"
                        for i, finding in enumerate(key_findings[:5], 1):
                            report_response += f"{i}. {finding}\n"
                        report_response += "\n"
                
                # === 3. å»ºè®®éƒ¨åˆ† ===
                recommendations = ai_analysis.get("recommendations", {})
                if recommendations:
                    report_response += "ğŸ’¡ ä¸“ä¸šå»ºè®®\n"
                    
                    immediate = recommendations.get("immediate", [])
                    if immediate:
                        report_response += "ç«‹å³å»ºè®®:\n"
                        for rec in immediate[:3]:
                            report_response += f"â€¢ {rec}\n"
                        report_response += "\n"
                    
                    short_term = recommendations.get("shortTerm", [])
                    if short_term:
                        report_response += "çŸ­æœŸå»ºè®®:\n"
                        for rec in short_term[:3]:
                            report_response += f"â€¢ {rec}\n"
                        report_response += "\n"
                
                # æ•°æ®ç»Ÿè®¡éƒ¨åˆ†
                data_summary = report.get("data_summary", {})
                event_stats = data_summary.get("event_statistics", {})
                if event_stats.get("total_events", 0) > 0:
                    report_response += "ğŸ“ˆ æ•°æ®åˆ†ææ¦‚è§ˆ\n"
                    most_common = event_stats.get("most_common_types", [])
                    if most_common:
                        report_response += "ä¸»è¦äº‹ä»¶ç±»å‹:\n"
                        for event_type, count in most_common[:3]:
                            report_response += f"â€¢ {event_type}: {count}æ¬¡\n"
                        report_response += "\n"
                
                report_response += "ğŸ“ è¯´æ˜\n"
                report_response += "æœ¬æŠ¥å‘ŠåŸºäºæ‚¨æœ€è¿‘30å¤©çš„å¯¹è¯æ•°æ®ç”Ÿæˆï¼Œæ—¨åœ¨å¸®åŠ©æ‚¨æ›´å¥½åœ°äº†è§£è‡ªå·±çš„å¿ƒç†çŠ¶æ€å’Œè¡Œä¸ºæ¨¡å¼ã€‚\n"
                report_response += "å¦‚éœ€æ›´è¯¦ç»†çš„åˆ†ææˆ–ä¸“ä¸šå»ºè®®ï¼Œå»ºè®®ç»§ç»­ä¿æŒå®šæœŸäº¤æµã€‚"
                
                # å°†æŠ¥å‘Šä½œä¸ºå“åº”å†…å®¹
                state.response = report_response
                print("âœ… è¯¦ç»†çŸ¥å·±æŠ¥å‘Šå·²ç”Ÿæˆ")
            else:
                print(f"âš ï¸ è¯¦ç»†æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {report.get('error')}")
                # ç”ŸæˆåŸºç¡€çš„æŠ¥å‘Š
                state.response = _generate_basic_report(state)
                print("âœ… å·²ç”ŸæˆåŸºç¡€çŸ¥å·±æŠ¥å‘Š")
                
        except Exception as e:
            print(f"âŒ åˆ†ææŠ¥å‘Šç”Ÿæˆé”™è¯¯: {e}")
            # ç”ŸæˆåŸºç¡€çš„æŠ¥å‘Š
            state.response = _generate_basic_report(state)
            print("âœ… å·²ç”ŸæˆåŸºç¡€çŸ¥å·±æŠ¥å‘Šï¼ˆå¤‡ç”¨æ–¹æ¡ˆï¼‰")
    
    state.processing_stage = "analysis_report_generated"
    state.stage_timings["analysis_report"] = datetime.now().timestamp() - start_time
    
    return state


"""ç”ŸæˆAIå“åº”èŠ‚ç‚¹"""
def generate_response(state: OptimizedSessionState) -> OptimizedSessionState:
    start_time = datetime.now().timestamp()

    try:
        # æ ¼å¼åŒ–å†å²è®°å½•
        chat_service = get_chat_service_instance()
        messages = [{"role": "system", "content": chat_service.prompt_template}]

        # æ·»åŠ å†å²å¯¹è¯
        for msg in state.history:
            if msg["role"] == "user":
                messages.append({"role": "user", "content": msg["content"]})
            elif msg["role"] == "agent":
                messages.append({"role": "assistant", "content": msg["content"]})

        # æ·»åŠ å½“å‰ç”¨æˆ·è¾“å…¥
        messages.append({"role": "user", "content": state.user_input})

        # å‡†å¤‡é™„åŠ ä¸Šä¸‹æ–‡
        additional_context = ""

        # æ·»åŠ RAGä¸Šä¸‹æ–‡
        if state.has_rag_context and state.rag_context:
            additional_context += f"\n\nä¸“ä¸šçŸ¥è¯†å‚è€ƒï¼š{state.rag_context}"

        if state.plan:
            additional_context += (
                f"\n\nå½“å‰å¯¹è¯è®¡åˆ’ï¼š{json.dumps(state.plan, ensure_ascii=False)}"
            )

        if state.search_results:
            additional_context += f"\n\nç›¸å…³æœç´¢ä¿¡æ¯ï¼š{state.search_results}"

        if state.memory_context:
            additional_context += f"\n\nç›¸å…³è®°å¿†ï¼š{state.memory_context}"

        if additional_context:
            messages[0]["content"] += additional_context

        # è°ƒç”¨LLMç”Ÿæˆå“åº”
        ai_response = chat_service.client.invoke(messages)
        reply = ai_response.content.strip() or "æŠ±æ­‰ï¼Œæš‚æ—¶æ— æ³•å›ç­”ã€‚"

        # æå–æƒ…ç»ª
        clean_content, emotion = chat_service.extract_emotion(reply)
        state.response = clean_content
        state.emotion = emotion

    except Exception as e:
        print(f"Error generating response: {e}")
        state.response = f"å¯¹è¯å¤„ç†å‡ºé”™: {e}"
        state.emotion = "neutral"

    state.processing_stage = "response_generated"
    state.stage_timings["response_generation"] = datetime.now().timestamp() - start_time

    return state


"""åå¤„ç†å’Œæ•°æ®ä¿å­˜èŠ‚ç‚¹"""
def postprocess_and_save(state: OptimizedSessionState) -> OptimizedSessionState:
    start_time = datetime.now().timestamp()

    try:
        # è·å–chat_serviceå®ä¾‹
        chat_service = get_chat_service_instance()
        
        # è®¡ç®—æƒ…ç»ªè¯„åˆ†
        emotion_score = SnowNLP(state.user_input).sentiments * 2 - 1

        # æ‰¹é‡ä¿å­˜æ•°æ®
        save_futures = []

        # ä¿å­˜æƒ…ç»ªè¯„åˆ†
        save_futures.append(
            chat_service.executor.submit(
                chat_service.db.save_emotion_score,
                state.user_id,
                state.session_id,
                emotion_score,
                state.emotion,
            )
        )

        # æ›´æ–°ç”¨æˆ·ç”»åƒ
        save_futures.append(
            chat_service.executor.submit(
                chat_service.db.save_user_profile,
                state.user_id,
                {
                    "last_interaction": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                    "recent_emotion_score": emotion_score,
                    "recent_emotion": state.emotion,
                },
            )
        )

        # ä¿å­˜è®°å¿†ï¼ˆæ¯3æ¬¡å¯¹è¯ä¿å­˜ä¸€æ¬¡åˆ°é•¿æœŸè®°å¿†ï¼‰
        emotion_history = chat_service.db.get_emotion_history(state.user_id)
        is_long_term = (len(emotion_history) + 1) % 3 == 0

        if is_long_term:
            save_futures.append(
                chat_service.executor.submit(
                    chat_service.db.save_long_term_memory,
                    state.user_id,
                    f"ç”¨æˆ·: {state.user_input}\nå’¨è¯¢å¸ˆ: {state.response}",
                )
            )

        # ç­‰å¾…æ‰€æœ‰ä¿å­˜æ“ä½œå®Œæˆ
        for future in as_completed(save_futures, timeout=3):
            try:
                future.result()
            except Exception as e:
                print(f"Save operation failed: {e}")

        # æ›´æ–°å†å²è®°å½•
        state.history.append({"role": "user", "content": state.user_input})
        state.history.append({"role": "agent", "content": state.response})

        # è·å–æ›´æ–°åçš„ç”¨æˆ·ç”»åƒ
        state.user_profile = chat_service.db.get_user_profile(state.user_id)

    except Exception as e:
        print(f"Error in postprocess_and_save: {e}")

    state.processing_stage = "completed"
    state.stage_timings["postprocess"] = datetime.now().timestamp() - start_time

    # è®¡ç®—æ€»å¤„ç†æ—¶é—´
    total_time = datetime.now().timestamp() - state.total_start_time
    state.stage_timings["total"] = total_time

    return state


# === è·¯ç”±å‡½æ•° ===


def should_skip_to_response(state: OptimizedSessionState) -> str:
    """æ£€æŸ¥æ˜¯å¦åº”è¯¥è·³è¿‡æŸäº›æ­¥éª¤ç›´æ¥ç”Ÿæˆå“åº”"""
    if state.crisis_detected:
        return "postprocess_save"
    return "intent_analysis"


def route_after_intent(state: OptimizedSessionState) -> str:
    """æ ¹æ®æ„å›¾è¯†åˆ«ç»“æœè¿›è¡Œè·¯ç”±"""
    if state.route_decision == "rag":
        return "rag_retrieval"
    elif state.route_decision == "web_search":
        return "web_search"
    else:
        return "context_build"


def should_update_plan_after_context(state: OptimizedSessionState) -> str:
    """ä¸Šä¸‹æ–‡æ„å»ºåæ£€æŸ¥æ˜¯å¦éœ€è¦æ›´æ–°è®¡åˆ’"""
    # ç®€å•çš„å¯å‘å¼ï¼šå¦‚æœæ˜¯é—®å€™è¯­æˆ–ç®€å•å›å¤ï¼Œè·³è¿‡è®¡åˆ’æ›´æ–°
    simple_inputs = {"ä½ å¥½", "è°¢è°¢", "å¥½çš„", "å—¯", "æ˜¯çš„", "ä¸æ˜¯"}
    if state.user_input in simple_inputs:
        state.skip_plan_update = True
        return "generate_response"
    return "plan_update"


def continue_after_rag(state: OptimizedSessionState) -> str:
    """RAGæ£€ç´¢åçš„è·¯ç”±"""
    return "context_build"


def continue_after_web_search(state: OptimizedSessionState) -> str:
    """ç½‘ç»œæœç´¢åçš„è·¯ç”±"""
    return "context_build"


def should_update_plan(state: OptimizedSessionState) -> str:
    """æ£€æŸ¥æ˜¯å¦éœ€è¦æ›´æ–°è®¡åˆ’"""
    # ç®€å•çš„å¯å‘å¼ï¼šå¦‚æœæ˜¯é—®å€™è¯­æˆ–ç®€å•å›å¤ï¼Œè·³è¿‡è®¡åˆ’æ›´æ–°
    simple_inputs = {"ä½ å¥½", "è°¢è°¢", "å¥½çš„", "å—¯", "æ˜¯çš„", "ä¸æ˜¯"}
    if state.user_input in simple_inputs:
        state.skip_plan_update = True
        return "generate_response"
    return "plan_update"


def route_after_plan_update(state: OptimizedSessionState) -> str:
    """è®¡åˆ’æ›´æ–°åçš„è·¯ç”±å†³ç­–"""
    print(f"ğŸ” è·¯ç”±å†³ç­–æ£€æŸ¥: need_analysis_report={state.need_analysis_report}, processing_stage={state.processing_stage}")
    
    # å¦‚æœæ£€æµ‹åˆ°çŸ¥å·±æŠ¥å‘Šè¯·æ±‚ï¼Œç›´æ¥è·³è½¬åˆ°æŠ¥å‘Šç”Ÿæˆ
    if state.need_analysis_report:
        print("ğŸ”„ è·¯ç”±åˆ°åˆ†ææŠ¥å‘Šç”ŸæˆèŠ‚ç‚¹")
        return "analysis_report_node"
    
    print("ğŸ”„ è·¯ç”±åˆ°å¼•å¯¼æ€§è¯¢é—®èŠ‚ç‚¹")
    return "guided_inquiry"


def route_after_guided_inquiry(state: OptimizedSessionState) -> str:
    """å¼•å¯¼æ€§è¯¢é—®åçš„è·¯ç”±å†³ç­–"""
    if state.need_pattern_analysis:
        return "pattern_analysis_node"
    return "generate_response"


def route_after_pattern_analysis(state: OptimizedSessionState) -> str:
    """æ¨¡å¼åˆ†æåçš„è·¯ç”±å†³ç­–"""
    if state.need_analysis_report:
        return "analysis_report_node"
    return "generate_response"


def route_after_analysis_report(state: OptimizedSessionState) -> str:
    """åˆ†ææŠ¥å‘Šç”Ÿæˆåçš„è·¯ç”±å†³ç­–"""
    # åˆ†ææŠ¥å‘Šå·²ç”Ÿæˆï¼Œç›´æ¥è·³åˆ°åå¤„ç†ä¿å­˜ï¼Œä¸éœ€è¦å†è°ƒç”¨generate_response
    return "postprocess_save"


# === æ„å»º LangGraph ===

# åˆ›å»ºå·¥ä½œæµ
workflow = StateGraph(OptimizedSessionState)

# æ·»åŠ èŠ‚ç‚¹
workflow.add_node("preprocess", preprocess_input)
workflow.add_node("crisis_check", detect_crisis)
workflow.add_node("intent_analysis", intent_analysis)
workflow.add_node("context_build", build_context)
workflow.add_node("rag_retrieval", rag_retrieval)
workflow.add_node("web_search", web_search_retrieval)
workflow.add_node("plan_update", update_plan)
workflow.add_node("guided_inquiry", guided_inquiry_assessment)
workflow.add_node("pattern_analysis_node", pattern_analysis)
workflow.add_node("analysis_report_node", generate_analysis_report)
workflow.add_node("generate_response", generate_response)
workflow.add_node("postprocess_save", postprocess_and_save)

# è®¾ç½®å…¥å£ç‚¹
workflow.set_entry_point("preprocess")

# æ·»åŠ è¾¹å’Œæ¡ä»¶è·¯ç”±
workflow.add_edge("preprocess", "crisis_check")
workflow.add_conditional_edges(
    "crisis_check",
    should_skip_to_response,
    {"intent_analysis": "intent_analysis", "postprocess_save": "postprocess_save"},
)
workflow.add_conditional_edges(
    "intent_analysis",
    route_after_intent,
    {
        "rag_retrieval": "rag_retrieval",
        "web_search": "web_search", 
        "context_build": "context_build"
    },
)
workflow.add_conditional_edges(
    "rag_retrieval",
    continue_after_rag,
    {"context_build": "context_build"},
)
workflow.add_conditional_edges(
    "web_search",
    continue_after_web_search,
    {"context_build": "context_build"},
)
workflow.add_conditional_edges(
    "context_build",
    should_update_plan,
    {"plan_update": "plan_update", "generate_response": "generate_response"},
)
workflow.add_conditional_edges(
    "plan_update",
    route_after_plan_update,
    {"guided_inquiry": "guided_inquiry", "analysis_report_node": "analysis_report_node"},
)
workflow.add_conditional_edges(
    "guided_inquiry",
    route_after_guided_inquiry,
    {"pattern_analysis_node": "pattern_analysis_node", "generate_response": "generate_response"},
)
workflow.add_conditional_edges(
    "pattern_analysis_node",
    route_after_pattern_analysis,
    {"analysis_report_node": "analysis_report_node", "generate_response": "generate_response"},
)
workflow.add_conditional_edges(
    "analysis_report_node",
    route_after_analysis_report,
    {"postprocess_save": "postprocess_save"},
)
workflow.add_edge("generate_response", "postprocess_save")
workflow.add_edge("postprocess_save", END)

# ç¼–è¯‘å·¥ä½œæµ
optimized_chat_app = workflow.compile()

# === ç³»ç»Ÿåˆå§‹åŒ–å‡½æ•° ===

def initialize_system():
    """
    ç³»ç»Ÿå¯åŠ¨æ—¶åˆå§‹åŒ–æ‰€æœ‰ç»„ä»¶
    æ³¨æ„ï¼šè¿™ä¸ªå‡½æ•°ç°åœ¨ä¸»è¦ç”¨äºæµ‹è¯•å’Œç‹¬ç«‹è¿è¡Œï¼Œæ­£å¸¸å¯åŠ¨æ—¶ç”±start.pyç®¡ç†
    """
    print("ğŸ”§ æ­£åœ¨åˆå§‹åŒ–ç³»ç»Ÿç»„ä»¶...")
    
    # é¢„å…ˆåˆå§‹åŒ–æ‰€æœ‰æœåŠ¡å®ä¾‹ï¼Œé¿å…åœ¨å¯¹è¯è¿‡ç¨‹ä¸­åˆå§‹åŒ–
    print("   ğŸ“Š åˆå§‹åŒ–æ•°æ®åº“æœåŠ¡...")
    get_db_instance()
    
    print("   ğŸš¨ åˆå§‹åŒ–å±æœºæ£€æµ‹å™¨...")
    get_crisis_detector_instance()
    
    print("   ğŸ” åˆå§‹åŒ–æœç´¢æœåŠ¡...")
    get_search_service_instance()
    
    print("   ğŸ’¬ åˆå§‹åŒ–èŠå¤©æœåŠ¡...")
    get_chat_service_instance()
    
    print("âœ… ç³»ç»Ÿç»„ä»¶åˆå§‹åŒ–å®Œæˆ")

# === ä¸»è¦æ¥å£å‡½æ•° ===


def optimized_chat(
    user_input: str,
    user_id: str = "default_user",
    session_id: str | None = None,
    history: List[Dict[str, str]] | None = None,
    enable_performance_monitoring: bool = False,
) -> Dict[str, Any]:
    """
    ä¼˜åŒ–çš„èŠå¤©æ¥å£å‡½æ•°

    Args:
        user_input: ç”¨æˆ·è¾“å…¥
        user_id: ç”¨æˆ·ID
        session_id: ä¼šè¯ID
        history: å¯¹è¯å†å²
        enable_performance_monitoring: æ˜¯å¦å¯ç”¨æ€§èƒ½ç›‘æ§

    Returns:
        åŒ…å«å“åº”å’Œå…¶ä»–ä¿¡æ¯çš„å­—å…¸
    """

    # åˆå§‹åŒ–çŠ¶æ€
    init_state = OptimizedSessionState(
        user_input=user_input,
        user_id=user_id,
        session_id=session_id,
        history=history or [],
    )

    try:
        # è¿è¡Œå·¥ä½œæµ
        result = optimized_chat_app.invoke(init_state)

        # ä»ç»“æœä¸­è·å–æœ€ç»ˆçŠ¶æ€
        final_state = result

        # æ„å»ºå“åº”
        response_data = {
            "response": final_state.get("response"),
            "emotion": final_state.get("emotion", "neutral"),
            "history": final_state.get("history", []),
            "crisis_detected": final_state.get("crisis_detected", False),
            "crisis_reason": final_state.get("crisis_reason"),
            "search_results": final_state.get("search_results"),
            "pattern_analysis": final_state.get("pattern_analysis"),
            "rag_context": final_state.get("rag_context"),
            "has_rag_context": final_state.get("has_rag_context", False),
            "intent_result": final_state.get("intent_result", {}),
            "route_decision": final_state.get("route_decision", "direct_chat"),
            # æ–°å¢å­—æ®µ
            "inquiry_result": final_state.get("inquiry_result"),
            "need_guided_inquiry": final_state.get("need_guided_inquiry", False),
            "need_pattern_analysis": final_state.get("need_pattern_analysis", False),
            "need_analysis_report": final_state.get("need_analysis_report", False),
            "analysis_report": final_state.get("analysis_report"),
        }

        # æ·»åŠ æ€§èƒ½ç›‘æ§ä¿¡æ¯
        if enable_performance_monitoring:
            response_data["performance"] = {
                "stage_timings": final_state.get("stage_timings", {}),
                "total_time": final_state.get("stage_timings", {}).get("total", 0),
                "processing_stage": final_state.get("processing_stage", "unknown"),
            }

        return response_data

    except Exception as e:
        print(f"Error in optimized_chat: {e}")
        return {
            "response": f"å¯¹è¯å¤„ç†å‡ºé”™: {e}",
            "emotion": "neutral",
            "history": history or [],
            "crisis_detected": False,
            "crisis_reason": None,
            "search_results": None,
            "pattern_analysis": None,
            "rag_context": None,
            "has_rag_context": False,
            "intent_result": {},
            "route_decision": "direct_chat",
            "inquiry_result": None,
            "need_guided_inquiry": False,
            "need_pattern_analysis": False,
            "need_analysis_report": False,
            "analysis_report": None,
        }


# === æµ‹è¯•å’Œè°ƒè¯•åŠŸèƒ½ ===


def test_performance():
    """æ€§èƒ½æµ‹è¯•å‡½æ•°"""
    test_messages = [
        "ä½ å¥½",
        "æˆ‘æœ€è¿‘æ„Ÿè§‰å¾ˆç„¦è™‘",
        "ä»€ä¹ˆæ˜¯æŠ‘éƒç—‡",
        "æˆ‘æƒ³è¦å¯»æ‰¾ä¸€äº›æ”¾æ¾çš„æ–¹æ³•",
        "ä»Šå¤©å¤©æ°”æ€ä¹ˆæ ·",
    ]

    print("ğŸš€ å¼€å§‹æ€§èƒ½æµ‹è¯•...")

    for i, msg in enumerate(test_messages):
        print(f"\n--- æµ‹è¯•æ¶ˆæ¯ {i+1}: {msg} ---")
        result = optimized_chat(msg, enable_performance_monitoring=True)

        if "performance" in result:
            perf = result["performance"]
            print(f"æ€»æ—¶é—´: {perf['total_time']:.2f}s")
            print("å„é˜¶æ®µè€—æ—¶:")
            for stage, time_spent in perf["stage_timings"].items():
                if time_spent > 0:
                    print(f"  {stage}: {time_spent:.2f}s")

        print(f"å“åº”: {result['response'][:100]}...")


if __name__ == "__main__":
    import sys

    if len(sys.argv) > 1 and sys.argv[1] == "test":
        test_performance()
    else:
        print("ğŸ©º å¿ƒç†å’¨è¯¢å¯¹è¯ç³»ç»Ÿ (LangGraphä¼˜åŒ–ç‰ˆ) (è¾“å…¥ 'é€€å‡º' ç»“æŸ)")
        print("ğŸ’¡ è¾“å…¥ 'perf' å¯æŸ¥çœ‹æ€§èƒ½ä¿¡æ¯")

        history = []
        while True:
            try:
                user_input = input("æ‚¨: ").strip()
            except (EOFError, KeyboardInterrupt):
                print("\nğŸ‘‹ å†è§ï¼")
                break

            if user_input.lower() in {"é€€å‡º", "quit", "q"}:
                print("ğŸ‘‹ å†è§ï¼ç¥æ‚¨ä¸€åˆ‡é¡ºåˆ©ã€‚")
                break

            enable_perf = user_input.lower() == "perf"
            if enable_perf:
                user_input = input("è¯·è¾“å…¥è¦æµ‹è¯•çš„æ¶ˆæ¯: ").strip()

            result = optimized_chat(
                user_input, history=history, enable_performance_monitoring=enable_perf
            )

            print(f"å’¨è¯¢å¸ˆ: {result['response']}")

            if enable_perf and "performance" in result:
                perf = result["performance"]
                print(f"\nâš¡ æ€§èƒ½ä¿¡æ¯:")
                print(f"æ€»æ—¶é—´: {perf['total_time']:.2f}s")
                for stage, time_spent in perf["stage_timings"].items():
                    if time_spent > 0:
                        print(f"  {stage}: {time_spent:.2f}s")

            history = result["history"]


# === æ¨¡å—çº§åˆ«çš„åˆå§‹åŒ– ===
# å½“æ¨¡å—è¢«å¯¼å…¥æ—¶ï¼Œé¢„å…ˆåˆå§‹åŒ–åŸºç¡€ç»„ä»¶ï¼Œä½†ä¸åˆå§‹åŒ–RAGç›¸å…³ç»„ä»¶
# RAGç»„ä»¶ç”±start.pyç»Ÿä¸€ç®¡ç†
if __name__ != "__main__":
    # åªåœ¨æ¨¡å—è¢«å¯¼å…¥æ—¶åˆå§‹åŒ–åŸºç¡€ç»„ä»¶ï¼Œä¸åœ¨ç›´æ¥è¿è¡Œæ—¶åˆå§‹åŒ–ï¼ˆé¿å…åœ¨æµ‹è¯•æ—¶é‡å¤åˆå§‹åŒ–ï¼‰
    print("ğŸ”§ æ­£åœ¨åˆå§‹åŒ–åŸºç¡€å¯¹è¯ç»„ä»¶...")
    
    # åªé¢„å…ˆåˆå§‹åŒ–åŸºç¡€æœåŠ¡å®ä¾‹ï¼Œä¸åŒ…æ‹¬èŠå¤©æœåŠ¡ï¼ˆé¿å…RAGé‡å¤åˆå§‹åŒ–ï¼‰
    print("   ğŸ“Š åˆå§‹åŒ–æ•°æ®åº“æœåŠ¡...")
    get_db_instance()
    
    print("   ğŸš¨ åˆå§‹åŒ–å±æœºæ£€æµ‹å™¨...")
    get_crisis_detector_instance()
    
    print("   ğŸ” åˆå§‹åŒ–æœç´¢æœåŠ¡...")
    get_search_service_instance()
    
    print("âœ… åŸºç¡€å¯¹è¯ç»„ä»¶åˆå§‹åŒ–å®Œæˆ")
