#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
å¿ƒç†å’¨è¯¢å¯¹è¯ç³»ç»Ÿ - LangGraphä¼˜åŒ–ç‰ˆæœ¬

è¯¥ç‰ˆæœ¬å°†å¯¹è¯æµç¨‹æ‹†åˆ†ä¸ºå¤šä¸ªç‹¬ç«‹çš„èŠ‚ç‚¹ï¼Œé€šè¿‡LangGraphè¿›è¡Œç¼–æ’ï¼š
1. è¾“å…¥é¢„å¤„ç†èŠ‚ç‚¹
2. å±æœºæ£€æµ‹èŠ‚ç‚¹
3. ä¸Šä¸‹æ–‡æ„å»ºèŠ‚ç‚¹
4. æœç´¢èŠ‚ç‚¹ï¼ˆå¯é€‰ï¼‰
5. è®¡åˆ’æ›´æ–°èŠ‚ç‚¹
6. LLMå“åº”èŠ‚ç‚¹
7. åå¤„ç†èŠ‚ç‚¹
8. æ•°æ®ä¿å­˜èŠ‚ç‚¹

ä¼˜åŒ–ç‚¹ï¼š
- å¹¶è¡Œå¤„ç†éä¾èµ–æ“ä½œ
- æ¡ä»¶è·¯ç”±å‡å°‘ä¸å¿…è¦çš„è®¡ç®—
- ä¼˜åŒ–æ•°æ®åº“æ“ä½œæ‰¹æ¬¡
- å¼‚æ­¥å¤„ç†æœç´¢åŠŸèƒ½
"""

import os
import json
import sys
from pathlib import Path
import asyncio
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import Dict, List, Tuple, Any, Optional

sys.path.append(str(Path(__file__).parent.parent))

from datetime import datetime
import re
import requests
from dotenv import load_dotenv
from langchain_core.messages import HumanMessage
from langchain_openai import ChatOpenAI
from langgraph.graph import StateGraph, END
from pydantic import BaseModel, Field
from snownlp import SnowNLP

from utils.extract_json import extract_json
from dao.database import Database

import warnings

warnings.filterwarnings("ignore", category=UserWarning, module="pydantic")

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()


# æ‰©å±•çš„ä¼šè¯çŠ¶æ€æ•°æ®æ¨¡å‹
class OptimizedSessionState(BaseModel):
    # åŸºæœ¬ä¿¡æ¯
    user_input: str
    user_id: str = "default_user"
    session_id: str | None = None

    # å“åº”ç›¸å…³
    response: str | None = None
    emotion: str = "neutral"

    # å†å²è®°å½•
    history: List[Dict[str, str]] = Field(default_factory=list)

    # ç”¨æˆ·ç”»åƒå’Œä¸Šä¸‹æ–‡
    user_profile: Dict[str, Any] = Field(default_factory=dict)
    memory_context: str = ""

    # å±æœºæ£€æµ‹
    crisis_detected: bool = False
    crisis_reason: str | None = None

    # æœç´¢ç›¸å…³
    need_search: bool = False
    search_results: str | None = None

    # è®¡åˆ’å’Œåˆ†æ
    plan: Dict[str, Any] = Field(default_factory=dict)
    pattern_analysis: Dict[str, Any] | None = None

    # å¤„ç†çŠ¶æ€
    processing_stage: str = "init"
    skip_search: bool = False
    skip_plan_update: bool = False

    # æ€§èƒ½ç›‘æ§
    stage_timings: Dict[str, float] = Field(default_factory=dict)
    total_start_time: float = 0.0


# ä¼˜åŒ–çš„å±æœºæ£€æµ‹å™¨
class OptimizedCrisisDetector:
    def __init__(self):
        self._keywords = {
            "high_risk": {"è‡ªæ€", "æƒ³æ­»", "æ´»ä¸ä¸‹å»", "ç»“æŸç”Ÿå‘½", "æ€äºº", "ä¼¤å®³è‡ªå·±"},
            "medium_risk": {"å—ä¸äº†", "ç»æœ›", "å´©æºƒ", "ç—›è‹¦", "æ²¡æœ‰å¸Œæœ›"},
        }
        self._sentiment_threshold = -0.3
        self._cache = {}  # ç®€å•çš„ç»“æœç¼“å­˜

    def check(self, text: str) -> Tuple[bool, str | None, str]:
        """
        æ£€æµ‹å±æœºç¨‹åº¦
        è¿”å›ï¼š(æ˜¯å¦å±æœº, åŸå› , ä¸¥é‡ç¨‹åº¦)
        """
        if text in self._cache:
            return self._cache[text]

        # æ£€æŸ¥é«˜é£é™©å…³é”®è¯
        for keyword in self._keywords["high_risk"]:
            if keyword in text:
                result = (True, f"æ£€æµ‹åˆ°é«˜å±è¯: '{keyword}'", "high")
                self._cache[text] = result
                return result

        # æ£€æŸ¥ä¸­é£é™©å…³é”®è¯
        for keyword in self._keywords["medium_risk"]:
            if keyword in text:
                result = (True, f"æ£€æµ‹åˆ°ä¸­å±è¯: '{keyword}'", "medium")
                self._cache[text] = result
                return result

        # æƒ…æ„Ÿåˆ†æ
        # try:
        #     polarity = SnowNLP(text).sentiments * 2 - 1
        #     if polarity <= self._sentiment_threshold:
        #         result = (True, f"æƒ…æ„Ÿææ€§è¿‡ä½ (polarity={polarity:.2f})", "low")
        #         self._cache[text] = result
        #         return result
        # except Exception:
        #     pass

        result = (False, None, "none")
        self._cache[text] = result
        return result


# ä¼˜åŒ–çš„æœç´¢æœåŠ¡
class OptimizedSearchService:
    def __init__(self):
        self.api_key = os.getenv("SERPAPI_KEY")
        self.search_triggers = [
            "ä»€ä¹ˆæ˜¯",
            "å¦‚ä½•",
            "ä¸ºä»€ä¹ˆ",
            "æ€ä¹ˆåŠ",
            "æœ€æ–°",
            "ç°åœ¨",
            "ä»Šå¤©",
            "æ–°é—»",
            "å¤©æ°”",
            "æŸ¥è¯¢",
            "æœç´¢",
        ]
        self.timeout = 8  # å‡å°‘è¶…æ—¶æ—¶é—´
        self.max_results = 3

    def should_search(self, text: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦éœ€è¦æœç´¢"""
        return any(trigger in text for trigger in self.search_triggers)

    async def search_async(self, query: str) -> str:
        """å¼‚æ­¥æœç´¢"""
        if not self.api_key:
            return "âš ï¸ [æœç´¢åŠŸèƒ½æœªå¯ç”¨: æœªè®¾ç½® SERPAPI_KEY]"

        try:
            loop = asyncio.get_event_loop()
            result = await loop.run_in_executor(None, self._sync_search, query)
            return result
        except Exception as e:
            return f"æœç´¢å‡ºé”™: {e}"

    def _sync_search(self, query: str) -> str:
        """åŒæ­¥æœç´¢å®ç°"""
        try:
            r = requests.get(
                "https://serpapi.com/search",
                params={
                    "q": query,
                    "api_key": self.api_key,
                    "hl": "zh-cn",
                    "num": self.max_results,
                },
                timeout=self.timeout,
            )
            r.raise_for_status()
            data = r.json()

            snippets = []
            for item in data.get("organic_results", [])[: self.max_results]:
                snippets.append(
                    f"æ ‡é¢˜: {item.get('title', '').strip()}\n"
                    f"æ‘˜è¦: {item.get('snippet', '').strip()}\n"
                    f"é“¾æ¥: {item.get('link', '').strip()}"
                )

            return "\n\n".join(snippets) if snippets else "æœªæ‰¾åˆ°ç›¸å…³æœç´¢ç»“æœ"
        except Exception as e:
            return f"æœç´¢å‡ºé”™: {e}"


# ä¼˜åŒ–çš„èŠå¤©æœåŠ¡
class OptimizedChatService:
    def __init__(self, database: Database):
        self.db = database
        self.model = os.environ.get("MODEL_NAME", "deepseek-chat")
        self.client = ChatOpenAI(
            model=self.model,
            api_key=os.environ.get("OPENAI_API_KEY"),
            base_url=os.environ.get("BASE_URL", "https://api.deepseek.com/v1"),
            temperature=float(os.environ.get("TEMPERATURE", "0.7")),
            max_tokens=int(os.environ.get("MAX_TOKENS", "1000")),
            timeout=30,  # å‡å°‘è¶…æ—¶æ—¶é—´
        )

        self.enable_guided_inquiry = (
            os.environ.get("ENABLE_GUIDED_INQUIRY", "true").lower() == "true"
        )
        self.enable_pattern_analysis = (
            os.environ.get("ENABLE_PATTERN_ANALYSIS", "true").lower() == "true"
        )

        # åŠ è½½æç¤ºè¯æ¨¡æ¿
        self.prompt_template = self._load_prompt_template()
        self.planning_prompt = self._load_planning_prompt()

        # åˆ›å»ºçº¿ç¨‹æ± ç”¨äºå¹¶è¡Œå¤„ç†
        self.executor = ThreadPoolExecutor(max_workers=3)

    def _load_prompt_template(self) -> str:
        """åŠ è½½å’¨è¯¢å¸ˆæç¤ºè¯æ¨¡æ¿"""
        prompt_dir = os.path.join(os.path.dirname(__file__), "../prompt")
        os.makedirs(prompt_dir, exist_ok=True)
        prompt_file = os.path.join(prompt_dir, "counselor_prompt.txt")

        if not os.path.exists(prompt_file):
            default_prompt = """ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„å¿ƒç†å’¨è¯¢å¸ˆï¼Œåå«"çŸ¥å·±å’¨è¯¢å¸ˆ"ã€‚ä½ çš„ç›®æ ‡æ˜¯é€šè¿‡å¯¹è¯å¸®åŠ©ç”¨æˆ·è§£å†³å¿ƒç†å›°æ‰°ã€æƒ…ç»ªé—®é¢˜ï¼Œå¹¶æä¾›ä¸“ä¸šçš„å¿ƒç†æ”¯æŒã€‚

è¯·æ³¨æ„ä»¥ä¸‹æŒ‡å¯¼åŸåˆ™ï¼š
1. ä¿æŒå…±æƒ…ã€å°Šé‡å’Œæ”¯æŒçš„æ€åº¦
2. æä¾›å¾ªè¯çš„å¿ƒç†å­¦å»ºè®®
3. ä¸è¦ç»™å‡ºåŒ»ç–—è¯Šæ–­æˆ–å¤„æ–¹
4. å½“ç”¨æˆ·éœ€è¦ä¸“ä¸šåŒ»ç–—å¸®åŠ©æ—¶ï¼Œå»ºè®®ä»–ä»¬å¯»æ±‚ä¸“ä¸šåŒ»ç”Ÿçš„å¸®åŠ©
5. å›å¤è¦ç®€æ´ã€æ¸…æ™°ï¼Œæ˜“äºç”¨æˆ·ç†è§£
6. é€‚å½“ä½¿ç”¨å¼€æ”¾å¼é—®é¢˜é¼“åŠ±ç”¨æˆ·è¡¨è¾¾

ç¤ºä¾‹å›å¤æ ¼å¼ï¼š
"æˆ‘ç†è§£ä½ ç°åœ¨çš„æ„Ÿå—ã€‚è¿™ç§æƒ…å†µä¸‹ï¼Œä½ å¯ä»¥å°è¯•...
å¸Œæœ›è¿™äº›å»ºè®®å¯¹ä½ æœ‰æ‰€å¸®åŠ©ï¼"
"""
            with open(prompt_file, "w", encoding="utf-8") as f:
                f.write(default_prompt)
            return default_prompt

        with open(prompt_file, "r", encoding="utf-8") as f:
            return f.read()

    def _load_planning_prompt(self) -> str:
        """åŠ è½½è®¡åˆ’æç¤ºè¯æ¨¡æ¿"""
        prompt_dir = os.path.join(os.path.dirname(__file__), "../prompt")
        prompt_file = os.path.join(prompt_dir, "planning_prompt.txt")

        if not os.path.exists(prompt_file):
            # åˆ›å»ºé»˜è®¤çš„è®¡åˆ’æç¤ºè¯
            default_planning_prompt = """ä½ æ˜¯ä¸€ä¸ªå¯¹è¯è®¡åˆ’åˆ†æå™¨ã€‚æ ¹æ®ç”¨æˆ·çš„è¾“å…¥å’Œå†å²å¯¹è¯ï¼Œæ›´æ–°å¯¹è¯è®¡åˆ’ã€‚

è¿”å›JSONæ ¼å¼çš„è®¡åˆ’ï¼ŒåŒ…å«ä»¥ä¸‹å­—æ®µï¼š
- user_intent: ç”¨æˆ·æ„å›¾åˆ†æ
- current_state: å½“å‰å¯¹è¯çŠ¶æ€
- steps: å»ºè®®çš„å¯¹è¯æ­¥éª¤
- context: ä¸Šä¸‹æ–‡ä¿¡æ¯
- inquiry_status: å¼•å¯¼å¼è¯¢é—®çŠ¶æ€

è¯·ä¿æŒJSONæ ¼å¼çš„å®Œæ•´æ€§ã€‚"""

            with open(prompt_file, "w", encoding="utf-8") as f:
                f.write(default_planning_prompt)
            return default_planning_prompt

        with open(prompt_file, "r", encoding="utf-8") as f:
            return f.read()

    def batch_get_user_data(self, user_id: str) -> Dict[str, Any]:
        """æ‰¹é‡è·å–ç”¨æˆ·æ•°æ®"""
        try:
            # å¹¶è¡Œè·å–ç”¨æˆ·æ•°æ®
            futures = {
                "profile": self.executor.submit(self.db.get_user_profile, user_id),
                "memory": self.executor.submit(
                    self.db.get_long_term_memory, user_id, 5
                ),
                "emotion_history": self.executor.submit(
                    self.db.get_emotion_history, user_id
                ),
            }

            results = {}
            for key, future in futures.items():
                try:
                    results[key] = future.result(timeout=2)
                except Exception as e:
                    print(f"Error getting {key}: {e}")
                    results[key] = {} if key == "profile" else []

            return results
        except Exception as e:
            print(f"Error in batch_get_user_data: {e}")
            return {"profile": {}, "memory": [], "emotion_history": []}

    def format_memory_context(self, memories: List[Dict[str, Any]]) -> str:
        """æ ¼å¼åŒ–è®°å¿†ä¸Šä¸‹æ–‡"""
        if not memories:
            return ""
        return "\n".join(
            f"{m.get('time', '')}: {m.get('content', '')}" for m in memories
        )

    def extract_emotion(self, content: str) -> Tuple[str, str]:
        """æå–æƒ…ç»ªä¿¡æ¯"""
        # æ£€æŸ¥æ˜¾å¼æƒ…ç»ªæ ‡è®°
        emotion_match = re.search(
            r"#(happy|sad|angry|sleepy|neutral)\b", content, re.IGNORECASE
        )
        if emotion_match:
            emotion = emotion_match.group(1).lower()
            clean_content = re.sub(
                r"\s*#(happy|sad|angry|sleepy|neutral)\b",
                "",
                content,
                flags=re.IGNORECASE,
            )
            return clean_content, emotion

        # åŸºäºå…³é”®è¯çš„æƒ…ç»ªè¯†åˆ«
        content_lower = content.lower()
        emotion_keywords = {
            "happy": ["å¼€å¿ƒ", "é«˜å…´", "å¿«ä¹", "å¾ˆå¥½", "å¾ˆæ£’", "å…´å¥‹", "æ»¡æ„"],
            "sad": ["æ‚²ä¼¤", "éš¾è¿‡", "ä¼¤å¿ƒ", "ç—›è‹¦", "æŠ‘éƒ", "å¤±è½"],
            "angry": ["ç”Ÿæ°”", "æ„¤æ€’", "æ¼ç«", "çƒ¦èº", "çƒ¦æ¼", "ä¸æ»¡"],
            "sleepy": ["ç´¯äº†", "ç–²æƒ«", "å›°", "ç¡è§‰", "ä¼‘æ¯", "ç–²åŠ³"],
        }

        for emotion, keywords in emotion_keywords.items():
            if any(word in content_lower for word in keywords):
                return content, emotion

        return content, "neutral"


# åˆå§‹åŒ–æœåŠ¡
db = Database()
crisis_detector = OptimizedCrisisDetector()
search_service = OptimizedSearchService()
chat_service = OptimizedChatService(db)

# === LangGraph èŠ‚ç‚¹å‡½æ•° ===


def preprocess_input(state: OptimizedSessionState) -> OptimizedSessionState:
    """è¾“å…¥é¢„å¤„ç†èŠ‚ç‚¹"""
    start_time = datetime.now().timestamp()
    state.total_start_time = start_time

    # åŸºæœ¬ä¿¡æ¯è®¾ç½®
    state.user_input = state.user_input.strip()
    state.session_id = state.session_id or state.user_id
    state.processing_stage = "preprocessed"

    # åˆ¤æ–­æ˜¯å¦éœ€è¦æœç´¢
    state.need_search = search_service.should_search(state.user_input)

    # è®°å½•æ—¶é—´
    state.stage_timings["preprocess"] = datetime.now().timestamp() - start_time

    return state


def detect_crisis(state: OptimizedSessionState) -> OptimizedSessionState:
    """å±æœºæ£€æµ‹èŠ‚ç‚¹"""
    start_time = datetime.now().timestamp()

    crisis, reason, severity = crisis_detector.check(state.user_input)
    state.crisis_detected = crisis
    state.crisis_reason = reason

    if crisis:
        # æ ¹æ®ä¸¥é‡ç¨‹åº¦ç”Ÿæˆä¸åŒçš„å“åº”
        if severity == "high":
            state.response = (
                f"âš ï¸ æ£€æµ‹åˆ°é«˜å±æƒ…ç»ªå±æœº: {reason}\n\n"
                "è¯·ç«‹å³è”ç³»ä¸“ä¸šäººå£«æˆ–æ‹¨æ‰“å¿ƒç†æ´åŠ©çƒ­çº¿ 400-161-9995ã€‚\n"
                "æ‚¨çš„ç”Ÿå‘½å¾ˆå®è´µï¼Œè¯·å¯»æ±‚å¸®åŠ©ã€‚æˆ‘ä»¬å…³å¿ƒæ‚¨ã€‚"
            )
        elif severity == "medium":
            state.response = (
                f"âš ï¸ æ£€æµ‹åˆ°æƒ…ç»ªå›°æ‰°: {reason}\n\n"
                "æˆ‘ç†è§£æ‚¨ç°åœ¨çš„æ„Ÿå—å¾ˆå›°éš¾ã€‚è®©æˆ‘ä»¬ä¸€èµ·é¢å¯¹è¿™ä¸ªæŒ‘æˆ˜ã€‚\n"
                "å¦‚æœéœ€è¦ï¼Œä¹Ÿå¯ä»¥è”ç³»ä¸“ä¸šå¿ƒç†æ´åŠ©çƒ­çº¿ 400-161-9995ã€‚"
            )
        else:
            state.response = (
                f"æˆ‘æ³¨æ„åˆ°æ‚¨å¯èƒ½æƒ…ç»ªä¸å¤ªå¥½: {reason}\n\n"
                "æˆ‘åœ¨è¿™é‡Œé™ªä¼´æ‚¨ï¼Œè®©æˆ‘ä»¬ä¸€èµ·èŠèŠå§ã€‚"
            )

        state.processing_stage = "crisis_handled"
        # ä¿å­˜å±æœºè®°å½•
        chat_service.db.save_long_term_memory(
            state.user_id, f"[CRISIS-{severity.upper()}] {state.user_input} â€“ {reason}"
        )

    state.stage_timings["crisis_detection"] = datetime.now().timestamp() - start_time
    return state


def build_context(state: OptimizedSessionState) -> OptimizedSessionState:
    """ä¸Šä¸‹æ–‡æ„å»ºèŠ‚ç‚¹"""
    start_time = datetime.now().timestamp()

    # æ‰¹é‡è·å–ç”¨æˆ·æ•°æ®
    user_data = chat_service.batch_get_user_data(state.user_id)

    state.user_profile = user_data.get("profile", {})
    state.memory_context = chat_service.format_memory_context(
        user_data.get("memory", [])
    )

    state.processing_stage = "context_built"
    state.stage_timings["context_building"] = datetime.now().timestamp() - start_time

    return state


def search_information(state: OptimizedSessionState) -> OptimizedSessionState:
    """æœç´¢ä¿¡æ¯èŠ‚ç‚¹ï¼ˆåŒæ­¥ç‰ˆæœ¬ï¼‰"""
    start_time = datetime.now().timestamp()

    if state.need_search and not state.skip_search:
        try:
            search_results = search_service._sync_search(state.user_input)
            state.search_results = search_results
        except Exception as e:
            state.search_results = f"æœç´¢åŠŸèƒ½æš‚æ—¶ä¸å¯ç”¨: {e}"

    state.processing_stage = "search_completed"
    state.stage_timings["search"] = datetime.now().timestamp() - start_time

    return state


def update_plan(state: OptimizedSessionState) -> OptimizedSessionState:
    """æ›´æ–°å¯¹è¯è®¡åˆ’èŠ‚ç‚¹"""
    start_time = datetime.now().timestamp()

    if state.skip_plan_update:
        state.stage_timings["plan_update"] = 0
        return state

    try:
        # è·å–æˆ–åˆ›å»ºè®¡åˆ’
        plan = chat_service.db.get_session_plan(state.session_id)
        if not plan:
            plan = {
                "session_id": state.session_id,
                "user_intent": {
                    "type": "unknown",
                    "description": "",
                    "confidence": 0.0,
                    "identified_at": datetime.now().isoformat(),
                },
                "current_state": {
                    "stage": "intent_identification",
                    "progress": 0.0,
                    "last_updated": datetime.now().isoformat(),
                },
                "steps": [],
                "context": {"key_points": [], "emotions": [], "concerns": []},
                "inquiry_status": {
                    "stage": "åˆå§‹é˜¶æ®µ",
                    "information_completeness": 0,
                    "collected_info": {},
                    "pattern_analyzed": False,
                },
            }

        # æ„å»ºæ¶ˆæ¯
        messages = [
            {"role": "system", "content": chat_service.planning_prompt},
            {
                "role": "user",
                "content": f"Current plan: {json.dumps(plan, ensure_ascii=False)}\n\n"
                f"Current message: {state.user_input}\n\n"
                f"History: {json.dumps(state.history, ensure_ascii=False)}",
            },
        ]

        # è°ƒç”¨LLMæ›´æ–°è®¡åˆ’
        response = chat_service.client.invoke(messages)
        reply = response.content.strip()

        # è§£ææ›´æ–°åçš„è®¡åˆ’
        updated_plan = extract_json(reply)
        if updated_plan:
            if "inquiry_status" not in updated_plan:
                updated_plan["inquiry_status"] = plan.get(
                    "inquiry_status",
                    {
                        "stage": "åˆå§‹é˜¶æ®µ",
                        "information_completeness": 0,
                        "collected_info": {},
                        "pattern_analyzed": False,
                    },
                )

            chat_service.db.save_session_plan(state.session_id, updated_plan)
            state.plan = updated_plan
        else:
            plan["current_state"]["last_updated"] = datetime.now().isoformat()
            state.plan = plan

    except Exception as e:
        print(f"Error updating plan: {e}")
        state.plan = {}

    state.processing_stage = "plan_updated"
    state.stage_timings["plan_update"] = datetime.now().timestamp() - start_time

    return state


def generate_response(state: OptimizedSessionState) -> OptimizedSessionState:
    """ç”ŸæˆAIå“åº”èŠ‚ç‚¹"""
    start_time = datetime.now().timestamp()

    try:
        # æ ¼å¼åŒ–å†å²è®°å½•
        messages = [{"role": "system", "content": chat_service.prompt_template}]

        # æ·»åŠ å†å²å¯¹è¯
        for msg in state.history:
            if msg["role"] == "user":
                messages.append({"role": "user", "content": msg["content"]})
            elif msg["role"] == "agent":
                messages.append({"role": "assistant", "content": msg["content"]})

        # æ·»åŠ å½“å‰ç”¨æˆ·è¾“å…¥
        messages.append({"role": "user", "content": state.user_input})

        # å‡†å¤‡é™„åŠ ä¸Šä¸‹æ–‡
        additional_context = ""

        if state.plan:
            additional_context += (
                f"\n\nå½“å‰å¯¹è¯è®¡åˆ’ï¼š{json.dumps(state.plan, ensure_ascii=False)}"
            )

        if state.search_results:
            additional_context += f"\n\nç›¸å…³æœç´¢ä¿¡æ¯ï¼š{state.search_results}"

        if state.memory_context:
            additional_context += f"\n\nç›¸å…³è®°å¿†ï¼š{state.memory_context}"

        if additional_context:
            messages[0]["content"] += additional_context

        # è°ƒç”¨LLMç”Ÿæˆå“åº”
        ai_response = chat_service.client.invoke(messages)
        reply = ai_response.content.strip() or "æŠ±æ­‰ï¼Œæš‚æ—¶æ— æ³•å›ç­”ã€‚"

        # æå–æƒ…ç»ª
        clean_content, emotion = chat_service.extract_emotion(reply)
        state.response = clean_content
        state.emotion = emotion

    except Exception as e:
        print(f"Error generating response: {e}")
        state.response = f"å¯¹è¯å¤„ç†å‡ºé”™: {e}"
        state.emotion = "neutral"

    state.processing_stage = "response_generated"
    state.stage_timings["response_generation"] = datetime.now().timestamp() - start_time

    return state


def postprocess_and_save(state: OptimizedSessionState) -> OptimizedSessionState:
    """åå¤„ç†å’Œæ•°æ®ä¿å­˜èŠ‚ç‚¹"""
    start_time = datetime.now().timestamp()

    try:
        # è®¡ç®—æƒ…ç»ªè¯„åˆ†
        emotion_score = SnowNLP(state.user_input).sentiments * 2 - 1

        # æ‰¹é‡ä¿å­˜æ•°æ®
        save_futures = []

        # ä¿å­˜æƒ…ç»ªè¯„åˆ†
        save_futures.append(
            chat_service.executor.submit(
                chat_service.db.save_emotion_score,
                state.user_id,
                state.session_id,
                emotion_score,
                state.emotion,
            )
        )

        # æ›´æ–°ç”¨æˆ·ç”»åƒ
        save_futures.append(
            chat_service.executor.submit(
                chat_service.db.save_user_profile,
                state.user_id,
                {
                    "last_interaction": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                    "recent_emotion_score": emotion_score,
                    "recent_emotion": state.emotion,
                },
            )
        )

        # ä¿å­˜è®°å¿†ï¼ˆæ¯3æ¬¡å¯¹è¯ä¿å­˜ä¸€æ¬¡åˆ°é•¿æœŸè®°å¿†ï¼‰
        emotion_history = chat_service.db.get_emotion_history(state.user_id)
        is_long_term = (len(emotion_history) + 1) % 3 == 0

        if is_long_term:
            save_futures.append(
                chat_service.executor.submit(
                    chat_service.db.save_long_term_memory,
                    state.user_id,
                    f"ç”¨æˆ·: {state.user_input}\nå’¨è¯¢å¸ˆ: {state.response}",
                )
            )

        # ç­‰å¾…æ‰€æœ‰ä¿å­˜æ“ä½œå®Œæˆ
        for future in as_completed(save_futures, timeout=3):
            try:
                future.result()
            except Exception as e:
                print(f"Save operation failed: {e}")

        # æ›´æ–°å†å²è®°å½•
        state.history.append({"role": "user", "content": state.user_input})
        state.history.append({"role": "agent", "content": state.response})

        # è·å–æ›´æ–°åçš„ç”¨æˆ·ç”»åƒ
        state.user_profile = chat_service.db.get_user_profile(state.user_id)

    except Exception as e:
        print(f"Error in postprocess_and_save: {e}")

    state.processing_stage = "completed"
    state.stage_timings["postprocess"] = datetime.now().timestamp() - start_time

    # è®¡ç®—æ€»å¤„ç†æ—¶é—´
    total_time = datetime.now().timestamp() - state.total_start_time
    state.stage_timings["total"] = total_time

    return state


# === è·¯ç”±å‡½æ•° ===


def should_skip_to_response(state: OptimizedSessionState) -> str:
    """æ£€æŸ¥æ˜¯å¦åº”è¯¥è·³è¿‡æŸäº›æ­¥éª¤ç›´æ¥ç”Ÿæˆå“åº”"""
    if state.crisis_detected:
        return "postprocess_save"
    return "context_build"


def should_search(state: OptimizedSessionState) -> str:
    """æ£€æŸ¥æ˜¯å¦éœ€è¦æœç´¢"""
    if state.need_search:
        return "search_info"
    return "plan_update"


def should_update_plan(state: OptimizedSessionState) -> str:
    """æ£€æŸ¥æ˜¯å¦éœ€è¦æ›´æ–°è®¡åˆ’"""
    # ç®€å•çš„å¯å‘å¼ï¼šå¦‚æœæ˜¯é—®å€™è¯­æˆ–ç®€å•å›å¤ï¼Œè·³è¿‡è®¡åˆ’æ›´æ–°
    simple_inputs = {"ä½ å¥½", "è°¢è°¢", "å¥½çš„", "å—¯", "æ˜¯çš„", "ä¸æ˜¯"}
    if state.user_input in simple_inputs:
        state.skip_plan_update = True
        return "generate_response"
    return "plan_update"


# === æ„å»º LangGraph ===

# åˆ›å»ºå·¥ä½œæµ
workflow = StateGraph(OptimizedSessionState)

# æ·»åŠ èŠ‚ç‚¹
workflow.add_node("preprocess", preprocess_input)
workflow.add_node("crisis_check", detect_crisis)
workflow.add_node("context_build", build_context)
workflow.add_node("search_info", search_information)
workflow.add_node("plan_update", update_plan)
workflow.add_node("generate_response", generate_response)
workflow.add_node("postprocess_save", postprocess_and_save)

# è®¾ç½®å…¥å£ç‚¹
workflow.set_entry_point("preprocess")

# æ·»åŠ è¾¹å’Œæ¡ä»¶è·¯ç”±
workflow.add_edge("preprocess", "crisis_check")
workflow.add_conditional_edges(
    "crisis_check",
    should_skip_to_response,
    {"context_build": "context_build", "postprocess_save": "postprocess_save"},
)
workflow.add_conditional_edges(
    "context_build",
    should_search,
    {"search_info": "search_info", "plan_update": "plan_update"},
)
workflow.add_conditional_edges(
    "search_info",
    should_update_plan,
    {"plan_update": "plan_update", "generate_response": "generate_response"},
)
workflow.add_edge("plan_update", "generate_response")
workflow.add_edge("generate_response", "postprocess_save")
workflow.add_edge("postprocess_save", END)

# ç¼–è¯‘å·¥ä½œæµ
optimized_chat_app = workflow.compile()

# === ä¸»è¦æ¥å£å‡½æ•° ===


def optimized_chat(
    user_input: str,
    user_id: str = "default_user",
    session_id: str | None = None,
    history: List[Dict[str, str]] | None = None,
    enable_performance_monitoring: bool = False,
) -> Dict[str, Any]:
    """
    ä¼˜åŒ–çš„èŠå¤©æ¥å£å‡½æ•°

    Args:
        user_input: ç”¨æˆ·è¾“å…¥
        user_id: ç”¨æˆ·ID
        session_id: ä¼šè¯ID
        history: å¯¹è¯å†å²
        enable_performance_monitoring: æ˜¯å¦å¯ç”¨æ€§èƒ½ç›‘æ§

    Returns:
        åŒ…å«å“åº”å’Œå…¶ä»–ä¿¡æ¯çš„å­—å…¸
    """

    # åˆå§‹åŒ–çŠ¶æ€
    init_state = OptimizedSessionState(
        user_input=user_input,
        user_id=user_id,
        session_id=session_id,
        history=history or [],
    )

    try:
        # è¿è¡Œå·¥ä½œæµ
        result = optimized_chat_app.invoke(init_state)

        # ä»ç»“æœä¸­è·å–æœ€ç»ˆçŠ¶æ€
        final_state = result

        # æ„å»ºå“åº”
        response_data = {
            "response": final_state.get("response"),
            "emotion": final_state.get("emotion", "neutral"),
            "history": final_state.get("history", []),
            "crisis_detected": final_state.get("crisis_detected", False),
            "crisis_reason": final_state.get("crisis_reason"),
            "search_results": final_state.get("search_results"),
            "pattern_analysis": final_state.get("pattern_analysis"),
        }

        # æ·»åŠ æ€§èƒ½ç›‘æ§ä¿¡æ¯
        if enable_performance_monitoring:
            response_data["performance"] = {
                "stage_timings": final_state.get("stage_timings", {}),
                "total_time": final_state.get("stage_timings", {}).get("total", 0),
                "processing_stage": final_state.get("processing_stage", "unknown"),
            }

        return response_data

    except Exception as e:
        print(f"Error in optimized_chat: {e}")
        return {
            "response": f"å¯¹è¯å¤„ç†å‡ºé”™: {e}",
            "emotion": "neutral",
            "history": history or [],
            "crisis_detected": False,
            "crisis_reason": None,
            "search_results": None,
            "pattern_analysis": None,
        }


# === æµ‹è¯•å’Œè°ƒè¯•åŠŸèƒ½ ===


def test_performance():
    """æ€§èƒ½æµ‹è¯•å‡½æ•°"""
    test_messages = [
        "ä½ å¥½",
        "æˆ‘æœ€è¿‘æ„Ÿè§‰å¾ˆç„¦è™‘",
        "ä»€ä¹ˆæ˜¯æŠ‘éƒç—‡",
        "æˆ‘æƒ³è¦å¯»æ‰¾ä¸€äº›æ”¾æ¾çš„æ–¹æ³•",
        "ä»Šå¤©å¤©æ°”æ€ä¹ˆæ ·",
    ]

    print("ğŸš€ å¼€å§‹æ€§èƒ½æµ‹è¯•...")

    for i, msg in enumerate(test_messages):
        print(f"\n--- æµ‹è¯•æ¶ˆæ¯ {i+1}: {msg} ---")
        result = optimized_chat(msg, enable_performance_monitoring=True)

        if "performance" in result:
            perf = result["performance"]
            print(f"æ€»æ—¶é—´: {perf['total_time']:.2f}s")
            print("å„é˜¶æ®µè€—æ—¶:")
            for stage, time_spent in perf["stage_timings"].items():
                if time_spent > 0:
                    print(f"  {stage}: {time_spent:.2f}s")

        print(f"å“åº”: {result['response'][:100]}...")


if __name__ == "__main__":
    import sys

    if len(sys.argv) > 1 and sys.argv[1] == "test":
        test_performance()
    else:
        print("ğŸ©º å¿ƒç†å’¨è¯¢å¯¹è¯ç³»ç»Ÿ (LangGraphä¼˜åŒ–ç‰ˆ) (è¾“å…¥ 'é€€å‡º' ç»“æŸ)")
        print("ğŸ’¡ è¾“å…¥ 'perf' å¯æŸ¥çœ‹æ€§èƒ½ä¿¡æ¯")

        history = []
        while True:
            try:
                user_input = input("æ‚¨: ").strip()
            except (EOFError, KeyboardInterrupt):
                print("\nğŸ‘‹ å†è§ï¼")
                break

            if user_input.lower() in {"é€€å‡º", "quit", "q"}:
                print("ğŸ‘‹ å†è§ï¼ç¥æ‚¨ä¸€åˆ‡é¡ºåˆ©ã€‚")
                break

            enable_perf = user_input.lower() == "perf"
            if enable_perf:
                user_input = input("è¯·è¾“å…¥è¦æµ‹è¯•çš„æ¶ˆæ¯: ").strip()

            result = optimized_chat(
                user_input, history=history, enable_performance_monitoring=enable_perf
            )

            print(f"å’¨è¯¢å¸ˆ: {result['response']}")

            if enable_perf and "performance" in result:
                perf = result["performance"]
                print(f"\nâš¡ æ€§èƒ½ä¿¡æ¯:")
                print(f"æ€»æ—¶é—´: {perf['total_time']:.2f}s")
                for stage, time_spent in perf["stage_timings"].items():
                    if time_spent > 0:
                        print(f"  {stage}: {time_spent:.2f}s")

            history = result["history"]
