#!/usr/bin/env python
# -*- coding: utf-8 -*-

import os
import json
from datetime import datetime, timedelta
from collections import Counter, defaultdict
from typing import Dict, List, Any, Optional
from openai import OpenAI
import statistics

class AnalysisReportService:
    """ç”¨æˆ·åˆ†ææŠ¥å‘ŠæœåŠ¡ï¼ŒåŸºäºè¡Œä¸ºæ¨¡å¼å’Œäº‹ä»¶æ•°æ®ç”Ÿæˆæ·±åº¦åˆ†ææŠ¥å‘Š"""

    def __init__(self):
        """åˆå§‹åŒ–åˆ†ææŠ¥å‘ŠæœåŠ¡"""
        self.model = os.environ.get("CHAT_MODEL_NAME")
        self.client = OpenAI(
            api_key=os.environ.get("CHAT_API_KEY"),
            base_url=os.environ.get("CHAT_BASE_URL"),
            timeout=120  # å¢åŠ è¶…æ—¶æ—¶é—´åˆ°120ç§’ï¼Œå› ä¸ºæŠ¥å‘Šç”Ÿæˆéœ€è¦è¾ƒé•¿æ—¶é—´
        )
        self.analysis_prompt = self._load_analysis_prompt()
        
    def _load_analysis_prompt(self) -> str:
        """åŠ è½½åˆ†ææŠ¥å‘Šç”Ÿæˆçš„Promptæ¨¡æ¿"""
        return """ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„å¿ƒç†å¥åº·æ•°æ®åˆ†æå¸ˆã€‚åŸºäºç”¨æˆ·çš„è¡Œä¸ºæ¨¡å¼ã€äº‹ä»¶å†å²å’Œæƒ…ç»ªæ•°æ®ï¼Œç”Ÿæˆä¸€ä»½ä¸“ä¸šçš„å¿ƒç†å¥åº·åˆ†ææŠ¥å‘Šã€‚

è¯·æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼ç”ŸæˆæŠ¥å‘Šï¼š
{
    "summary": {
        "overallStatus": "æ•´ä½“å¿ƒç†å¥åº·çŠ¶æ€è¯„ä¼°",
        "keyFindings": ["ä¸»è¦å‘ç°1", "ä¸»è¦å‘ç°2", "ä¸»è¦å‘ç°3"],
        "riskLevel": "low/medium/high",
        "progressTrend": "improving/stable/declining"
    },
    "behaviorPatterns": {
        "dominantPatterns": ["ä¸»è¦è¡Œä¸ºæ¨¡å¼1", "ä¸»è¦è¡Œä¸ºæ¨¡å¼2"],
        "triggers": ["ä¸»è¦è§¦å‘å› ç´ 1", "ä¸»è¦è§¦å‘å› ç´ 2"],
        "copingStrategies": ["åº”å¯¹ç­–ç•¥1", "åº”å¯¹ç­–ç•¥2"],
        "patternEvolution": "è¡Œä¸ºæ¨¡å¼å˜åŒ–è¶‹åŠ¿"
    },
    "eventAnalysis": {
        "frequentEventTypes": ["å¸¸è§äº‹ä»¶ç±»å‹1", "å¸¸è§äº‹ä»¶ç±»å‹2"],
        "impactfulEvents": ["é«˜å½±å“äº‹ä»¶1", "é«˜å½±å“äº‹ä»¶2"],
        "emotionalTriggers": ["æƒ…ç»ªè§¦å‘å› ç´ 1", "æƒ…ç»ªè§¦å‘å› ç´ 2"],
        "recoveryPatterns": "åº·å¤æ¨¡å¼åˆ†æ"
    },
    "emotionalProfile": {
        "dominantEmotions": ["ä¸»è¦æƒ…ç»ª1", "ä¸»è¦æƒ…ç»ª2"],
        "emotionalRange": "æƒ…ç»ªæ³¢åŠ¨èŒƒå›´æè¿°",
        "stableEmotions": ["ç¨³å®šæƒ…ç»ª1", "ç¨³å®šæƒ…ç»ª2"],
        "volatileEmotions": ["æ³¢åŠ¨æƒ…ç»ª1", "æ³¢åŠ¨æƒ…ç»ª2"]
    },
    "recommendations": {
        "immediate": ["ç«‹å³å»ºè®®1", "ç«‹å³å»ºè®®2"],
        "shortTerm": ["çŸ­æœŸå»ºè®®1", "çŸ­æœŸå»ºè®®2"],
        "longTerm": ["é•¿æœŸå»ºè®®1", "é•¿æœŸå»ºè®®2"],
        "professionalReferral": "æ˜¯å¦éœ€è¦ä¸“ä¸šè½¬ä»‹åŠåŸå› "
    },
    "riskAssessment": {
        "currentRisks": ["å½“å‰é£é™©1", "å½“å‰é£é™©2"],
        "protectiveFactors": ["ä¿æŠ¤å› ç´ 1", "ä¿æŠ¤å› ç´ 2"],
        "warningSignals": ["è­¦ç¤ºä¿¡å·1", "è­¦ç¤ºä¿¡å·2"],
        "interventionPriority": "high/medium/low"
    }
}

è¯·ç¡®ä¿åˆ†æå®¢è§‚ã€ä¸“ä¸šï¼Œæä¾›å…·ä½“å¯è¡Œçš„å»ºè®®ã€‚"""

    def generate_user_report(self, user_id: str, session_ids: List[str], time_period: int = 30) -> Dict[str, Any]:
        """ç”Ÿæˆç”¨æˆ·ç»¼åˆåˆ†ææŠ¥å‘Š
        
        Args:
            user_id: ç”¨æˆ·ID
            session_ids: ä¼šè¯IDåˆ—è¡¨ï¼Œå¦‚æœä¸ºNoneåˆ™è·å–æ‰€æœ‰ä¼šè¯
            time_period: åˆ†ææ—¶é—´æ®µï¼ˆå¤©æ•°ï¼‰
            
        Returns:
            Dict: åˆ†ææŠ¥å‘Š
        """
        try:
            # å¦‚æœsession_idsä¸ºNoneï¼Œè·å–ç”¨æˆ·çš„æ‰€æœ‰ä¼šè¯
            if session_ids is None:
                from dao.database import Database
                db = Database()
                session_ids = db.get_user_session_ids(user_id)
                print(f"ğŸ” è‡ªåŠ¨è·å–ç”¨æˆ·ä¼šè¯: {len(session_ids)} ä¸ªä¼šè¯")
            
            # æ”¶é›†ç”¨æˆ·æ•°æ®
            user_data = self._collect_user_data(user_id, session_ids, time_period)
            
            # ç”Ÿæˆæ•°æ®ç»Ÿè®¡
            statistics_data = self._generate_statistics(user_data)
            
            # ä½¿ç”¨AIç”Ÿæˆæ·±åº¦åˆ†æ
            ai_analysis = self._generate_ai_analysis(user_data, statistics_data)
            
            # æ•´åˆæŠ¥å‘Š
            report = {
                "user_id": user_id,
                "generated_at": datetime.now().isoformat(),
                "analysis_period": {
                    "days": time_period,
                    "start_date": (datetime.now() - timedelta(days=time_period)).isoformat(),
                    "end_date": datetime.now().isoformat()
                },
                "data_summary": statistics_data,
                "ai_analysis": ai_analysis,
                "metadata": {
                    "sessions_analyzed": len(session_ids),
                    "total_events": len(user_data.get("events", [])),
                    "total_patterns": len(user_data.get("patterns", [])),
                    "emotion_records": len(user_data.get("emotions", []))
                }
            }
            
            return report
            
        except Exception as e:
            print(f"Error generating user report: {str(e)}")
            return {"error": str(e)}

    def _collect_user_data(self, user_id: str, session_ids: List[str], time_period: int) -> Dict[str, Any]:
        """æ”¶é›†ç”¨æˆ·ç›¸å…³æ•°æ®
        
        Args:
            user_id: ç”¨æˆ·ID
            session_ids: ä¼šè¯IDåˆ—è¡¨
            time_period: æ—¶é—´æ®µ
            
        Returns:
            Dict: ç”¨æˆ·æ•°æ®é›†åˆ
        """
        from dao.database import Database
        db = Database()
        
        cutoff_date = datetime.now() - timedelta(days=time_period)
        
        # æ”¶é›†æ•°æ®
        user_data = {
            "events": [],
            "patterns": [],
            "emotions": [],
            "user_profile": {},
            "long_term_memory": []
        }
        
        # æ”¶é›†äº‹ä»¶æ•°æ®
        print(f"ğŸ“Š å¼€å§‹æ”¶é›†äº‹ä»¶æ•°æ®ï¼Œä¼šè¯æ•°é‡: {len(session_ids)}")
        for i, session_id in enumerate(session_ids, 1):
            print(f"  å¤„ç†ä¼šè¯ {i}/{len(session_ids)}: {session_id}")
            events = db.get_events(session_id)
            print(f"    åŸå§‹äº‹ä»¶æ•°: {len(events)}")
            
            # è¿‡æ»¤æ—¶é—´èŒƒå›´å†…çš„äº‹ä»¶
            filtered_events = [
                event for event in events 
                if datetime.fromisoformat(event.get("time", "1900-01-01")) >= cutoff_date
            ]
            print(f"    æ—¶é—´èŒƒå›´å†…äº‹ä»¶æ•°: {len(filtered_events)}")
            
            # æ‰“å°äº‹ä»¶æå–å†…å®¹
            if filtered_events:
                print(f"    äº‹ä»¶å†…å®¹ç¤ºä¾‹:")
                for j, event in enumerate(filtered_events[:3], 1):  # åªæ˜¾ç¤ºå‰3ä¸ªäº‹ä»¶
                    print(f"      äº‹ä»¶{j}: {event.get('primaryType', 'æœªçŸ¥ç±»å‹')} - {event.get('subType', 'æœªçŸ¥å­ç±»å‹')}")
                    print(f"        æ—¶é—´: {event.get('time', 'æœªçŸ¥æ—¶é—´')}")
                    print(f"        æè¿°: {event.get('description', 'æ— æè¿°')[:100]}...")
            
            user_data["events"].extend(filtered_events)
            
            # æ”¶é›†è¡Œä¸ºæ¨¡å¼æ•°æ®
            pattern = db.get_pattern_analysis(session_id)
            if pattern:
                print(f"    å‘ç°è¡Œä¸ºæ¨¡å¼åˆ†ææ•°æ®")
                user_data["patterns"].append(pattern)
        
        # æ”¶é›†æƒ…ç»ªæ•°æ®
        emotions = db.get_emotion_history(user_id, limit=50)
        user_data["emotions"] = [
            emotion for emotion in emotions 
            if datetime.fromisoformat(emotion.get("timestamp", "1900-01-01")) >= cutoff_date
        ]
        
        # æ”¶é›†ç”¨æˆ·ç”»åƒ
        user_data["user_profile"] = db.get_user_profile(user_id)
        
        # æ”¶é›†é•¿æœŸè®°å¿†
        user_data["long_term_memory"] = db.get_long_term_memory(user_id, limit=20)
        
        return user_data

    def _generate_statistics(self, user_data: Dict[str, Any]) -> Dict[str, Any]:
        """ç”Ÿæˆæ•°æ®ç»Ÿè®¡ä¿¡æ¯
        
        Args:
            user_data: ç”¨æˆ·æ•°æ®
            
        Returns:
            Dict: ç»Ÿè®¡æ•°æ®
        """
        stats = {
            "event_statistics": self._analyze_events(user_data["events"]),
            "pattern_statistics": self._analyze_patterns(user_data["patterns"]),
            "emotion_statistics": self._analyze_emotions(user_data["emotions"]),
            "engagement_statistics": self._analyze_engagement(user_data)
        }
        
        return stats

    def _analyze_events(self, events: List[Dict]) -> Dict[str, Any]:
        """åˆ†æäº‹ä»¶æ•°æ®
        
        Args:
            events: äº‹ä»¶åˆ—è¡¨
            
        Returns:
            Dict: äº‹ä»¶ç»Ÿè®¡
        """
        if not events:
            return {"total_events": 0, "event_types": {}, "most_common_types": []}
        
        # äº‹ä»¶ç±»å‹ç»Ÿè®¡
        primary_types = [event.get("primaryType", "unknown") for event in events]
        sub_types = [event.get("subType", "unknown") for event in events]
        
        type_counter = Counter(primary_types)
        subtype_counter = Counter(sub_types)
        
        # äº‹ä»¶æ—¶é—´åˆ†å¸ƒ
        event_times = []
        for event in events:
            try:
                event_time = datetime.fromisoformat(event.get("time", ""))
                event_times.append(event_time)
            except:
                continue
        
        return {
            "total_events": len(events),
            "event_types": dict(type_counter),
            "event_subtypes": dict(subtype_counter),
            "most_common_types": type_counter.most_common(3),
            "most_common_subtypes": subtype_counter.most_common(5),
            "time_distribution": self._analyze_time_distribution(event_times),
            "event_status": Counter([event.get("status", "unknown") for event in events])
        }

    def _analyze_patterns(self, patterns: List[Dict]) -> Dict[str, Any]:
        """åˆ†æè¡Œä¸ºæ¨¡å¼æ•°æ®
        
        Args:
            patterns: è¡Œä¸ºæ¨¡å¼åˆ—è¡¨
            
        Returns:
            Dict: æ¨¡å¼ç»Ÿè®¡
        """
        if not patterns:
            return {"total_patterns": 0, "common_patterns": [], "pattern_evolution": []}
        
        # æå–æ‰€æœ‰æ¨¡å¼ä¿¡æ¯
        all_patterns = []
        triggers = []
        coping_strategies = []
        
        for pattern in patterns:
            if "behavioral_patterns" in pattern:
                bp = pattern["behavioral_patterns"]
                all_patterns.extend(bp.get("patterns", []))
                triggers.extend(bp.get("triggers", []))
                coping_strategies.extend(bp.get("coping_strategies", []))
        
        return {
            "total_patterns": len(patterns),
            "common_patterns": Counter(all_patterns).most_common(5),
            "common_triggers": Counter(triggers).most_common(5),
            "common_coping_strategies": Counter(coping_strategies).most_common(5),
            "pattern_evolution": self._analyze_pattern_evolution(patterns)
        }

    def _analyze_emotions(self, emotions: List[Dict]) -> Dict[str, Any]:
        """åˆ†ææƒ…ç»ªæ•°æ®
        
        Args:
            emotions: æƒ…ç»ªåˆ—è¡¨
            
        Returns:
            Dict: æƒ…ç»ªç»Ÿè®¡
        """
        if not emotions:
            return {"total_records": 0, "emotion_distribution": {}, "average_intensity": 0}
        
        # æƒ…ç»ªç±»åˆ«ç»Ÿè®¡
        emotion_categories = [emotion.get("emotion_category", "unknown") for emotion in emotions]
        category_counter = Counter(emotion_categories)
        
        # æƒ…ç»ªå¼ºåº¦ç»Ÿè®¡
        intensities = []
        for emotion in emotions:
            try:
                intensity = float(emotion.get("emotion_score", 0))
                intensities.append(intensity)
            except:
                continue
        
        return {
            "total_records": len(emotions),
            "emotion_distribution": dict(category_counter),
            "most_common_emotions": category_counter.most_common(5),
            "average_intensity": statistics.mean(intensities) if intensities else 0,
            "intensity_range": {
                "min": min(intensities) if intensities else 0,
                "max": max(intensities) if intensities else 0,
                "median": statistics.median(intensities) if intensities else 0,
                "std_dev": statistics.stdev(intensities) if len(intensities) > 1 else 0
            },
            "emotion_trend": self._analyze_emotion_trend(emotions)
        }

    def _analyze_engagement(self, user_data: Dict[str, Any]) -> Dict[str, Any]:
        """åˆ†æç”¨æˆ·å‚ä¸åº¦
        
        Args:
            user_data: ç”¨æˆ·æ•°æ®
            
        Returns:
            Dict: å‚ä¸åº¦ç»Ÿè®¡
        """
        return {
            "total_sessions": len(user_data.get("patterns", [])),
            "total_events": len(user_data.get("events", [])),
            "total_emotions": len(user_data.get("emotions", [])),
            "memory_entries": len(user_data.get("long_term_memory", [])),
            "engagement_score": self._calculate_engagement_score(user_data)
        }

    def _analyze_time_distribution(self, event_times: List[datetime]) -> Dict[str, Any]:
        """åˆ†æäº‹ä»¶æ—¶é—´åˆ†å¸ƒ
        
        Args:
            event_times: äº‹ä»¶æ—¶é—´åˆ—è¡¨
            
        Returns:
            Dict: æ—¶é—´åˆ†å¸ƒç»Ÿè®¡
        """
        if not event_times:
            return {"hourly_distribution": {}, "daily_distribution": {}, "weekly_distribution": {}}
        
        # æŒ‰å°æ—¶åˆ†å¸ƒ
        hourly = Counter([dt.hour for dt in event_times])
        # æŒ‰æ˜ŸæœŸåˆ†å¸ƒ
        weekly = Counter([dt.weekday() for dt in event_times])
        # æŒ‰æ—¥æœŸåˆ†å¸ƒ
        daily = Counter([dt.date().isoformat() for dt in event_times])
        
        return {
            "hourly_distribution": dict(hourly),
            "weekly_distribution": dict(weekly),
            "daily_distribution": dict(daily)
        }

    def _analyze_pattern_evolution(self, patterns: List[Dict]) -> List[Dict]:
        """åˆ†æè¡Œä¸ºæ¨¡å¼æ¼”å˜
        
        Args:
            patterns: è¡Œä¸ºæ¨¡å¼åˆ—è¡¨
            
        Returns:
            List: æ¨¡å¼æ¼”å˜è¶‹åŠ¿
        """
        if len(patterns) < 2:
            return []
        
        # æŒ‰æ—¶é—´æ’åº
        sorted_patterns = sorted(patterns, key=lambda x: x.get("timestamp", ""))
        
        evolution = []
        for i in range(1, len(sorted_patterns)):
            prev_pattern = sorted_patterns[i-1]
            curr_pattern = sorted_patterns[i]
            
            evolution.append({
                "period": f"{prev_pattern.get('timestamp', '')} - {curr_pattern.get('timestamp', '')}",
                "changes": self._compare_patterns(prev_pattern, curr_pattern)
            })
        
        return evolution

    def _analyze_emotion_trend(self, emotions: List[Dict]) -> Dict[str, Any]:
        """åˆ†ææƒ…ç»ªè¶‹åŠ¿
        
        Args:
            emotions: æƒ…ç»ªåˆ—è¡¨
            
        Returns:
            Dict: æƒ…ç»ªè¶‹åŠ¿
        """
        if not emotions:
            return {"trend": "stable", "recent_average": 0, "overall_direction": "neutral"}
        
        # æŒ‰æ—¶é—´æ’åº
        sorted_emotions = sorted(emotions, key=lambda x: x.get("timestamp", ""))
        
        # è®¡ç®—æœ€è¿‘ä¸€å‘¨å’Œæ€»ä½“å¹³å‡å€¼
        recent_week = [e for e in sorted_emotions if datetime.fromisoformat(e.get("timestamp", "")) >= datetime.now() - timedelta(days=7)]
        
        recent_scores = [float(e.get("emotion_score", 0)) for e in recent_week]
        overall_scores = [float(e.get("emotion_score", 0)) for e in sorted_emotions]
        
        recent_avg = statistics.mean(recent_scores) if recent_scores else 0
        overall_avg = statistics.mean(overall_scores) if overall_scores else 0
        
        # åˆ¤æ–­è¶‹åŠ¿
        if recent_avg > overall_avg + 0.5:
            trend = "improving"
        elif recent_avg < overall_avg - 0.5:
            trend = "declining"
        else:
            trend = "stable"
        
        return {
            "trend": trend,
            "recent_average": recent_avg,
            "overall_average": overall_avg,
            "recent_period_days": 7,
            "score_difference": recent_avg - overall_avg
        }

    def _compare_patterns(self, prev_pattern: Dict, curr_pattern: Dict) -> List[str]:
        """æ¯”è¾ƒä¸¤ä¸ªè¡Œä¸ºæ¨¡å¼çš„å˜åŒ–
        
        Args:
            prev_pattern: å‰ä¸€ä¸ªæ¨¡å¼
            curr_pattern: å½“å‰æ¨¡å¼
            
        Returns:
            List: å˜åŒ–æè¿°
        """
        changes = []
        
        # æ¯”è¾ƒè¡Œä¸ºæ¨¡å¼çš„ä¸»è¦å­—æ®µ
        if "behavioral_patterns" in prev_pattern and "behavioral_patterns" in curr_pattern:
            prev_bp = prev_pattern["behavioral_patterns"]
            curr_bp = curr_pattern["behavioral_patterns"]
            
            # æ¯”è¾ƒæ¨¡å¼æ•°é‡
            prev_count = len(prev_bp.get("patterns", []))
            curr_count = len(curr_bp.get("patterns", []))
            
            if curr_count > prev_count:
                changes.append(f"è¡Œä¸ºæ¨¡å¼å¢åŠ äº†{curr_count - prev_count}ä¸ª")
            elif curr_count < prev_count:
                changes.append(f"è¡Œä¸ºæ¨¡å¼å‡å°‘äº†{prev_count - curr_count}ä¸ª")
            
            # æ¯”è¾ƒåº”å¯¹ç­–ç•¥
            prev_coping = set(prev_bp.get("coping_strategies", []))
            curr_coping = set(curr_bp.get("coping_strategies", []))
            
            new_coping = curr_coping - prev_coping
            lost_coping = prev_coping - curr_coping
            
            if new_coping:
                changes.append(f"æ–°å¢åº”å¯¹ç­–ç•¥: {', '.join(new_coping)}")
            if lost_coping:
                changes.append(f"å‡å°‘åº”å¯¹ç­–ç•¥: {', '.join(lost_coping)}")
        
        return changes

    def _calculate_engagement_score(self, user_data: Dict[str, Any]) -> float:
        """è®¡ç®—ç”¨æˆ·å‚ä¸åº¦è¯„åˆ†
        
        Args:
            user_data: ç”¨æˆ·æ•°æ®
            
        Returns:
            float: å‚ä¸åº¦è¯„åˆ† (0-10)
        """
        # åŸºäºä¸åŒæ•°æ®æºçš„å‚ä¸åº¦æƒé‡
        weights = {
            "events": 0.3,
            "patterns": 0.25,
            "emotions": 0.2,
            "memory": 0.15,
            "profile": 0.1
        }
        
        scores = {
            "events": min(len(user_data.get("events", [])) / 10, 1.0),  # æœ€å¤š10ä¸ªäº‹ä»¶å¾—æ»¡åˆ†
            "patterns": min(len(user_data.get("patterns", [])) / 5, 1.0),  # æœ€å¤š5ä¸ªæ¨¡å¼å¾—æ»¡åˆ†
            "emotions": min(len(user_data.get("emotions", [])) / 20, 1.0),  # æœ€å¤š20ä¸ªæƒ…ç»ªè®°å½•å¾—æ»¡åˆ†
            "memory": min(len(user_data.get("long_term_memory", [])) / 10, 1.0),  # æœ€å¤š10ä¸ªè®°å¿†å¾—æ»¡åˆ†
            "profile": 1.0 if user_data.get("user_profile") else 0.0  # æœ‰ç”¨æˆ·ç”»åƒå¾—æ»¡åˆ†
        }
        
        engagement_score = sum(scores[key] * weights[key] for key in weights) * 10
        return round(engagement_score, 2)

    def _generate_ai_analysis(self, user_data: Dict[str, Any], statistics_data: Dict[str, Any]) -> Dict[str, Any]:
        """ä½¿ç”¨AIç”Ÿæˆæ·±åº¦åˆ†æ
        
        Args:
            user_data: ç”¨æˆ·æ•°æ®
            statistics_data: ç»Ÿè®¡æ•°æ®
            
        Returns:
            Dict: AIåˆ†æç»“æœ
        """
        try:
            # å‡†å¤‡åˆ†ææ•°æ®
            analysis_input = {
                "user_data_summary": {
                    "total_events": len(user_data.get("events", [])),
                    "total_patterns": len(user_data.get("patterns", [])),
                    "total_emotions": len(user_data.get("emotions", [])),
                    "engagement_score": statistics_data.get("engagement_statistics", {}).get("engagement_score", 0)
                },
                "statistics": statistics_data
            }
            
            # æ„å»ºæç¤ºè¯
            prompt = f"""
            {self.analysis_prompt}
            
            ç”¨æˆ·æ•°æ®åˆ†æï¼š
            {json.dumps(analysis_input, ensure_ascii=False, indent=2)}
            
            è¯·åŸºäºä»¥ä¸Šæ•°æ®ç”Ÿæˆä¸“ä¸šçš„å¿ƒç†å¥åº·åˆ†ææŠ¥å‘Šã€‚
            """
            
            # è°ƒç”¨AIç”Ÿæˆåˆ†æ
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": prompt}
                ],
                max_tokens=2000,
                temperature=0.3,
                response_format={"type": "json_object"}
            )
            
            ai_analysis = json.loads(response.choices[0].message.content)
            return ai_analysis
            
        except Exception as e:
            print(f"Error in AI analysis: {str(e)}")
            return {
                "error": "AIåˆ†æç”Ÿæˆå¤±è´¥",
                "fallback_analysis": self._generate_fallback_analysis(statistics_data)
            }

    def _generate_fallback_analysis(self, statistics_data: Dict[str, Any]) -> Dict[str, Any]:
        """ç”Ÿæˆå¤‡ç”¨åˆ†æï¼ˆå½“AIåˆ†æå¤±è´¥æ—¶ä½¿ç”¨ï¼‰
        
        Args:
            statistics_data: ç»Ÿè®¡æ•°æ®
            
        Returns:
            Dict: å¤‡ç”¨åˆ†æç»“æœ
        """
        event_stats = statistics_data.get("event_statistics", {})
        emotion_stats = statistics_data.get("emotion_statistics", {})
        
        return {
            "summary": {
                "overallStatus": "éœ€è¦è¿›ä¸€æ­¥åˆ†æ",
                "keyFindings": [
                    f"å…±è®°å½•äº†{event_stats.get('total_events', 0)}ä¸ªäº‹ä»¶",
                    f"æƒ…ç»ªè®°å½•å¹³å‡å¼ºåº¦ä¸º{emotion_stats.get('average_intensity', 0):.1f}",
                    f"å‚ä¸åº¦è¯„åˆ†ä¸º{statistics_data.get('engagement_statistics', {}).get('engagement_score', 0)}"
                ],
                "riskLevel": "unknown",
                "progressTrend": "éœ€è¦æ›´å¤šæ•°æ®"
            },
            "recommendations": {
                "immediate": ["ç»§ç»­è®°å½•æƒ…ç»ªå’Œäº‹ä»¶", "ä¿æŒå®šæœŸäº¤æµ"],
                "shortTerm": ["å»ºç«‹è§„å¾‹çš„è‡ªæˆ‘åæ€ä¹ æƒ¯"],
                "longTerm": ["æŒç»­å…³æ³¨å¿ƒç†å¥åº·çŠ¶å†µ"]
            }
        }

    def export_report(self, report: Dict[str, Any], format_type: str = "json") -> str:
        """å¯¼å‡ºæŠ¥å‘Š
        
        Args:
            report: æŠ¥å‘Šæ•°æ®
            format_type: å¯¼å‡ºæ ¼å¼ ("json", "text")
            
        Returns:
            str: å¯¼å‡ºçš„æŠ¥å‘Šå†…å®¹
        """
        if format_type == "json":
            return json.dumps(report, ensure_ascii=False, indent=2)
        elif format_type == "text":
            return self._format_report_as_text(report)
        else:
            raise ValueError(f"Unsupported format type: {format_type}")

    def _format_report_as_text(self, report: Dict[str, Any]) -> str:
        """å°†æŠ¥å‘Šæ ¼å¼åŒ–ä¸ºæ–‡æœ¬
        
        Args:
            report: æŠ¥å‘Šæ•°æ®
            
        Returns:
            str: æ–‡æœ¬æ ¼å¼çš„æŠ¥å‘Š
        """
        text_report = []
        text_report.append("=" * 50)
        text_report.append("ç”¨æˆ·å¿ƒç†å¥åº·åˆ†ææŠ¥å‘Š")
        text_report.append("=" * 50)
        text_report.append(f"ç”¨æˆ·ID: {report.get('user_id', 'Unknown')}")
        text_report.append(f"ç”Ÿæˆæ—¶é—´: {report.get('generated_at', 'Unknown')}")
        text_report.append(f"åˆ†æå‘¨æœŸ: {report.get('analysis_period', {}).get('days', 'Unknown')}å¤©")
        text_report.append("")
        
        # æ•°æ®æ‘˜è¦
        metadata = report.get("metadata", {})
        text_report.append("æ•°æ®æ‘˜è¦:")
        text_report.append(f"- åˆ†æä¼šè¯æ•°: {metadata.get('sessions_analyzed', 0)}")
        text_report.append(f"- äº‹ä»¶æ€»æ•°: {metadata.get('total_events', 0)}")
        text_report.append(f"- è¡Œä¸ºæ¨¡å¼æ•°: {metadata.get('total_patterns', 0)}")
        text_report.append(f"- æƒ…ç»ªè®°å½•æ•°: {metadata.get('emotion_records', 0)}")
        text_report.append("")
        
        # AIåˆ†æç»“æœ
        ai_analysis = report.get("ai_analysis", {})
        if "summary" in ai_analysis:
            summary = ai_analysis["summary"]
            text_report.append("æ•´ä½“è¯„ä¼°:")
            text_report.append(f"- å¥åº·çŠ¶æ€: {summary.get('overallStatus', 'Unknown')}")
            text_report.append(f"- é£é™©ç­‰çº§: {summary.get('riskLevel', 'Unknown')}")
            text_report.append(f"- è¿›æ­¥è¶‹åŠ¿: {summary.get('progressTrend', 'Unknown')}")
            text_report.append("")
            
            key_findings = summary.get("keyFindings", [])
            if key_findings:
                text_report.append("ä¸»è¦å‘ç°:")
                for finding in key_findings:
                    text_report.append(f"- {finding}")
                text_report.append("")
        
        # å»ºè®®
        if "recommendations" in ai_analysis:
            recommendations = ai_analysis["recommendations"]
            text_report.append("å»ºè®®:")
            
            immediate = recommendations.get("immediate", [])
            if immediate:
                text_report.append("ç«‹å³å»ºè®®:")
                for rec in immediate:
                    text_report.append(f"- {rec}")
            
            short_term = recommendations.get("shortTerm", [])
            if short_term:
                text_report.append("çŸ­æœŸå»ºè®®:")
                for rec in short_term:
                    text_report.append(f"- {rec}")
            
            long_term = recommendations.get("longTerm", [])
            if long_term:
                text_report.append("é•¿æœŸå»ºè®®:")
                for rec in long_term:
                    text_report.append(f"- {rec}")
        
        text_report.append("")
        text_report.append("=" * 50)
        text_report.append("æŠ¥å‘Šç»“æŸ")
        text_report.append("=" * 50)
        
        return "\n".join(text_report) 