#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
å¿ƒç†å’¨è¯¢å¯¹è¯ç³»ç»Ÿ - å¿«é€Ÿå“åº”ç‰ˆæœ¬

ä¸“æ³¨äºè§£å†³å“åº”é€Ÿåº¦é—®é¢˜çš„ç®€åŒ–ç‰ˆæœ¬ï¼š
1. å‡å°‘ä¸å¿…è¦çš„LLMè°ƒç”¨
2. ä¼˜åŒ–æ•°æ®åº“æ“ä½œ
3. æ™ºèƒ½ç¼“å­˜
4. å¼‚æ­¥å¤„ç†éå…³é”®è·¯å¾„
"""

import os
import json
import sys
from pathlib import Path
import time
from typing import Dict, List, Tuple, Any, Optional
from concurrent.futures import ThreadPoolExecutor
import threading

sys.path.append(str(Path(__file__).parent.parent))

from datetime import datetime
import re
from dotenv import load_dotenv
from langchain_core.messages import HumanMessage
from langchain_openai import ChatOpenAI
from langgraph.graph import StateGraph, END
from pydantic import BaseModel, Field
from snownlp import SnowNLP

from utils.extract_json import extract_json
from dao.database import Database

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# å¿«é€Ÿå“åº”çŠ¶æ€æ¨¡å‹
class FastSessionState(BaseModel):
    user_input: str
    user_id: str = "default_user"
    session_id: str | None = None
    response: str | None = None
    history: List[Dict[str, str]] = Field(default_factory=list)
    crisis_detected: bool = False
    crisis_reason: str | None = None
    emotion: str = "neutral"
    skip_plan: bool = False
    skip_search: bool = False
    processing_time: float = 0.0

# å¿«é€Ÿå±æœºæ£€æµ‹å™¨ï¼ˆä½¿ç”¨ç¼“å­˜ï¼‰
class FastCrisisDetector:
    def __init__(self):
        self.high_risk_keywords = {"è‡ªæ€", "æƒ³æ­»", "æ´»ä¸ä¸‹å»", "ç»“æŸç”Ÿå‘½"}
        self.cache = {}
        self.cache_lock = threading.Lock()
    
    def check(self, text: str) -> Tuple[bool, str]:
        with self.cache_lock:
            if text in self.cache:
                return self.cache[text]
        
        # å¿«é€Ÿå…³é”®è¯æ£€æµ‹
        for keyword in self.high_risk_keywords:
            if keyword in text:
                result = (True, f"æ£€æµ‹åˆ°é«˜å±è¯: '{keyword}'")
                with self.cache_lock:
                    self.cache[text] = result
                return result
        
        result = (False, "")
        with self.cache_lock:
            self.cache[text] = result
        return result

# å¿«é€ŸèŠå¤©æœåŠ¡
class FastChatService:
    def __init__(self, database: Database):
        self.db = database
        self.client = ChatOpenAI(
            model=os.environ.get("MODEL_NAME", "deepseek-chat"),
            api_key=os.environ.get("OPENAI_API_KEY"),
            base_url=os.environ.get("BASE_URL", "https://api.deepseek.com/v1"),
            temperature=float(os.environ.get("TEMPERATURE", "0.7")),
            max_tokens=int(os.environ.get("MAX_TOKENS", "800")),  # å‡å°‘tokenæ•°
            timeout=25,  # å‡å°‘è¶…æ—¶æ—¶é—´
        )
        
        # ç®€åŒ–çš„æç¤ºè¯
        self.prompt = """ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„å¿ƒç†å’¨è¯¢å¸ˆã€‚è¯·ç»™å‡ºç®€æ´ã€æœ‰ç”¨çš„å›å¤ï¼Œä¸“æ³¨äºï¼š
1. å…±æƒ…å’Œç†è§£
2. å®ç”¨å»ºè®®
3. é€‚å½“çš„é—®é¢˜å¼•å¯¼

ä¿æŒå›å¤ç®€æ´ä½†æœ‰æ¸©åº¦ã€‚"""
        
        # çº¿ç¨‹æ± ç”¨äºåå°ä»»åŠ¡
        self.executor = ThreadPoolExecutor(max_workers=2)
        
        # å“åº”ç¼“å­˜
        self.response_cache = {}
        self.cache_lock = threading.Lock()
    
    def get_cached_response(self, user_input: str) -> Optional[str]:
        """è·å–ç¼“å­˜çš„å“åº”"""
        with self.cache_lock:
            return self.response_cache.get(user_input)
    
    def cache_response(self, user_input: str, response: str):
        """ç¼“å­˜å“åº”"""
        with self.cache_lock:
            if len(self.response_cache) > 100:  # é™åˆ¶ç¼“å­˜å¤§å°
                # ç§»é™¤æœ€æ—§çš„ç¼“å­˜
                oldest_key = next(iter(self.response_cache))
                del self.response_cache[oldest_key]
            self.response_cache[user_input] = response
    
    def should_use_simple_response(self, user_input: str) -> Optional[str]:
        """åˆ¤æ–­æ˜¯å¦å¯ä»¥ä½¿ç”¨ç®€å•çš„é¢„è®¾å“åº”"""
        simple_responses = {
            "ä½ å¥½": "ä½ å¥½ï¼æˆ‘æ˜¯å¿ƒç†å’¨è¯¢å¸ˆï¼Œå¾ˆé«˜å…´ä¸ºæ‚¨æœåŠ¡ã€‚æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®åŠ©æ‚¨çš„å—ï¼Ÿ",
            "è°¢è°¢": "ä¸ç”¨è°¢ï¼Œæˆ‘å¾ˆé«˜å…´èƒ½å¸®åŠ©åˆ°æ‚¨ã€‚è¿˜æœ‰å…¶ä»–éœ€è¦èŠçš„å—ï¼Ÿ",
            "å¥½çš„": "å¥½çš„ï¼Œæˆ‘æ˜ç™½äº†ã€‚è¿˜æœ‰ä»€ä¹ˆæƒ³åˆ†äº«çš„å—ï¼Ÿ",
            "å—¯": "æˆ‘åœ¨å¬ï¼Œè¯·ç»§ç»­è¯´ã€‚",
            "æ˜¯çš„": "å¥½çš„ï¼Œæˆ‘ç†è§£ã€‚è¯·ç»§ç»­ã€‚",
            "å†è§": "å†è§ï¼è®°ä½ï¼Œæˆ‘éšæ—¶éƒ½åœ¨è¿™é‡Œã€‚ç¥æ‚¨ä¸€åˆ‡é¡ºåˆ©ï¼"
        }
        
        user_input_clean = user_input.strip()
        return simple_responses.get(user_input_clean)
    
    def extract_emotion(self, content: str) -> str:
        """å¿«é€Ÿæƒ…ç»ªæå–"""
        content_lower = content.lower()
        
        if any(word in content_lower for word in ["å¼€å¿ƒ", "é«˜å…´", "å¿«ä¹", "å¾ˆå¥½"]):
            return "happy"
        elif any(word in content_lower for word in ["æ‚²ä¼¤", "éš¾è¿‡", "æŠ‘éƒ", "ç—›è‹¦"]):
            return "sad"
        elif any(word in content_lower for word in ["ç”Ÿæ°”", "æ„¤æ€’", "çƒ¦èº"]):
            return "angry"
        elif any(word in content_lower for word in ["ç´¯", "ç–²æƒ«", "å›°"]):
            return "sleepy"
        
        return "neutral"
    
    def async_save_data(self, user_id: str, session_id: str, user_input: str, response: str, emotion: str):
        """å¼‚æ­¥ä¿å­˜æ•°æ®"""
        def save_task():
            try:
                # è®¡ç®—æƒ…ç»ªè¯„åˆ†
                emotion_score = SnowNLP(user_input).sentiments * 2 - 1
                
                # ä¿å­˜æƒ…ç»ªè¯„åˆ†
                self.db.save_emotion_score(user_id, session_id, emotion_score, emotion)
                
                # æ›´æ–°ç”¨æˆ·ç”»åƒ
                self.db.save_user_profile(user_id, {
                    "last_interaction": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                    "recent_emotion_score": emotion_score,
                    "recent_emotion": emotion
                })
                
                # å¶å°”ä¿å­˜åˆ°é•¿æœŸè®°å¿†
                if emotion_score < -0.5 or "å›°æ‰°" in user_input or "é—®é¢˜" in user_input:
                    self.db.save_long_term_memory(user_id, f"ç”¨æˆ·: {user_input}\nå’¨è¯¢å¸ˆ: {response}")
                
            except Exception as e:
                print(f"Background save error: {e}")
        
        # æäº¤åˆ°çº¿ç¨‹æ± 
        self.executor.submit(save_task)

# åˆå§‹åŒ–æœåŠ¡
db = Database()
crisis_detector = FastCrisisDetector()
chat_service = FastChatService(db)

# === LangGraph èŠ‚ç‚¹ï¼ˆç®€åŒ–ç‰ˆï¼‰ ===

def fast_preprocess(state: FastSessionState) -> FastSessionState:
    """å¿«é€Ÿé¢„å¤„ç†"""
    start_time = time.time()
    
    state.user_input = state.user_input.strip()
    state.session_id = state.session_id or state.user_id
    
    # æ£€æŸ¥æ˜¯å¦å¯ä»¥ä½¿ç”¨ç®€å•å“åº”
    simple_response = chat_service.should_use_simple_response(state.user_input)
    if simple_response:
        state.response = simple_response
        state.skip_plan = True
        state.skip_search = True
        return state
    
    # æ£€æŸ¥ç¼“å­˜
    cached_response = chat_service.get_cached_response(state.user_input)
    if cached_response:
        state.response = cached_response
        state.skip_plan = True
        state.skip_search = True
        return state
    
    state.processing_time += time.time() - start_time
    return state

def fast_crisis_check(state: FastSessionState) -> FastSessionState:
    """å¿«é€Ÿå±æœºæ£€æµ‹"""
    start_time = time.time()
    
    if state.response:  # å¦‚æœå·²æœ‰å“åº”ï¼Œè·³è¿‡
        return state
    
    crisis, reason = crisis_detector.check(state.user_input)
    if crisis:
        state.crisis_detected = True
        state.crisis_reason = reason
        state.response = (
            f"âš ï¸ æˆ‘æ³¨æ„åˆ°æ‚¨å¯èƒ½é‡åˆ°äº†ä¸¥é‡çš„æƒ…ç»ªå›°æ‰°ã€‚\n"
            f"è¯·ç«‹å³è”ç³»ä¸“ä¸šå¿ƒç†æ´åŠ©çƒ­çº¿ 400-161-9995ï¼Œæˆ–å¯»æ±‚ä¸“ä¸šå¸®åŠ©ã€‚\n"
            f"æ‚¨çš„ç”Ÿå‘½å¾ˆå®è´µï¼Œè¯·ä¸è¦ç‹¬è‡ªæ‰¿å—ã€‚"
        )
        state.skip_plan = True
        state.skip_search = True
    
    state.processing_time += time.time() - start_time
    return state

def fast_generate_response(state: FastSessionState) -> FastSessionState:
    """å¿«é€Ÿç”Ÿæˆå“åº”"""
    start_time = time.time()
    
    if state.response:  # å¦‚æœå·²æœ‰å“åº”ï¼Œè·³è¿‡
        return state
    
    try:
        # æ„å»ºç®€åŒ–çš„æ¶ˆæ¯
        messages = [
            {"role": "system", "content": chat_service.prompt},
            {"role": "user", "content": state.user_input}
        ]
        
        # æ·»åŠ æœ€è¿‘çš„å†å²ï¼ˆæœ€å¤š2æ¡ï¼‰
        if state.history:
            recent_history = state.history[-2:]
            for msg in recent_history:
                if msg["role"] == "user":
                    messages.insert(-1, {"role": "user", "content": msg["content"]})
                elif msg["role"] == "agent":
                    messages.insert(-1, {"role": "assistant", "content": msg["content"]})
        
        # è°ƒç”¨LLM
        response = chat_service.client.invoke(messages)
        reply = response.content.strip() or "æˆ‘ç†è§£æ‚¨çš„æ„Ÿå—ï¼Œè¯·ç»§ç»­åˆ†äº«ã€‚"
        
        state.response = reply
        state.emotion = chat_service.extract_emotion(state.user_input)
        
        # ç¼“å­˜å“åº”
        chat_service.cache_response(state.user_input, reply)
        
    except Exception as e:
        state.response = f"æŠ±æ­‰ï¼Œç³»ç»Ÿæš‚æ—¶å‡ºç°é—®é¢˜ã€‚è®©æˆ‘ä»¬æ¢ä¸ªè¯é¢˜èŠèŠå§ã€‚"
        print(f"LLM Error: {e}")
    
    state.processing_time += time.time() - start_time
    return state

def fast_postprocess(state: FastSessionState) -> FastSessionState:
    """å¿«é€Ÿåå¤„ç†"""
    start_time = time.time()
    
    # æ›´æ–°å†å²è®°å½•
    state.history.append({"role": "user", "content": state.user_input})
    state.history.append({"role": "agent", "content": state.response})
    
    # ä¿æŒå†å²è®°å½•æ•°é‡åœ¨åˆç†èŒƒå›´
    if len(state.history) > 20:
        state.history = state.history[-20:]
    
    # å¼‚æ­¥ä¿å­˜æ•°æ®
    chat_service.async_save_data(
        state.user_id, 
        state.session_id, 
        state.user_input, 
        state.response, 
        state.emotion
    )
    
    state.processing_time += time.time() - start_time
    return state

# === è·¯ç”±å‡½æ•° ===

def should_skip_llm(state: FastSessionState) -> str:
    """æ£€æŸ¥æ˜¯å¦åº”è¯¥è·³è¿‡LLMè°ƒç”¨"""
    if state.response:
        return "postprocess"
    return "generate"

# === æ„å»ºå¿«é€Ÿå·¥ä½œæµ ===

fast_workflow = StateGraph(FastSessionState)

# æ·»åŠ èŠ‚ç‚¹
fast_workflow.add_node("preprocess", fast_preprocess)
fast_workflow.add_node("crisis_check", fast_crisis_check)
fast_workflow.add_node("generate", fast_generate_response)
fast_workflow.add_node("postprocess", fast_postprocess)

# è®¾ç½®å…¥å£ç‚¹
fast_workflow.set_entry_point("preprocess")

# æ·»åŠ è¾¹
fast_workflow.add_edge("preprocess", "crisis_check")
fast_workflow.add_conditional_edges(
    "crisis_check",
    should_skip_llm,
    {
        "generate": "generate",
        "postprocess": "postprocess"
    }
)
fast_workflow.add_edge("generate", "postprocess")
fast_workflow.add_edge("postprocess", END)

# ç¼–è¯‘å·¥ä½œæµ
fast_chat_app = fast_workflow.compile()

# === ä¸»è¦æ¥å£ ===

def fast_chat(
    user_input: str,
    user_id: str = "default_user",
    session_id: str | None = None,
    history: List[Dict[str, str]] | None = None,
    show_timing: bool = False
) -> Dict[str, Any]:
    """
    å¿«é€ŸèŠå¤©æ¥å£
    
    ä¸“æ³¨äºé€Ÿåº¦ä¼˜åŒ–çš„èŠå¤©åŠŸèƒ½
    """
    
    overall_start = time.time()
    
    # åˆå§‹åŒ–çŠ¶æ€
    init_state = FastSessionState(
        user_input=user_input,
        user_id=user_id,
        session_id=session_id,
        history=history or []
    )
    
    try:
        # è¿è¡Œå·¥ä½œæµ
        result = fast_chat_app.invoke(init_state)
        
        total_time = time.time() - overall_start
        
        # æ„å»ºå“åº”
        response_data = {
            "response": result.get("response"),
            "emotion": result.get("emotion", "neutral"),
            "history": result.get("history", []),
            "crisis_detected": result.get("crisis_detected", False),
            "crisis_reason": result.get("crisis_reason"),
            "total_time": total_time
        }
        
        if show_timing:
            response_data["processing_time"] = result.get("processing_time", 0)
            response_data["llm_time"] = total_time - result.get("processing_time", 0)
        
        return response_data
        
    except Exception as e:
        total_time = time.time() - overall_start
        print(f"Fast chat error: {e}")
        return {
            "response": f"æŠ±æ­‰ï¼Œå¤„ç†å‡ºç°é—®é¢˜ã€‚è®©æˆ‘ä»¬é‡æ–°å¼€å§‹å§ã€‚",
            "emotion": "neutral",
            "history": history or [],
            "crisis_detected": False,
            "crisis_reason": None,
            "total_time": total_time
        }

# === æ€§èƒ½æµ‹è¯•å‡½æ•° ===

def benchmark_fast_chat():
    """æ€§èƒ½åŸºå‡†æµ‹è¯•"""
    test_messages = [
        "ä½ å¥½",
        "æˆ‘ä»Šå¤©å¿ƒæƒ…ä¸å¥½",
        "ä»€ä¹ˆæ˜¯ç„¦è™‘ç—‡",
        "æˆ‘è¯¥æ€ä¹ˆåŠ",
        "è°¢è°¢ä½ çš„å»ºè®®",
        "æˆ‘æ„Ÿè§‰å¾ˆæ²®ä¸§ï¼Œä¸çŸ¥é“è¯¥å¦‚ä½•æ˜¯å¥½",
        "æœ€è¿‘å·¥ä½œå‹åŠ›å¾ˆå¤§",
        "æˆ‘éœ€è¦ä¸“ä¸šå¸®åŠ©å—",
        "å¥½çš„ï¼Œæˆ‘æ˜ç™½äº†",
        "å†è§"
    ]
    
    print("ğŸš€ å¿«é€ŸèŠå¤©ç³»ç»Ÿæ€§èƒ½æµ‹è¯•")
    print("=" * 50)
    
    times = []
    for i, msg in enumerate(test_messages):
        print(f"\nğŸ“ æµ‹è¯• {i+1}: {msg}")
        
        start = time.time()
        result = fast_chat(msg, show_timing=True)
        end = time.time()
        
        actual_time = end - start
        times.append(actual_time)
        
        print(f"  âœ… æ€»è€—æ—¶: {actual_time:.2f}s")
        print(f"  ğŸ“Š å¤„ç†æ—¶é—´: {result.get('processing_time', 0):.2f}s")
        print(f"  ğŸ¤– LLMæ—¶é—´: {result.get('llm_time', 0):.2f}s")
        print(f"  ğŸ’¬ å“åº”: {result['response'][:50]}...")
    
    # ç»Ÿè®¡
    print(f"\nğŸ“ˆ æ€»ä½“ç»Ÿè®¡:")
    print(f"  å¹³å‡å“åº”æ—¶é—´: {sum(times)/len(times):.2f}s")
    print(f"  æœ€å¿«å“åº”: {min(times):.2f}s")
    print(f"  æœ€æ…¢å“åº”: {max(times):.2f}s")
    print(f"  æ€»æµ‹è¯•æ—¶é—´: {sum(times):.2f}s")

if __name__ == "__main__":
    import sys
    
    if len(sys.argv) > 1 and sys.argv[1] == "benchmark":
        benchmark_fast_chat()
    else:
        print("ğŸš€ å¿«é€Ÿå¿ƒç†å’¨è¯¢ç³»ç»Ÿ (è¾“å…¥ 'é€€å‡º' ç»“æŸ)")
        print("ğŸ’¡ è¾“å…¥ 'time' å¯æŸ¥çœ‹è¯¦ç»†è€—æ—¶ä¿¡æ¯")
        
        history = []
        while True:
            try:
                user_input = input("æ‚¨: ").strip()
            except (EOFError, KeyboardInterrupt):
                print("\nğŸ‘‹ å†è§ï¼")
                break
            
            if user_input.lower() in {"é€€å‡º", "quit", "q"}:
                print("ğŸ‘‹ å†è§ï¼ç¥æ‚¨ä¸€åˆ‡é¡ºåˆ©ã€‚")
                break
            
            show_timing = user_input.lower() == "time"
            if show_timing:
                user_input = input("è¯·è¾“å…¥æµ‹è¯•æ¶ˆæ¯: ").strip()
            
            result = fast_chat(user_input, history=history, show_timing=show_timing)
            
            print(f"å’¨è¯¢å¸ˆ: {result['response']}")
            
            if show_timing:
                print(f"âš¡ å“åº”æ—¶é—´: {result['total_time']:.2f}s")
                if 'processing_time' in result:
                    print(f"   å¤„ç†æ—¶é—´: {result['processing_time']:.2f}s")
                    print(f"   LLMæ—¶é—´: {result['llm_time']:.2f}s")
            
            history = result['history']
