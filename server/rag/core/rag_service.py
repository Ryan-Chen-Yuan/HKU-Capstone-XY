#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
RAGæ ¸å¿ƒæœåŠ¡
"""

import logging
from typing import List, Dict, Any, Optional, Tuple
from pathlib import Path
from tqdm import tqdm

from .knowledge_scanner import KnowledgeScanner, DocumentInfo
from .qwen_vector_store import QwenVectorStore
from .rag_retriever import RAGRetriever, RerankedResult

logger = logging.getLogger(__name__)


class RAGCoreService:
    """RAGæ ¸å¿ƒæœåŠ¡"""
    
    def __init__(self, 
                 knowledge_source_dir: str, 
                 data_dir: str,
                 embedding_model_path: str = None,
                 rerank_model_path: str = None,
                 device: str = None,
                 chunk_size: int = 512,
                 chunk_overlap: int = 50):
        """
        åˆå§‹åŒ–RAGæ ¸å¿ƒæœåŠ¡
        
        Args:
            knowledge_source_dir: çŸ¥è¯†æºç›®å½•
            data_dir: æ•°æ®ç›®å½•
            embedding_model_path: åµŒå…¥æ¨¡å‹è·¯å¾„
            rerank_model_path: é‡æ’åºæ¨¡å‹è·¯å¾„
            device: è®¡ç®—è®¾å¤‡
            chunk_size: æ–‡æ¡£åˆ†å—å¤§å°
            chunk_overlap: åˆ†å—é‡å å¤§å°
        """
        self.knowledge_source_dir = knowledge_source_dir
        self.data_dir = data_dir
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.scanner = KnowledgeScanner(
            knowledge_source_dir, 
            data_dir,
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap
        )
        self.vector_store = QwenVectorStore(
            data_dir, 
            embedding_model_path=embedding_model_path,
            device=device
        )
        self.retriever = RAGRetriever(
            self.vector_store,
            rerank_model_path=rerank_model_path,
            device=device
        )
        
        self.is_initialized = False
        
        logger.info("RAGæ ¸å¿ƒæœåŠ¡åˆå§‹åŒ–å®Œæˆ")
    
    def initialize(self) -> bool:
        """
        åˆå§‹åŒ–RAGç³»ç»Ÿ
        - æ‰«æçŸ¥è¯†åº“æ–‡ä»¶
        - åŠ è½½/æ„å»ºå‘é‡ç´¢å¼•
        - å¤„ç†æ–°æ–‡ä»¶
        
        Returns:
            æ˜¯å¦åˆå§‹åŒ–æˆåŠŸ
        """
        try:
            logger.info("å¼€å§‹åˆå§‹åŒ–RAGç³»ç»Ÿ...")
            
            # 1. å°è¯•åŠ è½½å·²æœ‰ç´¢å¼•
            index_loaded = self.vector_store.load_index()
            
            # 2. æ£€æŸ¥æ˜¯å¦æœ‰æ–‡ä»¶å˜åŒ–ï¼ˆå¿«é€Ÿæ£€æŸ¥ï¼‰
            if not self.scanner.has_changes():
                if index_loaded:
                    logger.info("çŸ¥è¯†åº“æ— å˜åŒ–ï¼Œè·³è¿‡æ–‡ä»¶æ‰«æå’Œå¤„ç†")
                    self.is_initialized = True
                    return True
                else:
                    logger.info("é¦–æ¬¡è¿è¡Œï¼Œéœ€è¦æ‰«æå’Œå¤„ç†æ–‡ä»¶")
            else:
                logger.info("æ£€æµ‹åˆ°çŸ¥è¯†åº“å˜åŒ–ï¼Œå¼€å§‹æ‰«ææ–‡ä»¶")
            
            # 3. æ‰«ææ–°æ–‡ä»¶å’Œä¿®æ”¹çš„æ–‡ä»¶
            new_files = self.scanner.scan_for_new_files()
            
            # 4. å¤„ç†éœ€è¦æ›´æ–°çš„æ–‡ä»¶
            if new_files:
                logger.info(f"å‘ç° {len(new_files)} ä¸ªéœ€è¦å¤„ç†çš„æ–‡ä»¶")
                processed_count = self._process_new_files(new_files)
                logger.info(f"æˆåŠŸå¤„ç† {processed_count} ä¸ªæ–‡ä»¶")
                
                # ä¿å­˜æ›´æ–°åçš„ç´¢å¼•
                self.vector_store.save_index()
            elif not index_loaded:
                logger.warning("æ²¡æœ‰æ‰¾åˆ°å·²æœ‰ç´¢å¼•ï¼Œä¸”æ— æ–°æ–‡ä»¶å¯å¤„ç†")
                logger.warning("å°è¯•é‡æ–°æ‰«ææ‰€æœ‰æ–‡ä»¶...")
                
                # æ¸…ç†å·²æœ‰çš„processedæ ‡è®°ï¼Œé‡æ–°å¤„ç†æ‰€æœ‰æ–‡ä»¶
                for doc_info in self.scanner.processed_files.values():
                    doc_info.processed = False
                    doc_info.chunks_count = 0
                
                # é‡æ–°æ‰«æ
                new_files = self.scanner.scan_for_new_files()
                if new_files:
                    processed_count = self._process_new_files(new_files)
                    logger.info(f"é‡æ–°å¤„ç† {processed_count} ä¸ªæ–‡ä»¶")
                    self.vector_store.save_index()
                else:
                    logger.warning("RAGç³»ç»Ÿå°†åœ¨ç©ºç´¢å¼•çŠ¶æ€ä¸‹è¿è¡Œ")
            else:
                logger.info("å‘é‡ç´¢å¼•å·²åŠ è½½ï¼Œæ— æ–°æ–‡ä»¶éœ€è¦å¤„ç†")
            
            self.is_initialized = True
            logger.info("RAGç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
            return True
            
        except Exception as e:
            logger.error(f"RAGç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥: {e}")
            return False
    
    def _process_new_files(self, new_files: List[DocumentInfo]) -> int:
        """å¤„ç†æ–°æ–‡ä»¶"""
        documents = []
        processed_count = 0
        
        print(f"ğŸ“ å¼€å§‹å¤„ç† {len(new_files)} ä¸ªæ–°æ–‡ä»¶...")
        
        # æ·»åŠ è¿›åº¦æ¡ç”¨äºæ–‡æ¡£å†…å®¹æå–
        for doc_info in tqdm(new_files, desc="æå–æ–‡æ¡£å†…å®¹", unit="ä¸ª"):
            try:
                # æå–æ–‡æ¡£å†…å®¹
                content = self.scanner.extract_document_content(doc_info)
                if content.strip():
                    documents.append((content, doc_info.file_name))
                    logger.info(f"æå–æ–‡æ¡£å†…å®¹: {doc_info.file_name} ({len(content)} å­—ç¬¦)")
                else:
                    logger.warning(f"æ–‡æ¡£å†…å®¹ä¸ºç©º: {doc_info.file_name}")
                    continue
                    
            except Exception as e:
                logger.error(f"å¤„ç†æ–‡æ¡£å¤±è´¥ {doc_info.file_name}: {e}")
                continue
        
        # æ‰¹é‡æ·»åŠ åˆ°å‘é‡å­˜å‚¨
        if documents:
            try:
                print(f"ğŸ“Š å¼€å§‹å‘é‡åŒ– {len(documents)} ä¸ªæœ‰æ•ˆæ–‡æ¡£...")
                chunks_count = self.vector_store.add_documents(documents)
                
                # æ ‡è®°æ–‡ä»¶ä¸ºå·²å¤„ç†
                print("ğŸ“ æ ‡è®°æ–‡ä»¶ä¸ºå·²å¤„ç†...")
                for doc_info in tqdm(new_files, desc="æ›´æ–°çŠ¶æ€", unit="ä¸ª"):
                    # ä»åŸå§‹æ–‡æ¡£ä¸­æŸ¥æ‰¾å¯¹åº”çš„å†…å®¹æ¥è®¡ç®—chunkæ•°é‡
                    content = self.scanner.extract_document_content(doc_info)
                    if content.strip():
                        # ä¼°ç®—è¿™ä¸ªæ–‡ä»¶çš„chunkæ•°é‡
                        estimated_chunks = max(1, len(content) // self.vector_store.chunk_size)
                        self.scanner.mark_as_processed(doc_info.file_path, estimated_chunks)
                        processed_count += 1
                
                print(f"âœ… æ–‡ä»¶å¤„ç†å®Œæˆ: {processed_count} ä¸ªæ–‡ä»¶ï¼Œ{chunks_count} ä¸ªæ–‡æ¡£å—")
                
            except Exception as e:
                logger.error(f"å‘é‡åŒ–æ–‡æ¡£å¤±è´¥: {e}")
                print(f"âŒ å‘é‡åŒ–æ–‡æ¡£å¤±è´¥: {e}")
        
        return processed_count
    
    def search(self, 
               query: str, 
               top_k: int = 5, 
               use_rerank: bool = True) -> List[RerankedResult]:
        """
        æ‰§è¡ŒRAGæœç´¢
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            top_k: è¿”å›ç»“æœæ•°é‡
            use_rerank: æ˜¯å¦ä½¿ç”¨é‡æ’åº
            
        Returns:
            æœç´¢ç»“æœåˆ—è¡¨
        """
        if not self.is_initialized:
            logger.warning("RAGç³»ç»Ÿæœªåˆå§‹åŒ–")
            return []
        
        return self.retriever.search(query, top_k, use_rerank)
    
    def get_context_for_query(self, 
                             query: str, 
                             top_k: int = 3, 
                             use_rerank: bool = True) -> str:
        """
        ä¸ºæŸ¥è¯¢ç”Ÿæˆä¸Šä¸‹æ–‡
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            top_k: ä½¿ç”¨çš„æ–‡æ¡£å—æ•°é‡
            use_rerank: æ˜¯å¦ä½¿ç”¨é‡æ’åº
            
        Returns:
            æ ¼å¼åŒ–çš„ä¸Šä¸‹æ–‡å­—ç¬¦ä¸²
        """
        if not self.is_initialized:
            logger.warning("RAGç³»ç»Ÿæœªåˆå§‹åŒ–")
            return ""
        
        return self.retriever.get_context_for_query(query, top_k, use_rerank)
    
    def force_rebuild_index(self):
        """å¼ºåˆ¶é‡å»ºç´¢å¼•"""
        logger.info("å¼€å§‹å¼ºåˆ¶é‡å»ºå‘é‡ç´¢å¼•...")
        print("ğŸ”„ å¼€å§‹å¼ºåˆ¶é‡å»ºå‘é‡ç´¢å¼•...")
        
        # æ¸…ç©ºç°æœ‰ç´¢å¼•
        self.vector_store.clear()
        
        # é‡æ–°æ‰«ææ‰€æœ‰æ–‡ä»¶
        all_files = []
        knowledge_dir = Path(self.knowledge_source_dir)
        
        if knowledge_dir.exists():
            print("ğŸ“‚ æ‰«æçŸ¥è¯†åº“ç›®å½•...")
            for file_path in knowledge_dir.rglob('*'):
                if file_path.is_file() and file_path.suffix.lower() in {'.pdf', '.md', '.txt'}:
                    stat = file_path.stat()
                    doc_info = DocumentInfo(
                        file_path=str(file_path),
                        file_name=file_path.name,
                        file_hash="",  # é‡å»ºæ—¶ä¸éœ€è¦æ£€æŸ¥å“ˆå¸Œ
                        file_size=stat.st_size,
                        modification_time=stat.st_mtime,
                        document_type=file_path.suffix.lower()[1:]
                    )
                    all_files.append(doc_info)
            
            print(f"ğŸ“‹ å‘ç° {len(all_files)} ä¸ªæ–‡ä»¶å¾…å¤„ç†")
        
        # å¤„ç†æ‰€æœ‰æ–‡ä»¶
        if all_files:
            processed_count = self._process_new_files(all_files)
            
            print("ğŸ’¾ ä¿å­˜å‘é‡ç´¢å¼•...")
            self.vector_store.save_index()
            
            print(f"âœ… ç´¢å¼•é‡å»ºå®Œæˆ: å¤„ç†äº† {processed_count} ä¸ªæ–‡ä»¶")
            logger.info(f"ç´¢å¼•é‡å»ºå®Œæˆï¼Œå¤„ç†äº† {processed_count} ä¸ªæ–‡ä»¶")
        else:
            print("âš ï¸ æ²¡æœ‰æ‰¾åˆ°å¯å¤„ç†çš„æ–‡ä»¶")
            logger.warning("æ²¡æœ‰æ‰¾åˆ°å¯å¤„ç†çš„æ–‡ä»¶")
    
    def get_statistics(self) -> Dict[str, Any]:
        """è·å–ç³»ç»Ÿç»Ÿè®¡ä¿¡æ¯"""
        scanner_stats = self.scanner.get_processed_stats()
        vector_stats = self.vector_store.get_stats()
        
        return {
            'knowledge_base': scanner_stats,
            'vector_store': vector_stats,
            'is_initialized': self.is_initialized,
            'rerank_enabled': self.retriever.rerank_enabled
        }
    
    def print_debug_info(self, query: str, results: List[RerankedResult]):
        """æ‰“å°è°ƒè¯•ä¿¡æ¯"""
        self.retriever.print_debug_info(query, results)
