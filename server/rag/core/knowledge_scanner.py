#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
çŸ¥è¯†åº“æ–‡ä»¶æ‰«æå™¨
"""

import os
import json
import hashlib
import logging
from pathlib import Path
from typing import List, Dict, Set, Optional
from dataclasses import dataclass
from datetime import datetime
from tqdm import tqdm

import fitz  # PyMuPDF

logger = logging.getLogger(__name__)


@dataclass
class DocumentInfo:
    """æ–‡æ¡£ä¿¡æ¯"""
    file_path: str
    file_name: str
    file_hash: str
    file_size: int
    modification_time: float
    document_type: str  # pdf, md, txt
    processed: bool = False
    chunks_count: int = 0


class KnowledgeScanner:
    """çŸ¥è¯†åº“æ–‡ä»¶æ‰«æå™¨"""
    
    def __init__(self, knowledge_source_dir: str, data_dir: str, chunk_size: int = 512, chunk_overlap: int = 50):
        """
        åˆå§‹åŒ–æ‰«æå™¨
        
        Args:
            knowledge_source_dir: çŸ¥è¯†æºç›®å½•
            data_dir: æ•°æ®ç›®å½• (å­˜æ”¾ç´¢å¼•æ–‡ä»¶)
            chunk_size: æ–‡æ¡£åˆ†å—å¤§å°
            chunk_overlap: åˆ†å—é‡å å¤§å°
        """
        self.knowledge_source_dir = Path(knowledge_source_dir)
        self.data_dir = Path(data_dir)
        self.index_file = self.data_dir / "knowledge_index.json"
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        
        # æ”¯æŒçš„æ–‡ä»¶ç±»å‹
        self.supported_extensions = {'.pdf', '.md', '.txt'}
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        self.data_dir.mkdir(parents=True, exist_ok=True)
        
        # åŠ è½½å·²å¤„ç†æ–‡ä»¶ç´¢å¼•
        self.processed_files = self._load_index()
    
    def _load_index(self) -> Dict[str, DocumentInfo]:
        """åŠ è½½å·²å¤„ç†æ–‡ä»¶ç´¢å¼•"""
        if not self.index_file.exists():
            return {}
        
        try:
            with open(self.index_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            index = {}
            for file_path, info in data.items():
                index[file_path] = DocumentInfo(**info)
            
            logger.info(f"åŠ è½½å·²å¤„ç†æ–‡ä»¶ç´¢å¼•: {len(index)} ä¸ªæ–‡ä»¶")
            return index
            
        except Exception as e:
            logger.warning(f"åŠ è½½æ–‡ä»¶ç´¢å¼•å¤±è´¥: {e}")
            return {}
    
    def _save_index(self):
        """ä¿å­˜æ–‡ä»¶ç´¢å¼•"""
        try:
            data = {}
            for file_path, doc_info in self.processed_files.items():
                data[file_path] = {
                    'file_path': doc_info.file_path,
                    'file_name': doc_info.file_name,
                    'file_hash': doc_info.file_hash,
                    'file_size': doc_info.file_size,
                    'modification_time': doc_info.modification_time,
                    'document_type': doc_info.document_type,
                    'processed': doc_info.processed,
                    'chunks_count': doc_info.chunks_count
                }
            
            with open(self.index_file, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
                
            logger.debug(f"ä¿å­˜æ–‡ä»¶ç´¢å¼•: {len(data)} ä¸ªæ–‡ä»¶")
            
        except Exception as e:
            logger.error(f"ä¿å­˜æ–‡ä»¶ç´¢å¼•å¤±è´¥: {e}")
    
    def _calculate_file_hash(self, file_path: Path) -> str:
        """è®¡ç®—æ–‡ä»¶å“ˆå¸Œå€¼"""
        hash_md5 = hashlib.md5()
        try:
            with open(file_path, "rb") as f:
                for chunk in iter(lambda: f.read(4096), b""):
                    hash_md5.update(chunk)
            return hash_md5.hexdigest()
        except Exception as e:
            logger.error(f"è®¡ç®—æ–‡ä»¶å“ˆå¸Œå¤±è´¥ {file_path}: {e}")
            return ""
    
    def _extract_text_from_pdf(self, file_path: Path) -> str:
        """ä»PDFæå–æ–‡æœ¬"""
        try:
            doc = fitz.open(file_path)
            text = ""
            for page in doc:
                text += page.get_text()
            doc.close()
            return text.strip()
        except Exception as e:
            logger.error(f"PDFæ–‡æœ¬æå–å¤±è´¥ {file_path}: {e}")
            return ""
    
    def _extract_text_from_file(self, file_path: Path) -> str:
        """ä»æ–‡ä»¶æå–æ–‡æœ¬å†…å®¹"""
        try:
            if file_path.suffix.lower() == '.pdf':
                return self._extract_text_from_pdf(file_path)
            else:
                # txt, md æ–‡ä»¶
                with open(file_path, 'r', encoding='utf-8') as f:
                    return f.read().strip()
        except Exception as e:
            logger.error(f"æ–‡ä»¶æ–‡æœ¬æå–å¤±è´¥ {file_path}: {e}")
            return ""
    
    def scan_for_new_files(self) -> List[DocumentInfo]:
        """
        æ‰«ææ–°æ–‡ä»¶å’Œå·²ä¿®æ”¹çš„æ–‡ä»¶
        
        Returns:
            éœ€è¦å¤„ç†çš„æ–‡ä»¶åˆ—è¡¨
        """
        if not self.knowledge_source_dir.exists():
            logger.warning(f"çŸ¥è¯†æºç›®å½•ä¸å­˜åœ¨: {self.knowledge_source_dir}")
            return []
        
        new_files = []
        current_files = set()
        
        # æ”¶é›†æ‰€æœ‰æ”¯æŒçš„æ–‡ä»¶
        all_files = []
        for file_path in self.knowledge_source_dir.rglob('*'):
            if file_path.is_file() and file_path.suffix.lower() in self.supported_extensions:
                all_files.append(file_path)
        
        print(f"ğŸ“‚ æ‰«æ {len(all_files)} ä¸ªæ–‡ä»¶...")
        
        # æ‰«ææ‰€æœ‰æ”¯æŒçš„æ–‡ä»¶ - æ·»åŠ è¿›åº¦æ¡
        for file_path in tqdm(all_files, desc="æ‰«ææ–‡ä»¶", unit="ä¸ª"):
            current_files.add(str(file_path))
            
            # è·å–æ–‡ä»¶ä¿¡æ¯
            stat = file_path.stat()
            file_hash = self._calculate_file_hash(file_path)
            
            doc_info = DocumentInfo(
                file_path=str(file_path),
                file_name=file_path.name,
                file_hash=file_hash,
                file_size=stat.st_size,
                modification_time=stat.st_mtime,
                document_type=file_path.suffix.lower()[1:]  # å»æ‰ç‚¹å·
            )
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦å¤„ç†
            key = str(file_path)
            if key not in self.processed_files:
                # æ–°æ–‡ä»¶
                logger.info(f"å‘ç°æ–°æ–‡ä»¶: {file_path.name}")
                new_files.append(doc_info)
                self.processed_files[key] = doc_info
            elif (self.processed_files[key].file_hash != file_hash or 
                  self.processed_files[key].modification_time != stat.st_mtime):
                # æ–‡ä»¶å·²ä¿®æ”¹
                logger.info(f"æ–‡ä»¶å·²ä¿®æ”¹: {file_path.name}")
                new_files.append(doc_info)
                self.processed_files[key] = doc_info
        
        # æ¸…ç†å·²åˆ é™¤çš„æ–‡ä»¶
        removed_files = set(self.processed_files.keys()) - current_files
        for removed_file in removed_files:
            logger.info(f"æ–‡ä»¶å·²åˆ é™¤: {self.processed_files[removed_file].file_name}")
            del self.processed_files[removed_file]
        
        # ä¿å­˜ç´¢å¼•
        if new_files or removed_files:
            self._save_index()
        
        print(f"ğŸ“‹ æ‰«æå®Œæˆ: {len(new_files)} ä¸ªæ–°æ–‡ä»¶ï¼Œ{len(removed_files)} ä¸ªå·²åˆ é™¤")
        logger.info(f"æ‰«æå®Œæˆ: å‘ç° {len(new_files)} ä¸ªéœ€è¦å¤„ç†çš„æ–‡ä»¶")
        return new_files
    
    def has_changes(self) -> bool:
        """
        æ£€æŸ¥æ˜¯å¦æœ‰æ–‡ä»¶å˜åŒ–ï¼Œæ— éœ€é‡æ–°å¤„ç†çš„æƒ…å†µç›´æ¥è¿”å›False
        
        Returns:
            True if there are changes that require processing
        """
        if not self.knowledge_source_dir.exists():
            return False
        
        current_files = {}
        
        # å¿«é€Ÿæ‰«æå½“å‰æ–‡ä»¶çŠ¶æ€
        for file_path in self.knowledge_source_dir.rglob('*'):
            if file_path.is_file() and file_path.suffix.lower() in self.supported_extensions:
                stat = file_path.stat()
                current_files[str(file_path)] = {
                    'mtime': stat.st_mtime,
                    'size': stat.st_size
                }
        
        # æ¯”è¾ƒæ–‡ä»¶æ•°é‡
        if len(current_files) != len(self.processed_files):
            return True
        
        # æ¯”è¾ƒå·²çŸ¥æ–‡ä»¶çš„ä¿®æ”¹æ—¶é—´å’Œå¤§å°
        for file_path, current_info in current_files.items():
            if file_path not in self.processed_files:
                return True
            
            stored_info = self.processed_files[file_path]
            if (stored_info.modification_time != current_info['mtime'] or 
                stored_info.file_size != current_info['size']):
                return True
        
        return False
    
    def extract_document_content(self, doc_info: DocumentInfo) -> str:
        """æå–æ–‡æ¡£å†…å®¹"""
        return self._extract_text_from_file(Path(doc_info.file_path))
    
    def mark_as_processed(self, file_path: str, chunks_count: int):
        """æ ‡è®°æ–‡ä»¶ä¸ºå·²å¤„ç†"""
        if file_path in self.processed_files:
            self.processed_files[file_path].processed = True
            self.processed_files[file_path].chunks_count = chunks_count
            self._save_index()
    
    def get_processed_stats(self) -> Dict[str, int]:
        """è·å–å¤„ç†ç»Ÿè®¡ä¿¡æ¯"""
        total_files = len(self.processed_files)
        processed_files = sum(1 for doc in self.processed_files.values() if doc.processed)
        total_chunks = sum(doc.chunks_count for doc in self.processed_files.values() if doc.processed)
        
        return {
            'total_files': total_files,
            'processed_files': processed_files,
            'pending_files': total_files - processed_files,
            'total_chunks': total_chunks
        }
