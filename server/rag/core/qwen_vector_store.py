#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
åŸºäºQwenæ¨¡å‹çš„å‘é‡å­˜å‚¨æœåŠ¡
"""

import os
import json
import pickle
import logging
import numpy as np
import torch
from pathlib import Path
from typing import List, Dict, Any, Optional, Tuple
from dataclasses import dataclass

import faiss
from sentence_transformers import SentenceTransformer
from transformers import AutoTokenizer, AutoModel
from tqdm import tqdm

logger = logging.getLogger(__name__)


@dataclass
class DocumentChunk:
    """æ–‡æ¡£å—"""
    chunk_id: str
    content: str
    source_file: str
    chunk_index: int
    metadata: Dict[str, Any] = None


@dataclass
class SearchResult:
    """æœç´¢ç»“æœ"""
    chunk: DocumentChunk
    score: float
    rank: int


class QwenVectorStore:
    """åŸºäºQwençš„å‘é‡å­˜å‚¨"""
    
    def __init__(self, data_dir: str, embedding_model_path: str = None, device: str = None):
        """
        åˆå§‹åŒ–å‘é‡å­˜å‚¨
        
        Args:
            data_dir: æ•°æ®ç›®å½•
            embedding_model_path: åµŒå…¥æ¨¡å‹è·¯å¾„
            device: è®¡ç®—è®¾å¤‡
        """
        self.data_dir = Path(data_dir)
        self.data_dir.mkdir(parents=True, exist_ok=True)
        
        # å‘é‡å­˜å‚¨æ–‡ä»¶
        self.index_file = self.data_dir / "faiss_index.bin"
        self.metadata_file = self.data_dir / "chunks_metadata.json"
        self.chunks_file = self.data_dir / "document_chunks.pkl"
        
        # è®¾å¤‡é€‰æ‹© (ä¸ºMacBook Proä¼˜åŒ–)
        if device is None or device == "auto":
            if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
                device = "mps"
            elif torch.cuda.is_available():
                device = "cuda"
            else:
                device = "cpu"
        
        # éªŒè¯è®¾å¤‡æœ‰æ•ˆæ€§
        valid_devices = ["cpu", "cuda", "mps"]
        if device not in valid_devices:
            logger.warning(f"æ— æ•ˆçš„è®¾å¤‡ç±»å‹ '{device}'ï¼Œä½¿ç”¨ 'cpu' ä½œä¸ºé»˜è®¤å€¼")
            device = "cpu"
            
        # å¯¹äºMPSè®¾å¤‡ï¼Œè¿›è¡Œå®‰å…¨æ€§æ£€æŸ¥
        if device == "mps":
            try:
                # æµ‹è¯•MPSè®¾å¤‡æ˜¯å¦çœŸæ­£å¯ç”¨
                test_device = torch.device("mps")
                test_tensor = torch.ones(2, 2, device=test_device)
                _ = test_tensor * 2
                # æ¸…ç†æµ‹è¯•å¼ é‡
                del test_tensor
                torch.mps.empty_cache()
                logger.info("MPSè®¾å¤‡å¯ç”¨æ€§æµ‹è¯•é€šè¿‡")
                
                # è®¾ç½®MPSå†…å­˜ç®¡ç†
                os.environ["PYTORCH_MPS_HIGH_WATERMARK_RATIO"] = "0.0"
                os.environ["PYTORCH_MPS_ALLOCATOR_POLICY"] = "garbage_collection"
                
            except Exception as e:
                logger.warning(f"MPSè®¾å¤‡æµ‹è¯•å¤±è´¥ï¼Œå›é€€åˆ°CPU: {e}")
                device = "cpu"
            
        self.device = device
        logger.info(f"å‘é‡å­˜å‚¨ä½¿ç”¨è®¾å¤‡: {self.device}")
        
        # MPSå†…å­˜ç®¡ç†è®¾ç½®ï¼ˆä»…åœ¨ç¡®è®¤MPSå¯ç”¨æ—¶ï¼‰
        if self.device == "mps":
            # è®¾ç½®ä¿å®ˆçš„MPSå†…å­˜ç®¡ç†
            os.environ["PYTORCH_MPS_HIGH_WATERMARK_RATIO"] = "0.0"
            os.environ["PYTORCH_MPS_ALLOCATOR_POLICY"] = "garbage_collection"
            logger.info("å‘é‡å­˜å‚¨ä½¿ç”¨ä¼˜åŒ–çš„MPSå†…å­˜ç®¡ç†è®¾ç½®")
            
            # é¢å¤–çš„MPSç¨³å®šæ€§æ£€æŸ¥
            try:
                test_device = torch.device("mps")
                test_tensor = torch.ones(2, 2, device=test_device)
                _ = test_tensor * 2
                # æ¸…ç†æµ‹è¯•å¼ é‡
                del test_tensor
                torch.mps.empty_cache()
                logger.info("å‘é‡å­˜å‚¨MPSè®¾å¤‡ç¨³å®šæ€§æ£€æŸ¥é€šè¿‡")
            except Exception as e:
                logger.warning(f"å‘é‡å­˜å‚¨MPSè®¾å¤‡ç¨³å®šæ€§æ£€æŸ¥å¤±è´¥ï¼Œå›é€€åˆ°CPU: {e}")
                self.device = "cpu"
        
        # åµŒå…¥æ¨¡å‹
        self.embedding_model_path = embedding_model_path or "./qwen_embeddings"
        self.embedding_model = None
        self.embedding_dim = 768  # Qwen3-Embedding-0.6Bçš„ç»´åº¦
        
        # FAISSç´¢å¼•
        self.index = None
        self.chunks: List[DocumentChunk] = []
        self.chunk_metadata: Dict[str, Any] = {}
        
        # åˆ†å—å‚æ•°
        self.chunk_size = 512
        self.overlap_size = 100
        
        logger.info(f"åˆå§‹åŒ–Qwenå‘é‡å­˜å‚¨ï¼Œè®¾å¤‡: {self.device}")
    
    def _load_embedding_model(self):
        """åŠ è½½åµŒå…¥æ¨¡å‹"""
        if self.embedding_model is not None:
            return
        
        try:
            logger.info(f"åŠ è½½QwenåµŒå…¥æ¨¡å‹: {self.embedding_model_path}")
            
            # MPSè®¾å¤‡é¢„æ¸…ç†
            if self.device == "mps":
                torch.mps.empty_cache()
                logger.info("æ¨¡å‹åŠ è½½å‰MPSå†…å­˜æ¸…ç†å®Œæˆ")
            
            # ä½¿ç”¨SentenceTransformeråŠ è½½
            if os.path.exists(self.embedding_model_path):
                self.embedding_model = SentenceTransformer(
                    self.embedding_model_path,
                    device=self.device
                )
            else:
                # å¦‚æœæœ¬åœ°æ¨¡å‹ä¸å­˜åœ¨ï¼Œå°è¯•ä¸‹è½½
                logger.warning(f"æœ¬åœ°æ¨¡å‹ä¸å­˜åœ¨ï¼Œå°è¯•ä¸‹è½½QwenåµŒå…¥æ¨¡å‹")
                self.embedding_model = SentenceTransformer(
                    "Qwen/Qwen3-Embedding-0.6B",
                    device=self.device
                )
            
            # è·å–å®é™…çš„åµŒå…¥ç»´åº¦
            test_embedding = self.embedding_model.encode(["æµ‹è¯•æ–‡æœ¬"])
            self.embedding_dim = test_embedding.shape[1]
            
            # æ¸…ç†æµ‹è¯•åµŒå…¥
            del test_embedding
            if self.device == "mps":
                torch.mps.empty_cache()
                logger.info("æ¨¡å‹åŠ è½½åMPSå†…å­˜æ¸…ç†å®Œæˆ")
            
            logger.info(f"åµŒå…¥æ¨¡å‹åŠ è½½æˆåŠŸï¼Œç»´åº¦: {self.embedding_dim}")
            
        except Exception as e:
            logger.error(f"åŠ è½½åµŒå…¥æ¨¡å‹å¤±è´¥: {e}")
            raise
    
    def _split_text_into_chunks(self, text: str, source_file: str) -> List[DocumentChunk]:
        """å°†æ–‡æœ¬åˆ†å‰²æˆå—"""
        chunks = []
        
        # ç®€å•çš„æ»‘åŠ¨çª—å£åˆ†å—
        start = 0
        chunk_index = 0
        
        while start < len(text):
            end = min(start + self.chunk_size, len(text))
            chunk_text = text[start:end].strip()
            
            if chunk_text:
                chunk_id = f"{source_file}_{chunk_index}"
                chunk = DocumentChunk(
                    chunk_id=chunk_id,
                    content=chunk_text,
                    source_file=source_file,
                    chunk_index=chunk_index,
                    metadata={
                        'start_pos': start,
                        'end_pos': end,
                        'text_length': len(chunk_text)
                    }
                )
                chunks.append(chunk)
                chunk_index += 1
            
            # ç§»åŠ¨çª—å£ï¼Œè€ƒè™‘é‡å 
            start = end - self.overlap_size if end < len(text) else end
        
        logger.debug(f"æ–‡æ¡£åˆ†å—å®Œæˆ: {source_file} -> {len(chunks)} å—")
        return chunks
    
    def _create_faiss_index(self):
        """åˆ›å»ºFAISSç´¢å¼•"""
        if self.device == "mps" or self.device == "cuda":
            # GPUè®¾å¤‡ä½¿ç”¨FlatIP (å†…ç§¯)
            self.index = faiss.IndexFlatIP(self.embedding_dim)
        else:
            # CPUä½¿ç”¨ä½™å¼¦ç›¸ä¼¼åº¦
            self.index = faiss.IndexFlatIP(self.embedding_dim)
        
        logger.info(f"åˆ›å»ºFAISSç´¢å¼•: ç»´åº¦={self.embedding_dim}, è®¾å¤‡={self.device}")
    
    def load_index(self) -> bool:
        """åŠ è½½å·²å­˜åœ¨çš„ç´¢å¼•"""
        try:
            if not (self.index_file.exists() and self.metadata_file.exists() and self.chunks_file.exists()):
                logger.info("å‘é‡ç´¢å¼•æ–‡ä»¶ä¸å­˜åœ¨ï¼Œéœ€è¦é‡æ–°æ„å»º")
                return False
            
            print("ğŸ“¥ åŠ è½½å‘é‡ç´¢å¼•...")
            
            # åŠ è½½FAISSç´¢å¼•
            print("   - åŠ è½½FAISSç´¢å¼•...")
            self.index = faiss.read_index(str(self.index_file))
            
            # åŠ è½½å…ƒæ•°æ®
            print("   - åŠ è½½å…ƒæ•°æ®...")
            with open(self.metadata_file, 'r', encoding='utf-8') as f:
                self.chunk_metadata = json.load(f)
            
            # åŠ è½½æ–‡æ¡£å—
            print("   - åŠ è½½æ–‡æ¡£å—...")
            with open(self.chunks_file, 'rb') as f:
                self.chunks = pickle.load(f)
            
            print(f"âœ… å‘é‡ç´¢å¼•åŠ è½½æˆåŠŸ: {len(self.chunks)} ä¸ªæ–‡æ¡£å—")
            logger.info(f"å‘é‡ç´¢å¼•åŠ è½½æˆåŠŸ: {len(self.chunks)} ä¸ªæ–‡æ¡£å—")
            return True
            
        except Exception as e:
            logger.error(f"åŠ è½½å‘é‡ç´¢å¼•å¤±è´¥: {e}")
            print(f"âŒ åŠ è½½å‘é‡ç´¢å¼•å¤±è´¥: {e}")
            return False
    
    def save_index(self):
        """ä¿å­˜ç´¢å¼•åˆ°ç£ç›˜"""
        try:
            print("ğŸ’¾ ä¿å­˜å‘é‡ç´¢å¼•...")
            
            if self.index is not None:
                print("   - ä¿å­˜FAISSç´¢å¼•...")
                faiss.write_index(self.index, str(self.index_file))
            
            # ä¿å­˜å…ƒæ•°æ®
            print("   - ä¿å­˜å…ƒæ•°æ®...")
            with open(self.metadata_file, 'w', encoding='utf-8') as f:
                json.dump(self.chunk_metadata, f, ensure_ascii=False, indent=2)
            
            # ä¿å­˜æ–‡æ¡£å—
            print("   - ä¿å­˜æ–‡æ¡£å—...")
            with open(self.chunks_file, 'wb') as f:
                pickle.dump(self.chunks, f)
            
            print("âœ… å‘é‡ç´¢å¼•ä¿å­˜å®Œæˆ")
            logger.info("å‘é‡ç´¢å¼•ä¿å­˜æˆåŠŸ")
            
        except Exception as e:
            logger.error(f"ä¿å­˜å‘é‡ç´¢å¼•å¤±è´¥: {e}")
            print(f"âŒ ä¿å­˜å‘é‡ç´¢å¼•å¤±è´¥: {e}")
    
    def add_documents(self, documents: List[Tuple[str, str]]) -> int:
        """
        æ·»åŠ æ–‡æ¡£åˆ°å‘é‡å­˜å‚¨
        
        Args:
            documents: [(æ–‡æ¡£å†…å®¹, æºæ–‡ä»¶è·¯å¾„), ...]
            
        Returns:
            æ·»åŠ çš„æ–‡æ¡£å—æ•°é‡
        """
        self._load_embedding_model()
        
        if self.index is None:
            self._create_faiss_index()
        
        all_chunks = []
        all_texts = []
        
        # å¤„ç†æ‰€æœ‰æ–‡æ¡£ - æ·»åŠ è¿›åº¦æ¡
        print(f"ğŸ“„ å¼€å§‹å¤„ç† {len(documents)} ä¸ªæ–‡æ¡£...")
        for content, source_file in tqdm(documents, desc="åˆ†ææ–‡æ¡£", unit="ä¸ª"):
            chunks = self._split_text_into_chunks(content, source_file)
            all_chunks.extend(chunks)
            all_texts.extend([chunk.content for chunk in chunks])
        
        if not all_texts:
            return 0
        
        print(f"ğŸ“Š æ€»å…±ç”Ÿæˆ {len(all_texts)} ä¸ªæ–‡æ¡£å—")
        
        # æ‰¹é‡ç”ŸæˆåµŒå…¥ - æ·»åŠ è¿›åº¦æ¡
        logger.info(f"ç”Ÿæˆ {len(all_texts)} ä¸ªæ–‡æ¡£å—çš„åµŒå…¥å‘é‡...")
        
        # è®¡ç®—æ‰¹æ¬¡ - æ”¯æŒç¯å¢ƒå˜é‡é…ç½®
        default_batch_size = 8 if self.device == "mps" else 32
        batch_size = int(os.environ.get("RAG_BATCH_SIZE", default_batch_size))
        
        # å¦‚æœæ˜¯MPSè®¾å¤‡ï¼Œç¡®ä¿æ‰¹æ¬¡å¤§å°ä¸ä¼šå¤ªå¤§
        if self.device == "mps" and batch_size > 16:
            logger.warning(f"MPSè®¾å¤‡æ‰¹æ¬¡å¤§å°è¿‡å¤§({batch_size})ï¼Œè°ƒæ•´ä¸º16")
            batch_size = 16
        
        total_batches = (len(all_texts) + batch_size - 1) // batch_size
        
        all_embeddings = []
        print(f"ğŸ§  å¼€å§‹ç”ŸæˆåµŒå…¥å‘é‡ (è®¾å¤‡: {self.device}, æ‰¹å¤§å°: {batch_size})...")
        
        # åˆ†æ‰¹å¤„ç†ï¼Œæ˜¾ç¤ºè¿›åº¦
        for i in tqdm(range(0, len(all_texts), batch_size), 
                     desc="ç”Ÿæˆå‘é‡", unit="batch", total=total_batches):
            batch_texts = all_texts[i:i+batch_size]
            
            # MPSå†…å­˜ç®¡ç†
            if self.device == "mps":
                import torch
                # æ¸…ç†MPSç¼“å­˜
                torch.mps.empty_cache()
                # è®¾ç½®éšæœºç§å­
                torch.manual_seed(42)
            
            try:
                batch_embeddings = self.embedding_model.encode(
                    batch_texts, 
                    batch_size=batch_size,
                    convert_to_numpy=True,
                    normalize_embeddings=True,  # å½’ä¸€åŒ–ï¼Œç”¨äºä½™å¼¦ç›¸ä¼¼åº¦
                    show_progress_bar=False  # ç¦ç”¨å†…éƒ¨è¿›åº¦æ¡ï¼Œé¿å…å†²çª
                )
                all_embeddings.append(batch_embeddings)
                
                # MPSå†…å­˜æ¸…ç†
                if self.device == "mps":
                    torch.mps.empty_cache()
                    
            except RuntimeError as e:
                if "out of memory" in str(e).lower():
                    print(f"âš ï¸ æ‰¹æ¬¡ {i//batch_size + 1} å†…å­˜ä¸è¶³ï¼Œå°è¯•æ›´å°çš„æ‰¹æ¬¡...")
                    # å°è¯•æ›´å°çš„æ‰¹æ¬¡
                    smaller_batch_size = max(1, batch_size // 2)
                    sub_embeddings = []
                    
                    for j in range(0, len(batch_texts), smaller_batch_size):
                        sub_batch = batch_texts[j:j+smaller_batch_size]
                        if self.device == "mps":
                            torch.mps.empty_cache()
                        
                        sub_embedding = self.embedding_model.encode(
                            sub_batch,
                            batch_size=smaller_batch_size,
                            convert_to_numpy=True,
                            normalize_embeddings=True,
                            show_progress_bar=False
                        )
                        sub_embeddings.append(sub_embedding)
                        
                        if self.device == "mps":
                            torch.mps.empty_cache()
                    
                    # åˆå¹¶å­æ‰¹æ¬¡
                    if sub_embeddings:
                        batch_embeddings = np.vstack(sub_embeddings)
                        all_embeddings.append(batch_embeddings)
                else:
                    raise e
        
        # åˆå¹¶æ‰€æœ‰åµŒå…¥å‘é‡
        embeddings = np.vstack(all_embeddings)
        
        # æ·»åŠ åˆ°FAISSç´¢å¼• - æ·»åŠ è¿›åº¦æ¡
        print(f"ğŸ—‚ï¸ å°† {len(embeddings)} ä¸ªå‘é‡æ·»åŠ åˆ°FAISSç´¢å¼•...")
        
        # å¯¹äºå¤§é‡å‘é‡ï¼Œåˆ†æ‰¹æ·»åŠ åˆ°FAISS
        faiss_batch_size = 1000
        for i in tqdm(range(0, len(embeddings), faiss_batch_size), 
                     desc="æ›´æ–°ç´¢å¼•", unit="batch"):
            end_idx = min(i + faiss_batch_size, len(embeddings))
            batch_vectors = embeddings[i:end_idx].astype(np.float32)
            self.index.add(batch_vectors)
        
        # æ›´æ–°æ–‡æ¡£å—åˆ—è¡¨
        self.chunks.extend(all_chunks)
        
        # æ›´æ–°å…ƒæ•°æ®
        print("ğŸ“‹ æ›´æ–°å…ƒæ•°æ®...")
        for i, chunk in enumerate(tqdm(all_chunks, desc="æ›´æ–°å…ƒæ•°æ®", unit="å—")):
            self.chunk_metadata[chunk.chunk_id] = {
                'source_file': chunk.source_file,
                'chunk_index': chunk.chunk_index,
                'content_length': len(chunk.content),
                'vector_index': len(self.chunks) - len(all_chunks) + i
            }
        
        logger.info(f"æˆåŠŸæ·»åŠ  {len(all_chunks)} ä¸ªæ–‡æ¡£å—åˆ°å‘é‡å­˜å‚¨")
        print(f"âœ… å‘é‡å­˜å‚¨æ›´æ–°å®Œæˆ: +{len(all_chunks)} ä¸ªæ–‡æ¡£å—")
        return len(all_chunks)
    
    def search(self, query: str, top_k: int = 5) -> List[SearchResult]:
        """
        å‘é‡æœç´¢
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            top_k: è¿”å›ç»“æœæ•°é‡
            
        Returns:
            æœç´¢ç»“æœåˆ—è¡¨
        """
        if self.index is None or len(self.chunks) == 0:
            logger.warning("å‘é‡ç´¢å¼•ä¸ºç©º")
            return []
        
        self._load_embedding_model()
        
        # ç”ŸæˆæŸ¥è¯¢å‘é‡
        query_embedding = self.embedding_model.encode(
            [query], 
            convert_to_numpy=True,
            normalize_embeddings=True
        )
        
        # æœç´¢
        scores, indices = self.index.search(
            query_embedding.astype(np.float32), 
            min(top_k, len(self.chunks))
        )
        
        # æ„å»ºç»“æœ
        results = []
        for rank, (score, idx) in enumerate(zip(scores[0], indices[0])):
            if idx < len(self.chunks):
                result = SearchResult(
                    chunk=self.chunks[idx],
                    score=float(score),
                    rank=rank + 1
                )
                results.append(result)
        
        return results
    
    def get_stats(self) -> Dict[str, Any]:
        """è·å–å‘é‡å­˜å‚¨ç»Ÿè®¡ä¿¡æ¯"""
        return {
            'total_chunks': len(self.chunks),
            'embedding_dim': self.embedding_dim,
            'device': self.device,
            'index_exists': self.index is not None,
            'chunk_size': self.chunk_size,
            'overlap_size': self.overlap_size
        }
    
    def clear(self):
        """æ¸…ç©ºå‘é‡å­˜å‚¨"""
        self.index = None
        self.chunks = []
        self.chunk_metadata = {}
        
        # åˆ é™¤æ–‡ä»¶
        for file in [self.index_file, self.metadata_file, self.chunks_file]:
            if file.exists():
                file.unlink()
        
        logger.info("å‘é‡å­˜å‚¨å·²æ¸…ç©º")
