#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
å¿ƒç†å’¨è¯¢å¯¹è¯ç³»ç»Ÿå¯åŠ¨è„šæœ¬
"""

import sys
import os
from pathlib import Path
from app import app

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(str(Path(__file__).parent))


def check_environment():
    """æ£€æŸ¥ç¯å¢ƒå˜é‡é…ç½®"""
    # é¦–å…ˆå°è¯•åŠ è½½ç¯å¢ƒå˜é‡
    try:
        from load_env import load_environment

        load_environment()
    except ImportError:
        print("âš ï¸ æ— æ³•åŠ è½½ç¯å¢ƒå˜é‡æ¨¡å—ï¼Œä½¿ç”¨ç³»ç»Ÿç¯å¢ƒå˜é‡")
        from dotenv import load_dotenv

        load_dotenv()
    
    # è®¾ç½®tokenizerså¹¶è¡Œå¤„ç†ç¯å¢ƒå˜é‡ï¼Œé¿å…forkè­¦å‘Š
    if not os.getenv("TOKENIZERS_PARALLELISM"):
        os.environ["TOKENIZERS_PARALLELISM"] = "false"
        print("ğŸ”§ è®¾ç½® TOKENIZERS_PARALLELISM=false ä»¥é¿å…å¹¶è¡Œå¤„ç†è­¦å‘Š")

    # TODO: æ•´ç†å®é™…æ‰€éœ€çš„ç¯å¢ƒå˜é‡
    required_vars = ["OPENAI_API_KEY", "BASE_URL", "MODEL_NAME"]

    missing_vars = []
    for var in required_vars:
        if not os.getenv(var):
            missing_vars.append(var)

    if missing_vars:
        print(f"âŒ ç¼ºå°‘å¿…è¦çš„ç¯å¢ƒå˜é‡: {', '.join(missing_vars)}")
        print("è¯·ç¡®ä¿åœ¨.envæ–‡ä»¶ä¸­è®¾ç½®è¿™äº›å˜é‡")
        return False

    print("âœ… ç¯å¢ƒå˜é‡æ£€æŸ¥é€šè¿‡")
    return True

def initialize_rag_system():
    """åˆå§‹åŒ–RAGç³»ç»Ÿ"""
    rag_enabled = os.environ.get("ENABLE_RAG", "true").lower() == "true"
    
    if not rag_enabled:
        print("â„¹ï¸ RAG åŠŸèƒ½å·²ç¦ç”¨")
        return True
    
    # ç¡®ä¿tokenizerså¹¶è¡Œå¤„ç†è®¾ç½®æ­£ç¡®
    if not os.getenv("TOKENIZERS_PARALLELISM"):
        os.environ["TOKENIZERS_PARALLELISM"] = "false"
    
    try:
        print("ğŸ” åˆå§‹åŒ–RAGç³»ç»Ÿ...")
        
        # å¯¼å…¥æ–°çš„RAGæ ¸å¿ƒæ¨¡å—
        from rag.core.rag_service import RAGCoreService
        import torch
        
        # è®¾ç½®è·¯å¾„
        knowledge_source_dir = str(Path(__file__).parent / "knowledge_source")
        data_dir = str(Path(__file__).parent / "data")
        embedding_model_path = str(Path(__file__).parent / "qwen_embeddings")
        rerank_model_path = str(Path(__file__).parent / "qwen_reranker")
        
        # è®¾å¤‡é€‰æ‹©ï¼ˆä¸ºMacBook Proä¼˜åŒ–ï¼‰
        device_config = os.environ.get("RAG_DEVICE", "auto").lower()
        force_cpu = os.environ.get("RAG_FORCE_CPU", "false").lower() == "true"
        
        if force_cpu:
            device = "cpu"
            print(f"ğŸ–¥ï¸ å¼ºåˆ¶ä½¿ç”¨è®¾å¤‡: {device}")
        elif device_config == "auto":
            # è‡ªåŠ¨é€‰æ‹©è®¾å¤‡
            if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
                device = "mps"
                # å®‰å…¨çš„MPSå†…å­˜ç®¡ç†è®¾ç½®
                mps_ratio = os.environ.get("PYTORCH_MPS_HIGH_WATERMARK_RATIO", "0.0")
                try:
                    # éªŒè¯MPS ratioæœ‰æ•ˆæ€§
                    ratio_float = float(mps_ratio)
                    if ratio_float < 0.0 or ratio_float > 1.0:
                        print(f"âš ï¸ æ— æ•ˆçš„MPSå†…å­˜æ¯”ç‡: {mps_ratio}ï¼Œä½¿ç”¨é»˜è®¤å€¼0.0")
                        mps_ratio = "0.0"
                        os.environ["PYTORCH_MPS_HIGH_WATERMARK_RATIO"] = mps_ratio
                    
                    # å¯¹äºMPSï¼Œä½¿ç”¨ä¿å®ˆçš„å†…å­˜ç®¡ç†è®¾ç½®
                    os.environ["PYTORCH_MPS_HIGH_WATERMARK_RATIO"] = "0.0"
                    
                    # è®¾ç½®MPSå†…å­˜ç®¡ç†ç¯å¢ƒå˜é‡
                    os.environ["PYTORCH_MPS_ALLOCATOR_POLICY"] = "garbage_collection"
                    
                    # æµ‹è¯•MPSæ˜¯å¦çœŸæ­£å¯ç”¨
                    try:
                        test_device = torch.device("mps")
                        test_tensor = torch.ones(2, 2, device=test_device)
                        _ = test_tensor * 2  # ç®€å•çš„è¿ç®—æµ‹è¯•
                        # æ¸…ç†æµ‹è¯•å¼ é‡
                        del test_tensor
                        torch.mps.empty_cache()
                        print(f"ğŸ§  MPSè®¾å¤‡æµ‹è¯•æˆåŠŸï¼Œä½¿ç”¨ä¼˜åŒ–çš„å†…å­˜ç®¡ç†")
                        
                        # æ˜¾ç¤ºMPSå†…å­˜ç®¡ç†è¯´æ˜
                        print(f"   ğŸ’¡ MPSå†…å­˜ç®¡ç†è¯´æ˜:")
                        print(f"   â€¢ é«˜æ°´ä½çº¿æ¯”ç‡: {mps_ratio} (0.0=æ— é™åˆ¶ï¼Œ0.7=é»˜è®¤)")
                        print(f"   â€¢ åƒåœ¾å›æ”¶ç­–ç•¥: å¯ç”¨")
                        print(f"   â€¢ æ‰¹å¤„ç†å¤§å°: {os.environ.get('RAG_BATCH_SIZE', '8')} (å¯åœ¨.envä¸­è°ƒæ•´)")
                        
                    except Exception as mps_e:
                        print(f"âš ï¸ MPSè®¾å¤‡æµ‹è¯•å¤±è´¥ï¼Œå›é€€åˆ°CPU: {mps_e}")
                        device = "cpu"
                        
                except ValueError:
                    print(f"âš ï¸ MPSå†…å­˜æ¯”ç‡æ ¼å¼é”™è¯¯: {mps_ratio}ï¼Œä½¿ç”¨CPUè®¾å¤‡")
                    device = "cpu"
            elif torch.cuda.is_available():
                device = "cuda"
            else:
                device = "cpu"
            print(f"ğŸ–¥ï¸ è‡ªåŠ¨é€‰æ‹©è®¾å¤‡: {device}")
        else:
            device = device_config
            print(f"ğŸ–¥ï¸ æŒ‡å®šä½¿ç”¨è®¾å¤‡: {device}")
        
        # è·å–åˆ†å—é…ç½®
        chunk_size = int(os.environ.get("RAG_CHUNK_SIZE", "512"))
        chunk_overlap = int(os.environ.get("RAG_CHUNK_OVERLAP", "50"))
        batch_size = int(os.environ.get("RAG_BATCH_SIZE", "8" if device == "mps" else "32"))
        
        print(f"ğŸ“„ åˆ†å—é…ç½®: å¤§å°={chunk_size}, é‡å ={chunk_overlap}")
        print(f"ğŸ”¢ æ‰¹å¤„ç†å¤§å°: {batch_size}")
        
        # å¦‚æœæ˜¯MPSè®¾å¤‡ï¼Œæ˜¾ç¤ºå†…å­˜ç®¡ç†ä¿¡æ¯
        if device == "mps":
            mps_ratio = os.environ.get("PYTORCH_MPS_HIGH_WATERMARK_RATIO", "0.0")
            print(f"ğŸ§  MPSå†…å­˜ç®¡ç†: é«˜æ°´ä½çº¿æ¯”ç‡={mps_ratio}, åƒåœ¾å›æ”¶ç­–ç•¥=å¯ç”¨")
        
        # åˆ›å»ºRAGæ ¸å¿ƒæœåŠ¡
        rag_service = RAGCoreService(
            knowledge_source_dir=knowledge_source_dir,
            data_dir=data_dir,
            embedding_model_path=embedding_model_path,
            rerank_model_path=rerank_model_path,
            device=device,
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap
        )
        
        # åˆå§‹åŒ–RAGç³»ç»Ÿï¼ˆä¼šæ‰«æå’Œå¤„ç†æ–°æ–‡ä»¶ï¼‰
        success = rag_service.initialize()
        
        if success:
            stats = rag_service.get_statistics()
            print(f"âœ… RAGç³»ç»Ÿåˆå§‹åŒ–æˆåŠŸ:")
            print(f"   - å·²å¤„ç†æ–‡ä»¶: {stats['knowledge_base']['processed_files']}")
            print(f"   - å‘é‡åº“æ–‡æ¡£å—: {stats['vector_store']['total_chunks']}")
            print(f"   - åµŒå…¥ç»´åº¦: {stats['vector_store']['embedding_dim']}")
            print(f"   - è®¡ç®—è®¾å¤‡: {stats['vector_store'].get('device', 'unknown')}")
            
            # å°†RAGæœåŠ¡å­˜å‚¨åˆ°å…¨å±€å˜é‡ï¼Œä¾›å…¶ä»–æ¨¡å—ä½¿ç”¨
            import sys
            import __main__
            __main__.rag_service = rag_service
            # åŒæ—¶ä¹Ÿå­˜å‚¨åˆ°startæ¨¡å—ï¼ˆå¦‚æœè¢«ä½œä¸ºæ¨¡å—å¯¼å…¥ï¼‰
            sys.modules['start'] = sys.modules[__name__]
            sys.modules['start'].rag_service = rag_service
            
            # ç°åœ¨åˆå§‹åŒ–èŠå¤©æœåŠ¡ï¼Œç¡®ä¿å®ƒèƒ½æ‰¾åˆ°å…¨å±€RAGæœåŠ¡
            print("ğŸ’¬ åˆå§‹åŒ–èŠå¤©æœåŠ¡...")
            from service.chat_langgraph_optimized import get_chat_service_instance
            get_chat_service_instance()
            print("âœ… èŠå¤©æœåŠ¡åˆå§‹åŒ–å®Œæˆ")
            
        else:
            print("âŒ RAGç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥")
            return False
            
        return True
        
    except ImportError as e:
        print(f"âš ï¸ RAG ä¾èµ–æ¨¡å—ç¼ºå¤±: {e}")
        print("RAG åŠŸèƒ½å°†è¢«ç¦ç”¨ï¼Œè¯·æ£€æŸ¥ä¾èµ–åŒ…å®‰è£…")
        return True  # ä¸é˜»æ­¢ç³»ç»Ÿå¯åŠ¨
    except Exception as e:
        print(f"âŒ RAGç³»ç»Ÿåˆå§‹åŒ–é”™è¯¯: {e}")
        return False


def check_dependencies():
    """æ£€æŸ¥ä¾èµ–åŒ…"""
    try:
        import flask
        import langchain_openai
        import langgraph
        import snownlp
        import pydantic

        print("âœ… åŸºç¡€ä¾èµ–åŒ…æ£€æŸ¥é€šè¿‡")
        
        # æ£€æŸ¥ RAG ä¾èµ–
        rag_enabled = os.environ.get("ENABLE_RAG", "true").lower() == "true"
        if rag_enabled:
            try:
                import numpy
                import faiss
                import sentence_transformers
                import torch
                import transformers
                import fitz  # PyMuPDF
                print("âœ… RAG ä¾èµ–åŒ…æ£€æŸ¥é€šè¿‡")
            except ImportError as e:
                print(f"âš ï¸ RAG ä¾èµ–åŒ…ç¼ºå¤±: {e}")
                print("RAG åŠŸèƒ½å°†è¢«ç¦ç”¨ï¼Œè¯·è¿è¡Œå®‰è£…å‘½ä»¤:")
                print("pip install -r requirements_rag.txt")
        else:
            print("â„¹ï¸ RAG åŠŸèƒ½å·²ç¦ç”¨")
        
        return True
    except ImportError as e:
        print(f"âŒ ç¼ºå°‘ä¾èµ–åŒ…: {e}")
        print("è¯·è¿è¡Œ: pip install -r requirements.txt")
        return False


def check_prompt_files():
    """æ£€æŸ¥æç¤ºæ–‡ä»¶"""
    prompt_dir = Path(__file__).parent / "prompt"
    required_files = [
        "counselor_prompt.txt",
        "planning_prompt.txt",
        "guided_inquiry_prompt.txt",
        "pattern_analysis_prompt.txt",
    ]

    missing_files = []
    for file in required_files:
        if not (prompt_dir / file).exists():
            missing_files.append(file)

    if missing_files:
        print(f"âš ï¸ ç¼ºå°‘æç¤ºæ–‡ä»¶: {', '.join(missing_files)}")
        print("ç³»ç»Ÿå°†ä½¿ç”¨é»˜è®¤æç¤ºæ¨¡æ¿")
    else:
        print("âœ… æç¤ºæ–‡ä»¶æ£€æŸ¥é€šè¿‡")

    return True


def initialize_data_directories():
    """åˆå§‹åŒ–æ•°æ®ç›®å½•"""
    data_dir = Path(__file__).parent / "data"
    subdirs = ["messages", "patterns", "plans", "events"]

    data_dir.mkdir(exist_ok=True)
    for subdir in subdirs:
        (data_dir / subdir).mkdir(exist_ok=True)

    print("âœ… æ•°æ®ç›®å½•åˆå§‹åŒ–å®Œæˆ")


def start_server():
    """å¯åŠ¨æœåŠ¡å™¨"""
    print("ğŸš€ å¯åŠ¨å¿ƒç†å’¨è¯¢å¯¹è¯ç³»ç»Ÿ...")
    print("ğŸ“Š ä½¿ç”¨é›†æˆç‰ˆæ¶æ„:")
    print("   - ç»Ÿä¸€æ•°æ®åº“ç®¡ç†")
    print("   - ç”¨æˆ·ç”»åƒå­˜å‚¨")
    print("   - é•¿çŸ­æœŸè®°å¿†")
    print("   - å±æœºæ£€æµ‹")
    print("   - è¡Œä¸ºæ¨¡å¼åˆ†æ")
    print("   - æƒ…ç»ªè¯„åˆ†")
    print("   - æ™ºèƒ½æ—¥å¿—è®°å½•")
    print()

    # æ˜¾ç¤ºæ—¥å¿—é…ç½®
    chat_logging = os.environ.get("ENABLE_CHAT_LOGGING", "true").lower() == "true"
    emotion_logging = os.environ.get("ENABLE_EMOTION_LOGGING", "true").lower() == "true"
    detailed_logging = (
        os.environ.get("ENABLE_DETAILED_LOGGING", "true").lower() == "true"
    )

    print("ğŸ“ æ—¥å¿—é…ç½®:")
    print(f"   èŠå¤©æ—¥å¿—: {'âœ… å¯ç”¨' if chat_logging else 'âŒ ç¦ç”¨'}")
    print(f"   æƒ…ç»ªæ—¥å¿—: {'âœ… å¯ç”¨' if emotion_logging else 'âŒ ç¦ç”¨'}")
    print(f"   è¯¦ç»†æ—¥å¿—: {'âœ… å¯ç”¨' if detailed_logging else 'âŒ ç¦ç”¨'}")
    print()

    # è·å–é…ç½®
    HOST = os.environ.get("HOST", "0.0.0.0")
    PORT = int(os.environ.get("PORT", 5858))
    DEBUG = os.environ.get("FLASK_ENV", "production") == "development"

    print(f"ğŸŒ æœåŠ¡å™¨åœ°å€: http://{HOST}:{PORT}")
    print(f"ğŸ”§ è°ƒè¯•æ¨¡å¼: {'å¼€å¯' if DEBUG else 'å…³é—­'}")
    print()
    print("APIç«¯ç‚¹:")
    print(f"  - POST http://{HOST}:{PORT}/api/chat - èŠå¤©å¯¹è¯")
    print(f"  - GET  http://{HOST}:{PORT}/api/chat/history - å¯¹è¯å†å²")
    print(f"  - POST http://{HOST}:{PORT}/api/mood - æƒ…ç»ªåˆ†æ")
    print()
    print("æŒ‰ Ctrl+C åœæ­¢æœåŠ¡å™¨")
    print("=" * 50)

    try:
        app.run(host=HOST, port=PORT, debug=DEBUG)
    except KeyboardInterrupt:
        print("\nğŸ‘‹ æœåŠ¡å™¨å·²åœæ­¢")


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ©º å¿ƒç†å’¨è¯¢å¯¹è¯ç³»ç»Ÿ")
    print("=" * 50)

    # æ£€æŸ¥ç¯å¢ƒ
    if not check_environment():
        return

    if not check_dependencies():
        return

    check_prompt_files()
    initialize_data_directories()

    # åˆå§‹åŒ–RAGç³»ç»Ÿ
    if not initialize_rag_system():
        print("RAGç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥ï¼Œæ˜¯å¦ç»§ç»­å¯åŠ¨ï¼Ÿ(y/N): ", end="")
        choice = input().lower()
        if choice != 'y':
            return


    print("=" * 50)
    start_server()


if __name__ == "__main__":
    main()
