\section{Operation Manual}
\label{sec:manual}

\subsection{System Installation and Startup}

\paragraph{Backend Server}
\begin{enumerate}
    \item Install Python 3.11 or above.
    \item Clone the repository and navigate to the \texttt{server/} directory.
    \item Install dependencies:
    \begin{verbatim}
    pip install -r requirements.txt
    \end{verbatim}
    \item Copy the environment variable example file and configure it:
    \begin{verbatim}
    cp .env.example .env
    \end{verbatim}
    \item Edit \texttt{.env} to fill in your API keys and adjust configuration as needed (see below for details).
    \item Start the backend server:
    \begin{verbatim}
    python start.py
    \end{verbatim}
    The server will run on \texttt{http://localhost:5858} by default.
\end{enumerate}

\paragraph{WeChat Mini Program}
\begin{enumerate}
    \item Download and install the WeChat Developer Tools.
    \item Import the \texttt{miniprogram/} project by file path.
    \item Configure the AppID (for demo, you may use the provided temporary AppID in \texttt{project.config.json}).
    \item In the developer tools, enable the option ``Do not verify legal domain name'' to allow local backend access.
    \item Click ``Compile'' to build and run the Mini Program.
\end{enumerate}

\subsection{Environment Variable Configuration}

The system uses a \texttt{.env} file in the \texttt{server/} directory to manage configuration. Key parameters include:

\begin{itemize}
    \item \texttt{FLASK_ENV}: Set to \texttt{development} for local testing.
    \item \texttt{PORT}: Backend server port (default: 5858).
    \item \texttt{HOST}: Server host (default: 0.0.0.0).
    \item \texttt{OPENAI_API_KEY}: Your OpenAI API key (required for LLM features).
    \item \texttt{BASE_URL}: LLM API base URL (default: https://api.sambanova.ai/v1).
    \item \texttt{MODEL_NAME}: Main LLM model (e.g., Meta-Llama-3.1-8B-Instruct).
    \item \texttt{MAX_TOKENS}: Maximum tokens per response (default: 1000).
    \item \texttt{TEMPERATURE}: Sampling temperature (e.g., 0.1 for stable, 0.7 for more creative).
    \item \texttt{ENABLE_GUIDED_INQUIRY}: Enable guided inquiry tool (True/False).
    \item \texttt{ENABLE_PATTERN_ANALYSIS}: Enable pattern analysis tool (True/False).
    \item \texttt{CHAT_API_KEY}, \texttt{CHAT_MODEL_NAME}, \texttt{CHAT_BASE_URL}: For chat-specific LLM configuration.
    \item \texttt{EVENT_API_KEY}, \texttt{EVENT_MODEL_NAME}, \texttt{EVENT_BASE_URL}: For event extraction LLM configuration.
    \item \texttt{SERPAPI_KEY}: API key for search feature (optional).
\end{itemize}

\subsection{User Guide}

\subsubsection{Getting Started}
\begin{enumerate}
    \item Open the WeChat Mini Program in the developer tools or on your device.
    \item Start a conversation with the AI assistant.
    \item Express your thoughts and feelings in natural language.
    \item Receive empathetic responses, mood analysis, and event extraction feedback.
    \item Review, confirm, or edit the system's analysis as needed.
\end{enumerate}

\subsection{Troubleshooting}

\subsubsection{Common Issues}
\begin{itemize}
    \item \textbf{Slow Responses}: May occur during peak usage or if the LLM API rate limit is reached. Try again later.
    \item \textbf{Connection Errors}: Ensure the backend server is running and accessible at the configured address.
    \item \textbf{API Key Issues}: Make sure all required API keys are set and valid in the \texttt{.env} file.
    \item \textbf{AI Response Quality}: Adjust \texttt{MODEL_NAME} or \texttt{TEMPERATURE} in the \texttt{.env} file for different response styles.
\end{itemize}

\subsubsection{Support}
For technical support, contact:
\begin{itemize}
    \item Chen Yuan (\texttt{ryan.chenyuan@connect.hku.hk})
    \item Xu Hanlin (\texttt{hallymxu@gmail.com})
    \item Yu Yitao (\texttt{yitao_yu2024@connect.hku.hk})
    \item Su Yingcheng (\texttt{suyingc@connect.hku.hk})
    \item Wang Xueyao (\texttt{xywang5@connect.hku.hk})
\end{itemize}